[
    {
        "pid": 1,
        "title": "LAMP: Une Approche Efficace pour la Détection de Fautes dans les Systèmes Distribués",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-8c9fef2eeec4bd4b48a7c8dc6708383bc228b9e964b406954e7cc20ec8cfc564",
            "timestamp": 1738244510,
            "size": 41185
        },
        "abstract_only": true,
        "abstract": "La détection des fautes constitue un élément clé pour assurer la fiabilité et la sécurité des\r\nsystèmes distribués. Cependant, les approches existantes, telles que l’audit décentralisé et les\r\npreuves cryptographiques, entraînent un coût significatif en termes de ressources. Elles im-\r\npliquent une consommation accrue de CPU et de bande passante en raison des échanges né-\r\ncessaires à la détection des fautes et du rejeu des opérations pour vérifier l’intégrité des appli-\r\ncations auditées. Cette surcharge peut impacter directement les performances des applications\r\nsurveillées, en augmentant la latence, en mobilisant des ressources supplémentaires et, dans\r\ncertains cas, en interrompant leur exécution pour effectuer des contrôles.\r\nCe travail explore LAMP, un protocole qui exploite les Environnements d’Exécution de Confiance\r\n(TEEs) afin de structurer différemment l’audit des systèmes distribués. LAMP propose une cen-\r\ntralisation partielle du processus de vérification au sein de nœuds de confiance, réduisant ainsi\r\nla charge sur l’ensemble du système. Par ailleurs, l’intégrité des journaux est assurée par des\r\nmécanismes garantissant l’inaltérabilité des enregistrements, limitant ainsi les risques de falsi-\r\nfication.\r\nLes travaux sur LAMP sont à un stade préliminaire et nécessitent des évaluations complé-\r\nmentaires, en particulier sur son impact en termes de consommation énergétique et sur les\r\nperformances des applications auditées. Une analyse approfondie est nécessaire pour mesurer\r\nle compromis entre sécurité, coût computationnel et latence, et ainsi déterminer dans quelles\r\nconditions cette approche peut être adoptée dans des environnements réels",
        "authors": [
            {
                "email": "lamyae.hassini@u-bordeaux.fr",
                "first": "Lamyae",
                "last": "Hassini",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "joachim.bruneau-queyreix@u-bordeaux.fr",
                "first": "Joachim",
                "last": "Bruneaux-Queyreix",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "laurent.reveillere@u-bordeaux.fr",
                "first": "Laurent",
                "last": "Réveillère",
                "affiliation": "Université de Bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "lamyae.hassini@u-bordeaux.fr",
                "first": "Lamyae",
                "last": "Hassini",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "joachim.bruneau-queyreix@u-bordeaux.fr",
                "first": "Joachim",
                "last": "Bruneaux-Queyreix",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "laurent.reveillere@u-bordeaux.fr",
                "first": "Laurent",
                "last": "Réveillère",
                "affiliation": "Université de Bordeaux"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Poster seul, sans présentation",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1738244510
    },
    {
        "pid": 2,
        "title": "AUPE : Protocole collaboratif d’échantillonnage de pairs tolérant aux fautes byzantines",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-8a17fb09b3c1dbbd418f171add9a9cc462612da197b406e7a12baac7514115e6",
            "timestamp": 1738659046,
            "size": 41427
        },
        "abstract_only": true,
        "abstract": "L’échantillonnage de pairs constitue une primitive fondamentale dans les systèmes distribués,\r\npermettant le partage d’informations à grande échelle, notamment dans les blockchains. Son\r\nobjectif est de maintenir et de mettre à jour dynamiquement une vue locale et partielle de l’en-\r\nsemble des membres du système. Cependant, ces protocoles sont vulnérables aux attaques de la\r\npart d’adversaires cherchant à perturber le fonctionnement des protocoles de plus haut niveau\r\nde la blockchain. En particulier, un adversaire contrôlant un ensemble de nœuds byzantins\r\npeut manipuler la perception des nœuds honnêtes en augmentant la représentation des nœuds\r\nmalveillants dans leur vue locale.\r\nBien que les protocoles d’échantillonnage de pairs tolérants aux fautes byzantines présent dans\r\nla littérature atténuent en partie ce biais, leur efficacité diminue significativement à mesure que\r\nla proportion de nœuds malveillants dans le système augmente. Cet article introduit AUPE, le\r\npremier protocole collaboratif d’échantillonnage de pairs tolérant aux fautes byzantines. AUPE\r\nexploite la présence de nœuds de confiance, tels que des dispositifs compatibles avec Intel SGX,\r\nafin de suivre de manière collaborative la propagation des identifiants au sein du système et de\r\nréajuster localement la représentation des nœuds byzantins.\r\nDes simulations menées sur un réseau de 10 000 nœuds montrent qu’AUPE surpasse les so-\r\nlutions de l’état de l’art, atteignant une résilience quasi parfaite même face à un adversaire\r\ncontrôlant jusqu’à 26% des nœuds. En intégrant seulement 10% de nœuds de confiance, AUPE\r\naméliore la tolérance du protocole BRAHMS de 60%, tout en réduisant considérablement l’im-\r\npact des attaques adversariales, y compris lorsque l’adversaire possède jusqu’à 40% des nœuds.",
        "authors": [
            {
                "email": "augusta.mukam@u-bordeaux.fr",
                "first": "Augusta",
                "last": "Mukam",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "joachim.bruneau-queyreix@u-bordeaux.fr",
                "first": "Joachim",
                "last": "Bruneau-Queyreix",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "laurent.reveillere@u-bordeaux.fr",
                "first": "Laurent",
                "last": "Réveillère",
                "affiliation": "Université de Bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "joachim.bruneau-queyreix@u-bordeaux.fr",
                "first": "Joachim",
                "last": "Bruneaux-Queyreix",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "laurent.reveillere@u-bordeaux.fr",
                "first": "Laurent",
                "last": "Réveillère",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "augusta.mukam@u-bordeaux.fr",
                "first": "Augusta",
                "last": "Mukam",
                "affiliation": "Université de Bordeaux"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1738659046
    },
    {
        "pid": 3,
        "title": "Checkpointing optimisation to prepare future exascale plasma turbulence simulations",
        "abstract_only": true,
        "abstract": "The advent of exascale computing has revolutionized high-performance computing (HPC) and enabled unprecedented advancements in nuclear fusion research. Simulations of plasma turbulence dynamics, such as the GYSELA code, now achieve unparalleled precision and complexity. However, this progress is accompanied by significant challenges in managing the exponential growth of data generated by these simulations. Traditional input\/output (I\/O) methods struggle to handle the massive data volumes, heightened concurrency, and fault-tolerance requirements inherent to exascale systems.\r\n\r\nThis paper investigates the I\/O bottlenecks inherent in exascale computing, with a particular focus on the checkpointing mechanisms of GYSELA. These mechanisms are critical for ensuring fault tolerance and must handle several terabytes of data efficiently to avoid undermining computational performance. We analyze the current implementation of GYSELA's checkpointing mechanism managed via the PDI data interface, identifying its limitations and proposing two alternative approaches aimed at enhancing scalability and resilience.\r\nExperiments conducted on pre-exascale architectures validate the efficiency of these methods through both strong and weak scaling benchmarks. We reduced the checkpointing execution time by a factor of four, achieving near-optimal bandwidth utilisation, and we have identified implementations well-suited for exascale architectures.\r\nOur findings suggest the potential for notable performance improvements and offer insights that could help optimise I\/O operations in exascale simulations.",
        "authors": [
            {
                "email": "meline.trochon@inria.fr",
                "first": "Méline",
                "last": "TROCHON",
                "affiliation": "INRIA BORDEAUX"
            }
        ],
        "contacts": [
            {
                "email": "meline.trochon@inria.fr",
                "first": "Méline",
                "last": "TROCHON",
                "affiliation": "INRIA BORDEAUX"
            }
        ],
        "topics": [
            "Architecture",
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1739798707
    },
    {
        "pid": 4,
        "title": "Co-conception sécurisante entre le back-end d’un compilateur et un système matériel",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-72c7f533e9a80a2f1897e4fe128db09a4d8ecb7d6f3a57c8bdcec2d0f89bc43b",
            "timestamp": 1739440716,
            "size": 77358
        },
        "abstract_only": true,
        "abstract": "Les systèmes matériels sur lesquels s'exécutent les logiciels sont, par nature, vulnérables aux attaques physiques. Il existe de nombreuses contremesures matérielles ou bien logicielles visant à protéger un système matériel et logiciel de ces attaques. De multiples vulnérabilités sont régulièrement découvertes, nécessitant de nouvelles contremesures. Ces contremesures ne font majoritairement état que de solutions exclusivement matérielles ou logicielles.\r\n\r\nCependant, si l'on souhaite un système sécurisé, il faut co-concevoir le matériel et le logiciel afin de :\r\n   - s'assurer que les propriétés de sécurité matérielles sont exprimées au niveau du back-end;\r\n   - exploiter les fonctionnalités de sécurité matérielles à disposition;\r\n   - corriger les vulnérabilités restantes au niveau matériel via des contremesures logicielles;\r\n\r\nCertaines contributions de ce domaine proposent de créer, ou d'étendre de nouveaux jeux d'instructions pour des architectures (ISA). D'autres proposent de nouvelles techniques de génération de code, en plus de l'extention\/modification d'ISA sur des processeurs.\r\n\r\n\r\nComplémentairement, les contrats matériel\/logiciel, proposent une manière formelle de prouver qu'une instruction d'ISA sécurise ou non le système, grâce à la formalisation des vulnérabilités matérielles. Cela permet de définir des contremesures formelles aux vulnérabilités.\r\n\r\n\r\nNotre objectif dans cette thèse est l'utilisation et l’extension des contrats matériel\/logiciel pour prendre en compte la génération de code sécurisé, c'est-à-dire exprimer les propriétés de sécurité au niveau de la conception matérielle et du back-end de compilateur.\r\nPour cela, nous prévoyons de:  \r\n    - créer des techniques de génération de code, par développement de passes de back-end du compilateur LLVM, qui évitent des vulnérabilités matérielles connues matériel-spécifiques;\r\n    - proposer une méthodologie de co-conception matériel\/back-end de compilateur pour la sécurité, validée formellement via les contrats.\r\n\r\nMotsClés: Attaques matérielles, RISC-V, génération de code LLVM, codesign matériel\/logiciel",
        "authors": [
            {
                "email": "clara.bourgeais@lcis.grenoble-inp.fr",
                "first": "Clara",
                "last": "BOURGEAIS",
                "affiliation": "LCIS - Grenoble INP - UGA"
            },
            {
                "email": "laure.gonnord@lcis.grenoble-inp.fr",
                "first": "Laure",
                "last": "GONNORD",
                "affiliation": "LCIS - Grenoble INP - UGA"
            },
            {
                "email": "david.hely@lcis.grenoble-inp.fr",
                "first": "David",
                "last": "HÉLY",
                "affiliation": "LCIS - Grenoble INP - UGA"
            }
        ],
        "contacts": [
            {
                "email": "clara.bourgeais@lcis.grenoble-inp.fr",
                "first": "Clara",
                "last": "BOURGEAIS",
                "affiliation": "LCIS - Grenoble INP - UGA"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1739440716
    },
    {
        "pid": 5,
        "title": "Un modèle d’intelligence artificielle pour détecter et corriger des erreurs dans du code MPI",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-1a71d7fef7edfbefbd83ace6f97005f64f98644a68df71eb83e4c1e835fa4837",
            "timestamp": 1739462883,
            "size": 48647
        },
        "abstract_only": true,
        "abstract": "En calcul haute performance et notamment dans le modèle de programmation MPI, les erreurs de programmation peuvent être très complexes et nécessiter l'exécution d'outils de détection automatisés pour être identifiées. Cependant, les outils disponibles actuellement ne répondent pas au besoin croissant d'assistance à la programmation. Les outils statiques qui s'appliquent sur le code source sont peu coûteux à exécuter mais ils sont également trop peu précis. Inversement, les outils dynamiques qui nécessitent l'exécution du programme sont plus précis sur leur détection d'erreur mais très coûteux à utiliser. L'utilisation de l'intelligence artificielle pour détecter et corriger des erreurs dans du code est une pratique de plus en plus courante, avec notamment des outils comme GitHub Copilot ou GPT-4, mais les modèles actuels sont très généraux et peu adaptés aux problèmes spécifiques du calcul haute performance. Nous cherchons alors à savoir s'il est possible de développer un outil de détection et de correction grâce à l'intelligence artificielle pour identifier, à moindre coût et avec une grande précision, la présence ou non d'erreurs dans du code MPI.\r\n\r\nUn des principaux défis soulevés par cette étude étant qu'il n'existe pour l'instant aucune base de données d'erreurs réelles en calcul haute performance, nous proposons une nouvelle base de données composée à partir de programmes en MPI récupérés de plusieurs milliers de projets open-source depuis GitHub. Nous trions et filtrons ces fichiers, puis nous introduisons des erreurs par mutation synthétique du code source, avec l'objectif de reproduire le plus fidèlement possible le comportement de programmeurs et programmeuses qui pourraient faire ces erreurs naturellement. Nous représentons ensuite les fichiers mutés par des vecteurs d'embedding, qui nous serviront à entraîner des modèles de détection d'erreur. Nous voulons étudier la précision et la capacité de généralisation de tels modèles sur des données nouvelles et réalistes. Dans le futur, nous voudrions apprendre à générer du code correct à partir d'une erreur. Pour cela, nous envisageons de représenter sous forme d'un unique vecteur la différence entre deux versions d'un même code, avant et après une modification, afin de caractériser ce changement en fonction des erreurs introduites ou corrigées.",
        "authors": [
            {
                "email": "asia.auville@inria.fr",
                "first": "Asia",
                "last": "Auville",
                "affiliation": "Centre Inria de l'université de bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "asia.auville@inria.fr",
                "first": "Asia",
                "last": "Auville",
                "affiliation": "Centre Inria de l'université de bordeaux"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1739462883
    },
    {
        "pid": 6,
        "title": "Performance de Proxmox pour les Applications HPC en Contextes de Partage de Ressources",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-9256c87172116bd535cd8537d65441de693677b1c46c7da2e51d3d4865bc7d8c",
            "timestamp": 1739488762,
            "size": 344136
        },
        "abstract_only": false,
        "abstract": "Cette étude compare les performances de la plateforme de virtualisation Proxmox et de l'hyperviseur KVM, en les comparant à un environnement bare-metal dans un contexte HPC. Les résultats montrent que, en mode compétitif, Proxmox présente une perte de performance de 6\\% à 12\\% par rapport au bare-metal et de 5\\% à 10\\% par rapport à KVM dans les tests CPU intensifs. En mode coopératif, Proxmox affiche une augmentation de latence de 8\\% par rapport à KVM. L'analyse de l'utilisation des ressources révèle une consommation mémoire 5\\% supérieure sous Proxmox. Bien que Proxmox introduise une légère surcharge, ses performances restent compétitives pour les applications HPC dans des environnements virtuels, offrant une solution efficace pour la gestion centralisée et flexible des ressources.",
        "authors": [
            {
                "email": "aurelien.izoulet@epita.fr",
                "first": "Aurélien",
                "last": "Izoulet",
                "affiliation": "École Pour l'Informatique et les Techniques Avancées (EPITA)"
            },
            {
                "email": "dw.beserra@gmail.com",
                "first": "David",
                "last": "Beserra",
                "affiliation": "École Pour l'Informatique et les Techniques Avancées (EPITA)"
            }
        ],
        "contacts": [
            {
                "email": "dw.beserra@gmail.com",
                "first": "David",
                "last": "Beserra",
                "affiliation": "École Pour l'Informatique et les Techniques Avancées (EPITA)"
            },
            {
                "email": "aurelien.izoulet@epita.fr",
                "first": "Aurélien",
                "last": "Izoulet",
                "affiliation": "École Pour l'Informatique et les Techniques Avancées (EPITA)"
            }
        ],
        "topics": [
            "Architecture",
            "Parallélisme",
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1739488762
    },
    {
        "pid": 7,
        "title": "MLKAPS : Machine Learning and Adaptive Sampling for HPC Kernel Auto-tuning",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-666f961d03e02db4683d470e427ab5d4a1ec8cdeb241a01c6cc06116cfe17fd4",
            "timestamp": 1739525523,
            "size": 49682
        },
        "abstract_only": true,
        "abstract": "We introduce a novel auto-tuning tool, MLKAPS, designed to address the challenges of optimizing high-performance computing (HPC) kernel parameters. MLKAPS generates decision trees that provide an efficient parameter configuration for any user input. It replaces manually tuned configurations, which are resource-intensive to collect, prone to human bias, and often result in suboptimal performance in certain input regions. MLKAPS automates this process using machine learning and adaptive sampling, significantly improving efficiency and scalability.\r\n\r\nWe introduce a novel sampling strategy, GA-Adaptive, which balances exploration and exploitation in high-dimensional search spaces. By iteratively refining a surrogate model built with Gradient Boosting Decision Trees (GBDT), MLKAPS identifies optimized configurations and leverages transfer-learning to efficiently optimize many inputs. This approach allows MLKAPS to adapt to diverse hardware architectures and input spaces without manual intervention.\r\n\r\nThe effectiveness of MLKAPS is demonstrated on two Intel Math Kernel Library (MKL) kernels: dgetrf (LU) and dgeqrf (QR). The results indicate that MLKAPS outperforms hand-tuned configurations by achieving a geometric mean speedup of x1.30 on 85\\% of the input space, and highlights blind spots in the existing manual tuning. Compared to state-of-the-art tools like Optuna and GPTune, MLKAPS achieves faster convergence, better scalability, and superior performance in larger design spaces. On the Scalapack PDGEQRF kernel taken from the GPTune examples, MLKAPS not only outperforms GPTune in terms of final performance, it also collects more samples for a fraction of the cost in both time and memory.\r\n\r\nBeyond its performance gains, MLKAPS provides practical advantages by generating decision trees that can be embedded directly into HPC kernels. The runtime overhead is minimal and it enables simplified deployment in production environments. Future work will focus on extending MLKAPS to support more kernel types, smarter exploration of the input space, and refined decision trees to maximize performance and deployability.",
        "authors": [
            {
                "email": "mathys.jam@uvsq.fr",
                "first": "Mathys",
                "last": "Jam",
                "affiliation": "Université Paris-Saclay, UVSQ, LI-PaRAD"
            },
            {
                "email": "eric.petit@intel.com",
                "first": "Eric",
                "last": "Petit",
                "affiliation": "Intel Corp."
            },
            {
                "email": "pablo.oliveira@uvsq.fr",
                "first": "Pablo de Oliveira",
                "last": "Castro",
                "affiliation": "Université Paris-Saclay, UVSQ, LI-PaRAD"
            },
            {
                "email": "david.defour@univ-perp.fr",
                "first": "David",
                "last": "Defour",
                "affiliation": "Université de Perpignan via Domitia, LAMPS"
            },
            {
                "email": "greg.henry@intel.com",
                "first": "Greg",
                "last": "Henry",
                "affiliation": "Intel Corp."
            },
            {
                "email": "william.jalby@uvsq.fr",
                "first": "William",
                "last": "Jalby",
                "affiliation": "Université Paris-Saclay, UVSQ, LI-PaRAD"
            }
        ],
        "contacts": [
            {
                "email": "mathys.jam@uvsq.fr",
                "first": "Mathys",
                "last": "Jam",
                "affiliation": "Université Paris-Saclay, UVSQ, LI-PaRAD"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1739525523
    },
    {
        "pid": 8,
        "title": "High Performance Solver for Aeroacoustic Simulations for Airbus",
        "abstract_only": true,
        "abstract": "As a part of system design tasks, Airbus simulates the acoustic vibrations sustained by mechanical components during take-off to ensure minimal noise pollution and the prevention of component damage. These simulations involve solving a coupled sparse-dense linear system, which results from modeling the heterogeneous medium of jet engine flow (with Finite Element Method, FEM) and the homogeneous medium of aircraft surface (with Boundary Element Method, BEM), respectively. Given the massive scale of these sparse-dense coupled systems (hundreds of millions of FEM unknowns and millions of BEM unknowns), optimizing computational time and memory usage is essential. Despite significant advances in dense and sparse direct solvers, their application to coupled sparse\/dense systems has been under-explored. In this presentation, the first aim is to empirically evaluate the existing sparse-dense solver combinations to solve these coupled systems. The second aim is to discuss, through experimental results, if solving the whole coupled system with only H-mat solver is more efficient than using two different solvers to avoid the overhead of coupling two existing non-adapted solvers.",
        "authors": [
            {
                "email": "emmanuel.agullo@inria.fr",
                "first": "Emmanuel",
                "last": "Agullo",
                "affiliation": "Inria center at the university of Bordeaux"
            },
            {
                "email": "esragul.korkmaz@inria.fr",
                "first": "Esragul",
                "last": "Korkmaz",
                "affiliation": "Inria center at the university of Bordeaux"
            },
            {
                "email": "guillaume.sylvand@airbus.com",
                "first": "Guillaume",
                "last": "Sylvand",
                "affiliation": "Airbus"
            }
        ],
        "contacts": [
            {
                "email": "esragul.korkmaz@inria.fr",
                "first": "Esragul",
                "last": "Korkmaz",
                "affiliation": "Inria center at the university of Bordeaux"
            },
            {
                "email": "emmanuel.agullo@inria.fr",
                "first": "Emmanuel",
                "last": "Agullo",
                "affiliation": "Inria"
            }
        ],
        "topics": [],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740006190
    },
    {
        "pid": 9,
        "title": "État des connaissances sur la gestion de la responsabilité pour les microcontrôleurs multi-parties dans le Continuum Cloud-to-IoT",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-9cfc7dfba349851bc606d6e76b9d2c47d51716f4d5e448f1a85d81c434b3c0a7",
            "timestamp": 1740147663,
            "size": 896875
        },
        "abstract_only": false,
        "abstract": "Les solutions de conteneurisation légère permettent l'exécution sécurisée sur microcontrôleur d'applicatifs venant de plusieurs acteurs se méfiant mutuellement les uns des autres. Ce nouveau paradigme pose des problèmes de gestion des responsabilités étant donné que les modèles de surveillance actuels sont prévus pour des micrologiciels (firmware) monolithiques et que les mécanismes d'autres infrastructures comme le Cloud ne sont pas adaptés aux contraintes des microcontrôleurs. Cet article fournit un état des connaissances sur ce sujet. Premièrement, nous définissons l'architecture des microcontrôleurs multi-parties et ses différentes parties prenantes.\r\nEnsuite, nous analysons la capacité de plusieurs solutions de conteneurisation à assurer des engagements sur des microcontrôleurs multi-parties. Finalement, nous examinons les méthodes permettant aux différents acteurs de vérifier le respect des engagements. Cette étude souligne le besoin d'améliorer les solutions de conteneurisation pour prendre en compte les engagements, notamment de performances, ainsi que le besoin de méthodes de vérifications des engagements adapté aux contraintes des locataires déployant leur conteneur.",
        "authors": [
            {
                "email": "bastien.buil@lecnam.net",
                "first": "Bastien",
                "last": "Buil",
                "affiliation": "CNAM \/ Orange"
            },
            {
                "email": "chrystel.gaber@orange.com",
                "first": "Chrystel",
                "last": "Gaber",
                "affiliation": "Orange"
            },
            {
                "email": "samia.bouzefrane@lecnam.net",
                "first": "Samia",
                "last": "Bouzefrane",
                "affiliation": "Cnam"
            },
            {
                "email": "Emmanuel.Baccelli@inria.fr",
                "first": "Emmanuel",
                "last": "Baccelli",
                "affiliation": "Inria"
            }
        ],
        "contacts": [
            {
                "email": "bastien.buil@lecnam.net",
                "first": "Bastien",
                "last": "BUIL",
                "affiliation": "CNAM"
            },
            {
                "email": "chrystel.gaber@orange.com",
                "first": "Chrystel",
                "last": "Gaber",
                "affiliation": "Orange"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740147847
    },
    {
        "pid": 10,
        "title": "Porting the Gysela code on GPU enabled supercomputers",
        "abstract_only": true,
        "abstract": "This study describes the porting of high-dimensional operators on accelerators involving complex memory patterns and memory usage requirements which appear in five dimensional fusion plasma turbulence codes such as GYSELA. Motivated by the gyrokinetic simulations' huge computational needs, the porting of GYSELA, one of the few international 5D gyrokinetic codes able to perform global, full-f and flux-driven simulations, lead to a \"partial rewrite\" of the code with the addition of OpenMP annotations allowing the offloading of the work to a device such as but not limited to, GPUs. We focus on the porting workflow applied to GYSELA, we present the limitations of the programming model implementation used and show that the externalization of a problematic operator and rewriting it in Kokkos is indeed viable and that the return on investment is positive with it being reusable by other codes. A set of benchmarks on the Adastra parallel machine has permitted us to evaluate the performance of the resulting code on a subset of the latest GPU (AMD MI250X), APU (AMD MI300A) and CPU (AMD GENOA) technologies. We discuss the remaining bottlenecks and problems after the porting of the algorithms. Some possible solutions are foreseen.",
        "authors": [
            {
                "email": "malaboeuf@cines.fr",
                "first": "Etienne",
                "last": "Malaboeuf",
                "affiliation": "Cines"
            },
            {
                "email": "mathieu.peybernes@epfl.ch",
                "first": "Mathieu",
                "last": "Peybernes",
                "affiliation": "EPFL"
            },
            {
                "email": "obrejan@cea.fr",
                "first": "Kevin",
                "last": "Obrejan",
                "affiliation": "CEA"
            },
            {
                "email": "grandgirard@cea.fr",
                "first": "Virginie",
                "last": "Grandgirard",
                "affiliation": "CEA"
            }
        ],
        "contacts": [
            {
                "email": "malaboeuf@cines.fr",
                "first": "Etienne",
                "last": "Malaboeuf",
                "affiliation": "CINES"
            }
        ],
        "topics": [
            "Architecture",
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740065493
    },
    {
        "pid": 11,
        "title": "Isolation et Protection des SoC : Les TEE dans les Applications Embarquées",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-8dee85215a37189c892752940a0d20ab6b969301706150208bc824ede1234731",
            "timestamp": 1740143946,
            "size": 39290
        },
        "abstract_only": true,
        "abstract": "Avec le ralentissement de la loi de Moore, les System-on-Chip (SoC) hétérogènes sont de plus en plus présents dans les systèmes embarqués. Ces systèmes regroupent sur une même puce des composants de nature différente, tels que des processeurs, des mémoires, des accélérateurs matériels et des périphériques. Cependant, cette diversité de composants présente un inconvénient majeur pour la sécurité. Chaque composant introduit des vulnérabilités qui lui sont propres, et les interactions entre les composants peuvent également ouvrir de nouveaux vecteurs d'attaques. Aussi, l'effort nécessaire pour assurer la sécurité du SoC augmente avec le nombre de composants qu'il contient. En réponse à ces défis, de nombreux travaux ont proposé des solutions pour renforcer la sécurité, soit au niveau des composants individuels, soit au niveau de l'architecture globale du SoC. Parmi les solutions proposées dans la littérature, une approche très populaire consiste à isoler les composants (logiciels et matériels) du SoC que le concepteur considère comme critiques, du reste des composants du SoC qu'un adversaire pourrait corrompre. Un riche écosystème de systèmes en isolation (Trusted Execution Environments) existe, marqué par une forte dualité entre le domaine académique et le domaine commercial, ce dernier étant dominé par un petit nombre de solutions. Cette dichotomie peut, en partie, s'expliquer par le surcoût induit par le déploiement de ces solutions sur les performances globales du système. Dans cette présentation, nous proposons d'étudier cet écosystème particulier et de comprendre les défis qui limitent l'adoption de nouvelles solutions dans le domaine commercial. Nous proposerons une taxonomie des différentes solutions actuelles en les distinguant sur la base des stratégies d'isolation qu'elles emploient, et nous identifierons les principaux axes de recherche à privilégier pour améliorer l'acceptabilité des nouvelles solutions dans le domaine commercial. En particulier, nous mettrons en avant l'importance de trouver des compromis sécurité\/performance acceptables en fonction des applications.",
        "authors": [
            {
                "email": "simon.baissat.chavent@univ-st-etienne.fr",
                "first": "Simon",
                "last": "Baissat-Chavent",
                "affiliation": "Université Jean-Monnet de ST-Etienne"
            },
            {
                "email": "lilian.bossuet@univ-st-etienne.fr",
                "first": "Lilian",
                "last": "Bossuet",
                "affiliation": "Université Jean-Monnet de ST-Etienne"
            },
            {
                "email": "cedric.killian@univ-st-etienne.fr",
                "first": "Cédric",
                "last": "Killian",
                "affiliation": "Université Jean-Monnet de ST-Etienne"
            }
        ],
        "contacts": [
            {
                "email": "cedric.killian@univ-st-etienne.fr",
                "first": "Cédric",
                "last": "Killian",
                "affiliation": "Université Jean-Monnet de ST-Etienne"
            },
            {
                "email": "simon.baissat.chavent@univ-st-etienne.fr",
                "first": "Simon",
                "last": "Baissat-Chavent",
                "affiliation": "Laboratoire Hubert Curien"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740143946
    },
    {
        "pid": 12,
        "title": "Interruptions en espace utilisateur depuis le réseau BXI pour le recouvrement calcul\/communications",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-a1dccc7ba2d1bddf0426f437f878c09dbd6ea69c1c26d4ffe6a09b07eafd3caf",
            "timestamp": 1740609195,
            "size": 148961
        },
        "abstract_only": true,
        "abstract": "En HPC, les supercalculateurs sont composés de noeuds interconnectés par un réseau haute\r\nperformance, comme par exemple le réseau BXI d’Eviden. La carte BXI exécute la plupart des\r\ntâches de communications par elle-même ; cependant, l’application doit être prévenue de l’arrivée\r\nde messages. Pour celà, elle peut utiliser une scrutation ou des interruptions. Pour maximiser\r\nle temps dédié au calcul, il est préférable d’éviter la scrutation. Un mécanisme d’interruptions\r\nqui traite les évènements puis retourne au calcul évite les appels inutiles. En revanche, les\r\nréseaux haute performance se programment depuis l’espace utilisateur pour éviter le coût des\r\nappels systèmes. Or, un mécanisme d’interruption classique passe par le noyau, ce que l’on\r\ncherche à éviter. Le mécanisme d’interruptions en espace utilisateur est disponible sur certains\r\nprocesseurs récents. Cependant, seul l’envoi entre deux coeurs est disponible pour l’instant.\r\nNotre contribution est un mécanisme qui permet de lever une interruption en espace utilisateur\r\nà partir d’un périphérique PCIe. Pour ce faire, nous utilisons le mécanisme interrupt-posting\r\nde l’IOMMU, qui n’est pas prévue pour les interruptions en espace utilisateur. Nous avons\r\ndétourné le mécanisme pour transformer des interruptions classiques en interruptions en espace\r\nutilisateur. Nous avons également adapté la bibliothèque Portals ainsi que le module noyau BXI\r\npour y ajouter le support des UINTR.\r\nLa latence est de 1.5µs pour la scrutation, de 3.6µs avec les interruptions en espace utilisateur et\r\nde 9µs avec les interruptions classiques. Pour le recouvrement des communications par du calcul,\r\nnous avons étendu la suite de bench overlap pour ajouter un support direct des opérations de\r\nla spécification Portals 4, avec un mécanisme de rendez-vous basique simulant un protocole\r\nMPI. Nous observons un recouvrement mauvais en scrutation, et presque parfait avec les UINTR.\r\nLa suite de nos travaux porte sur l’intégration des interruptions en espace utilisateur dans la\r\nbibliothèque de communications NewMadeleine.",
        "authors": [
            {
                "email": "charles.goedefroit@inria.fr",
                "first": "Charles",
                "last": "Goedefroit",
                "affiliation": "Eviden, Univ. Bordeaux"
            },
            {
                "email": "Alexandre.Denis@inria.fr",
                "first": "Alexandre",
                "last": "Denis",
                "affiliation": "Centre Inria de l'université de Bordeaux"
            },
            {
                "email": "Brice.Goglin@inria.fr",
                "first": "Brice",
                "last": "Goglin",
                "affiliation": "Centre Inria de l'université de Bordeaux"
            },
            {
                "email": "Mathieu.Barbe@eviden.com",
                "first": "Mathieu",
                "last": "Barbe",
                "affiliation": "Eviden"
            },
            {
                "email": "Gregoire.Pichon@eviden.com",
                "first": "Grégoire",
                "last": "Pichon",
                "affiliation": "Eviden"
            }
        ],
        "contacts": [
            {
                "email": "Alexandre.Denis@inria.fr",
                "first": "Alexandre",
                "last": "Denis",
                "affiliation": "Inria"
            },
            {
                "email": "charles.goedefroit@inria.fr",
                "first": "Charles",
                "last": "Goedefroit",
                "affiliation": "Eviden, Univ. Bordeaux"
            }
        ],
        "topics": [
            "Parallélisme",
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740147934
    },
    {
        "pid": 13,
        "title": "Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-816141e919a26a06dbb4dddccdd78266aff5c626c909d99a37a09b2bed62398d",
            "timestamp": 1740406137,
            "size": 444075
        },
        "abstract_only": false,
        "abstract": "The rapid growth of global data volumes has created a demand for scalable distributed systems that can maintain a high quality of service. Data replication is a widely used technique that provides fault tolerance, improved performance and higher availability. Traditional implementations often rely on threshold-based activation mechanisms, which can vary depending on workload changes and system architecture. System administrators typically bear the responsibility of adjusting these thresholds. To address this challenge, reinforcement learning can be used to dynamically adapt to workload changes and different architectures. In this paper, we propose a novel data replication strategy for cloud systems that employs reinforcement learning to automatically learn system characteristics and adapt to workload changes. We present the architecture behind our solution and describe the reinforcement learning model by defining the states, actions and rewards.",
        "authors": [
            {
                "email": "amir.najjar@irit.fr",
                "first": "Amir",
                "last": "Najjar",
                "affiliation": "Institut de recherche en informatique de Toulouse"
            },
            {
                "email": "riad.mokadem@irit.fr",
                "first": "Riad",
                "last": "Mokadem",
                "affiliation": "Institut de recherche en informatique de Toulouse"
            },
            {
                "email": "jean-marc.pierson@irit.fr",
                "first": "Jean-Marc",
                "last": "Pierson",
                "affiliation": "Institut de recherche en informatique de Toulouse"
            }
        ],
        "contacts": [
            {
                "email": "amir.najjar@irit.fr",
                "first": "Amir",
                "last": "Najjar",
                "affiliation": "Institut de recherche en informatique de Toulouse"
            },
            {
                "email": "riad.mokadem@irit.fr",
                "first": "Riad",
                "last": "Mokadem",
                "affiliation": "Institut de recherche en informatique de Toulouse"
            },
            {
                "email": "jean-marc.pierson@irit.fr",
                "first": "Jean-Marc",
                "last": "Pierson",
                "affiliation": "Institut de recherche en informatique de Toulouse"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740647367
    },
    {
        "pid": 14,
        "title": "AI Optimization on embedded systems",
        "abstract_only": true,
        "abstract": "Recent advancements have shown the importance of embedding Artificial Intelligence (AI) models into devices with constraints on size, power, and processing capabilities. This integration aims to develop intelligent and efficient technologies that operate within the limitations of everyday devices. Consequently, the hyperparameters for designing these models must be carefully selected to balance prediction efficiency, embeddability, and operational requirements.\r\n\r\nTraditional hyperparameter selection methods prioritize prediction efficiency without considering resource constraints, often resulting in complex models that require more hardware resources. While some recent studies have suggested optimizing models for specific hardware environments, the software and hardware design processes remain largely disconnected.\r\n\r\nIn our research, we propose a compreh8ensive approach that optimizes model selection and parameterization using conventional software tools like TensorFlow from the design phase itself, incorporating hardware and operational constraints. We explore both static and dynamic design phases to meet operational constraints such as bandwidth and latency. The static approach assumes consistent inference for all samples, whereas the dynamic approach adapts based on operational constraints.\r\n\r\nOur first contribution involves hyperparameter search and optimization for embedded targets. This includes selecting training parameters and architecture parameters, validated on FPGAs with Random Forest, Neural Networks, and Convolutional Neural Networks. We implement a 2-step optimization method for FPGAs, predicting resources like latency, energy, and hardware usage using polynomial regressors derived from initial implementation trials. Metaheuristics like Bayesian optimization and genetic algorithms are employed to optimize parameters, generating a set of solutions visualized via a Pareto front for model selection. A secondary “low-level” optimization refines implementation parameters.\r\nThe second contribution focuses on dynamic adaptation techniques for real-time constraints, such as resource availability or energy consumption. By leveraging techniques like early exits and gated inference mechanisms, we enable adaptive inference, tailoring the inference phase dynamically based on external factors or contextual samples.\r\nWhile our static optimization method has been validated across several models and constraints, our dynamic method is in the implementation phase, necessitating further experiments to confirm its theoretical benefits. Overall, our work demonstrates promising advancements in embedding AI models in constrained environments, with potential for broader applications across various AI and hardware targets.",
        "authors": [
            {
                "email": "druart@univ-brest.fr",
                "first": "Kevin",
                "last": "Druart",
                "affiliation": "UBO\/Lab-STICC\/Thales DMS"
            },
            {
                "email": "espes@univ-brest.fr",
                "first": "David",
                "last": "Espes",
                "affiliation": "UBO\/Lab-STICC"
            },
            {
                "email": "dezan@univ-brest.fr",
                "first": "Catherine",
                "last": "Dezan",
                "affiliation": "UBO\/Lab-STICC"
            },
            {
                "email": "alain.deturche@fr.thalesgroup.com",
                "first": "Alain",
                "last": "Deturche",
                "affiliation": "Thales DMS"
            }
        ],
        "contacts": [
            {
                "email": "dezan@univ-brest.fr",
                "first": "Catherine",
                "last": "Dezan",
                "affiliation": "Université de Bretagne Occidentale - Brest"
            },
            {
                "email": "druart@univ-brest.fr",
                "first": "Kevin",
                "last": "Druart",
                "affiliation": "UBO\/Lab-STICC\/Thales DMS"
            },
            {
                "email": "espes@univ-brest.fr",
                "first": "David",
                "last": "Espes",
                "affiliation": "UBO\/Lab-STICC"
            }
        ],
        "topics": [
            "Architecture",
            "Système"
        ],
        "format": "Poster seul, sans présentation",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740490599
    },
    {
        "pid": 15,
        "title": "ArmoniK : une solution open source pour l’orchestration et la distribution de calculs",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-b159108d12262bf8169ab84afee4cf5e53cf9d696749cfdcb5d2d1f69d9a83cd",
            "timestamp": 1740663073,
            "size": 41536
        },
        "abstract_only": true,
        "abstract": "Dans un contexte où les besoins en calcul haute performance (HPC) et en traitement massif des données ne cessent de croître, ArmoniK est une plateforme open-source conçue pour l’exécution efficace et évolutive de charges de travail parallèles. Compatible avec des infrastructures hétérogènes, elle vise à simplifier le développement et le déploiement industriel de calculs distribués tout en optimisant l’utilisation des ressources, qu’il s’agisse de clouds publics ou privés, et bientôt, de clusters HPC. ArmoniK s'applique à permettre aux utilisateurs de se concentrer sur le développement de leurs applications sans se soucier de la complexité sous-jacente liée à l’exécution distribuée.\r\n\r\nL’orchestrateur de calcul ArmoniK propose nativemment la distribution dynamique de graphes de tâches de calcul et de leurs données associées, la tolérance aux pannes, l’élasticité, la portabilité et l’observabilité. Il permet d’exécuter des charges de travail parallèles sous la forme de tâches indépendantes ou dépendantes, en gérant automatiquement leur ordonnancement et leur distribution sur les ressources disponibles. La gestion des échecs est intégrée à travers des mécanismes de reprise automatique des tâches en cas de panne, garantissant une exécution fiable et robuste. Grâce à une gestion fine de l’orchestration des tâches et à son architecture élastique, ArmoniK assure une mise à l’échelle dynamique et optimale, s’adaptant aux variations de charge tout en maintenant un haut niveau de performance, assurant ainsi une scalabilité horizontale. Reposant sur une architecture modulaire et orientée microservices, ArmoniK propose un modèle de distribution qui maximise l’exploitation des ressources disponibles tout en réduisant la latence. De plus, la plateforme offre des SDKs en plusieurs langages (C++, Python, Rust, C#, Java) pour simplifier le développement d’applications distribuées.\r\n\r\nCette présentation détaillera les principes architecturaux d’ArmoniK, ses performances sur différents cas d’usage et les perspectives d’évolution.\r\n\r\nArmoniK s’inscrit ainsi dans une démarche visant à répondre aux défis liés à la montée en charge, à l’optimisation des ressources et à la gestion efficace des calculs distribués dans des environnements complexes. En offrant une abstraction de haut niveau, la plateforme facilite le développement et le déploiement d’applications de calcul intensif à large échelle, tout en garantissant une utilisation optimale des infrastructures sous-jacentes.",
        "authors": [
            {
                "email": "jgurhem@aneo.fr",
                "first": "Jérôme",
                "last": "Gurhem",
                "affiliation": "Aneo"
            },
            {
                "email": "wkirschenmann@aneo.fr",
                "first": "Wilfried",
                "last": "Kirschenmann",
                "affiliation": "Aneo"
            }
        ],
        "contacts": [
            {
                "email": "jerome.gurhem@hotmail.fr",
                "first": "Jérôme",
                "last": "Gurhem",
                "affiliation": "Aneo"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740663073
    },
    {
        "pid": 16,
        "title": "Vers un système de réservation de ressources basée sur NDN pour le computing continuum",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-26fe786a0d91aca4ed65c778f7efda6ee5e364c7c0c288df6c0aff024d0c19da",
            "timestamp": 1740651822,
            "size": 1183058
        },
        "abstract_only": false,
        "abstract": "Cet article présente NRN, un système de réservation de ressources permettant à l'utilisateur de formuler des besoins en ressources. Chaque ressource disponible (CPU, RAM, GPU, etc.) reçoit un nom unique selon le modèle NDN. NRN est un système de réservation de bout en bout qui fournit un moyen de découvrir des ressources à travers le réseau et de formuler des exigences réseau pour atteindre la ressource réservée.\r\nIl tire parti du contexte du computing continuum et est capable d'exploiter des ressources n'importe où dans le réseau, que ce soit des équipements situés à l'edge, des infrastructures cloud ou des équipements de réseau.",
        "authors": [
            {
                "email": "thierry.arrabal@insa-lyon.fr",
                "first": "Thierry",
                "last": "ARRABAL",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "julien.caposiena@insa-lyon.fr",
                "first": "Julien",
                "last": "CAPOSIENA",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "frederic.le-mouel@insa-lyon.fr",
                "first": "Frédéric",
                "last": "LE MOUËL",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "stephane.frenot@insa-lyon.fr",
                "first": "Stéphane",
                "last": "FRÉNOT",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            }
        ],
        "contacts": [
            {
                "email": "thierry.arrabal@insa-lyon.fr",
                "first": "Thierry",
                "last": "ARRABAL",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "julien.caposiena@insa-lyon.fr",
                "first": "Julien",
                "last": "CAPOSIENA",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "frederic.le-mouel@insa-lyon.fr",
                "first": "Fredéric",
                "last": "Le Mouël",
                "affiliation": "INSA Lyon, Laboratoire CITI"
            }
        ],
        "topics": [
            "Architecture",
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740473697
    },
    {
        "pid": 17,
        "title": "Distributed Topological Data Analysis with TTK and MPI",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-bbc15e95d6b8a8af30737b18d23f0ac292b84ff910b0e53d15687ab98d61e285",
            "timestamp": 1740560713,
            "size": 46724
        },
        "abstract_only": true,
        "abstract": "Topological Data Analysis (TDA) is an ensemble of methods that extract characteristics of a 2D or 3D dataset in order to facilitate its representation and analysis. Applications include a variety of domains such as fluid dynamics, chemistry or astrophysics. The Topology ToolKit (TTK) is an open-source framework that implements a number of those TDA methods to allow for efficient and robust computations. However, up until now, TTK has only been parallelized in a shared-memory context, limiting the size of the datasets it can process.\r\nIn this talk, we will present the results of our paper, \"TTK is Getting MPI-Ready\". \r\nIt documents the technical foundations for the extension of TTK to distributed-memory parallelism with the Message Passing Interface (MPI). While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe a versatile approach for the support of topological analysis pipelines, i.e. a sequence of topological algorithms interacting together. While developing this extension, we faced several algorithmic and software engineering challenges, which we documented. We describe an MPI extension of TTK's data structure for triangulation representation and traversal, a central component to the global performance and generality of TTK's topological implementations. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Performance analyses show that parallel efficiencies range from 20% to 80% (depending on the algorithms), and that the MPI-specific preconditioning introduced by our framework induces a negligible computation time overhead. We illustrate the new distributed-memory capabilities of TTK with an example of advanced analysis pipeline, combining multiple algorithms, run on the largest publicly available dataset we have found (120 billion vertices) on a cluster with 64 nodes (for a total of 1536 cores). We also provide a roadmap for the completion of TTK's MPI extension, along with generic recommendations for each algorithm communication category. Finally, we will present our latest work to port the computation of a more complex topological representation to a distributed-memory context.",
        "authors": [
            {
                "email": "eve.le-guillou@lip6.fr",
                "first": "Eve",
                "last": "Le Guillou",
                "affiliation": "Sorbonne Université - CNRS"
            }
        ],
        "contacts": [
            {
                "email": "eve.le-guillou@lip6.fr",
                "first": "Eve",
                "last": "Le Guillou",
                "affiliation": "Sorbonne Université - CNRS"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740560713
    },
    {
        "pid": 18,
        "title": "Scalable Domain Decomposition Methods for Large Sparse Linear Systems on Modern Architectures",
        "abstract_only": true,
        "abstract": "In this work, we address the challenge of scaling domain decomposition\r\nmethods for solving large sparse linear systems on modern\r\narchitectures. We leverage the Composyx software framework,\r\nintegrating state-of-the-art libraries such as BLAS\/LAPACK for dense\r\nlinear algebra, direct sparse solvers for subproblem resolution, and\r\nefficient partitioners for domain decomposition. Ensuring seamless\r\nintegration and optimal resource utilization is critical for\r\ncomputational efficiency.\r\n\r\nWe first optimize these methods for homogeneous multi-core systems,\r\nrefining each algorithmic stage to minimize overhead and fully exploit\r\nshared memory within each domain. A key aspect of this optimization is\r\nensuring that each library efficiently utilizes multi-core\r\narchitectures through Composyx, both independently and within a shared\r\nexecution context. Additionally, proper thread and process placement\r\nis crucial, ensuring that each process is bound to a specific portion\r\nof the machine for optimal performance. To better understand the\r\neffectiveness of these optimizations, we will conduct an in-depth\r\nanalysis of the trade-offs between computational and numerical\r\nfactors, specifically the number of cores assigned per domain versus\r\nthe number of domains. The goal is to identify the best configuration\r\nthat balances parallel efficiency, numerical accuracy, and overall\r\nsolver performance. This study will provide valuable insights into how\r\ndomain decomposition methods can be adapted to different problem sizes\r\nand hardware configurations, ensuring robustness and scalability.\r\nBuilding on this foundation, ongoing work focuses on extending our\r\nframework to heterogeneous CPU-GPU architectures through a\r\nfine-grained task-based parallelization strategy.\r\n\r\nBy addressing these challenges related to increasing platform\r\nheterogeneity and algorithmic complexity, this work seeks to ensure\r\nthat domain decomposition methods remain robust, scalable, and\r\nwell-suited to the computational demands of large-scale simulations in\r\nthe exascale era.",
        "authors": [
            {
                "email": "stojche.nakov@inria.fr",
                "first": "Stojche",
                "last": "Nakov",
                "affiliation": "Inria"
            },
            {
                "email": "emmanuel.agullo@inria.fr",
                "first": "Emmanuel",
                "last": "Agullo",
                "affiliation": "Inria"
            },
            {
                "email": "luc.giraud@inria.fr",
                "first": "Luc",
                "last": "Giraud",
                "affiliation": "Inria"
            },
            {
                "email": "gilles.marait@inria.fr",
                "first": "Gilles",
                "last": "Marait",
                "affiliation": "Inria"
            },
            {
                "email": "alfredo.buttari@irit.fr",
                "first": "Alfredo",
                "last": "Buttari",
                "affiliation": "Irit"
            }
        ],
        "contacts": [
            {
                "email": "stojche.nakov@inria.fr",
                "first": "Stojche",
                "last": "Nakov",
                "affiliation": "Inria"
            },
            {
                "email": "emmanuel.agullo@inria.fr",
                "first": "Emmanuel",
                "last": "Agullo",
                "affiliation": "Inria"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740584991
    },
    {
        "pid": 19,
        "title": "Increasing parallelism through software decomposition of individual-based simulations : A case study of the Aevol genome evolution simulator",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-5e9bca6a701d9e639f06a2b1e63160ec311cc976e234cce51df6115f7fab2a26",
            "timestamp": 1740650670,
            "size": 56058
        },
        "abstract_only": true,
        "abstract": "Individual-based simulation models are widely spread and have applications ranging from the bio-medical domain to finance and computer networking. Such models expose easy parallelism when most of the computational work is independent between individuals. But heterogeneity in workload between individuals lead to scheduling irregularities, that need to be addressed.\r\n    \r\nInstead of improving scheduling, it can be worth to expose inner parallelism in individuals, but it needs to break fairly complex and intertwined sequential processes. \r\nWe propose a new programming model, that tries to address this issue, by separating the inherent computational bulk from the sequential work. Specifically, it adopts a two-tiered architecture, with a symbols extraction phase, followed by a parsing automaton, which concentrates the model logic. The first phase is the main focus of this paper, and a highly-parallel section. All of the sequential work is gathered in the second part, thus reducing the critical path.\r\n    \r\nThe increase in parallelism coupled with a minimization of sequential sections allowed this software model to be ported successfully to GPU, achieving significant speedup for some simulation parameters, and allowed to run several orders of magnitude larger simulations in comparable amounts of time, as opposed to an individually-based parallel implementation. \r\nWe evaluate the effectiveness of this new architecture through a series of experiments, based on an implementation of the Aevol model, an individual-based simulation software modeling genome evolution. Experiments were conducted on various platforms, including state-of-the-art GPUs and many-core CPU architectures.",
        "authors": [
            {
                "email": "romain.galle@inria.fr",
                "first": "Romain",
                "last": "Gallé",
                "affiliation": "Inria \/ LIRIS \/ CITI"
            },
            {
                "email": "jonathan.rouzaud-cornabas@inria.fr",
                "first": "Jonathan",
                "last": "Rouzaud-Cornabas",
                "affiliation": "INSA de Lyon \/ LIRIS \/ CITI"
            },
            {
                "email": "thierry.gautier@inrialpes.fr",
                "first": "Thierry",
                "last": "Gautier",
                "affiliation": "Inria \/ LIP"
            }
        ],
        "contacts": [
            {
                "email": "Thierry.Gautier@inrialpes.fr",
                "first": "Thierry",
                "last": "Gautier",
                "affiliation": "INRIA - LIP - Lyon"
            },
            {
                "email": "romain.galle@inria.fr",
                "first": "Romain",
                "last": "Gallé",
                "affiliation": "Inria \/ LIRIS \/ CITI"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740650670
    },
    {
        "pid": 20,
        "title": "Exa-AToW data logistics framework for cross-facility scientific workflows",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-b9e62fd7461f6fbb57e9c7f6da9ce2892596dd08ac0edca350b742ff65e294f2",
            "timestamp": 1741086781,
            "size": 73213
        },
        "abstract_only": true,
        "abstract": "The Exa-AToW [1] vision defines large-scale workflows, also called cross-facility [8] or multi-\r\nfacility [10] workflows, as portable scientific processes designed to operate across distributed\r\nand heterogeneous infrastructures in order to simplify and automate complex data processing\r\nand analysis tasks. Large-scale workflows can be seen as a collaborative system-of-systems, an\r\nassembly of independent systems that voluntarily interact to achieve shared objectives. One of\r\nthe known problems in this context is data logistics, which refers to the process of managing\r\nthe data flow within a federation, including its collection, storage, transmission, analysis, and\r\nsecurity. To address this issue, we propose an approach of Ephemeral Buffers (EB) inspired\r\nfrom the concepts of Cybercosm [3] and the Hourglass model [4].\r\nWe assume a workflow representation such as, but not restricted to, the Common Workflow\r\nLanguage (CWL) [6]. Data logistics computation follows the workflow’s task allocations to the\r\ninfrastructures (HPC centers, data centers, Cloud, etc.). It decides origin-destination of the data\r\ntransfers and when to perform them. This process can be iteratively applied to optimize the\r\nworkflow orchestration.\r\nEBs are designed to be a straightforward and lightweight data transfer mechanism for data\r\nstaging, data retrieval, and dynamic buffer allocation. Following the Hourglass model prin-\r\nciples, they are designed to make them easy to deploy across the continuum with minimal\r\ncybersecurity risks. In [5], a similar approach is used, but EBs are extended to cross-facilities\r\ndeployment. EBs are defined by the following key properties: system-dependent maximum\r\nsize, time-limited availability, finite availability, a Unique IDentifier (UID), a set of mandatory\r\nmetadata and minimized access right.\r\nIn this presentation we will show how EBs can be used to deploy the radio astronomy data\r\nprocessing tool, DDF Pipeline [8, 9]. DDF Pipeline is developed to address the challenge of\r\nthe SKA [7] next generation radio telescope that faces unprecedented, massive data processing\r\nissues. We present an implementation of the approach on the national computing infrastruc-\r\nture of GENCI [2], the French national agency that gathers supercomputing facilities and data\r\ncenters from the SKA communities.",
        "authors": [
            {
                "email": "mathis.certenais@irisa.fr",
                "first": "Mathis",
                "last": "Certenais",
                "affiliation": "Université de Rennes"
            },
            {
                "email": "francois.bodin@irisa.fr",
                "first": "Francois",
                "last": "Bodin",
                "affiliation": "Université de Rennes"
            }
        ],
        "contacts": [
            {
                "email": "mathis.certenais@irisa.fr",
                "first": "Mathis",
                "last": "Certenais",
                "affiliation": "Université de Rennes"
            },
            {
                "email": "francois.bodin@irisa.fr",
                "first": "Francois",
                "last": "Bodin",
                "affiliation": "Université de Rennes"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740661326
    },
    {
        "pid": 22,
        "title": "Robust and Efficient Task Allocation in Large-Scale Distributed Systems: Bridging Theory and Practice",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-694ee0711edca90e6d67577701853d6c50ad20b6fd71c1adf7764096b373a2b6",
            "timestamp": 1741860758,
            "size": 41141
        },
        "abstract_only": true,
        "abstract": "Task allocation is a fundamental problem in distributed computing, requiring an efficient and dynamic assignment of tasks to computational processes over time. In large-scale cloud-based infrastructures, real-world task allocation systems face challenges such as fluctuating resource availability, numerous failures, and user-driven task cancellations. Ensuring both correctness and robustness while achieving high performance at scale remains a critical requirement for industrial applications.  \r\n\r\nOur research aims to develop a methodology for specifying, verifying and implementing distributed task allocation algorithms that bridge the gap between theoretical models and practical deployment. We demonstrate this approach through extensive cloud-based experiments leveraging ArmoniK, an industrial-grade open-source framework for executing dynamic task graphs on fault-prone elastic infrastructures. \r\n\r\nThis research forms the foundation of an ongoing  PhD thesis. This presentation provides an overview of the problem space and a roadmap for future work.",
        "authors": [
            {
                "email": "qdelamea@aneo.fr",
                "first": "Quentin",
                "last": "Delamea",
                "affiliation": "LISN, Aneo"
            },
            {
                "email": "burman@lisn.fr",
                "first": "Janna",
                "last": "Burman",
                "affiliation": "LISN"
            },
            {
                "email": "jgurhem@aneo.fr",
                "first": "Jérôme",
                "last": "Gurhem",
                "affiliation": "Aneo"
            },
            {
                "email": "stephane.vialle@centralesupelec.fr",
                "first": "Stéphane",
                "last": "Vialle",
                "affiliation": "LISN, CentraleSupélec"
            }
        ],
        "contacts": [
            {
                "email": "qdelamea@aneo.fr",
                "first": "Quentin",
                "last": "Delamea",
                "affiliation": "LISN, Aneo"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741860758
    },
    {
        "pid": 23,
        "title": "Architecting Value Prediction around In-Order Prediction",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-0c7ea6ba8af6f163f41debec8886a00302b5d66afe66a15049a7d5a3c85464d2",
            "timestamp": 1740663867,
            "size": 64716
        },
        "abstract_only": true,
        "abstract": "In the search for performance, in-order execution cannot expect to prevail as older long latency instructions prevent younger ones from issuing. Although stall-on-use processors allow independent instructions to issue in the shadow of a cache miss, the compiler cannot always find enough independent work to keep pipeline resources busy. In this paper, we study how both value prediction based on address prediction and direct value prediction can be built into an in-order pipeline to unlock significant performance. We further show that the in-order execution property provides advantages in that the pipeline may speculate aggressively without suffering from any recovery penalty. Finally, we combine this data speculation infrastructure with a reworked cache hierarchy that relies on a fast first level cache that can be written speculatively. We  show that such an in-order pipeline can reach a performance level that is comparable to an equally -- although moderately -- wide  out-of-order processor, without requiring support for partial out-of-order execution such as out-of-order memory hazard handling or full-fledged register renaming.\r\nOverall, we increase the performance of a 32-entry scoreboard, 4-issue in-order processor based on a scaled up Open Hardware Group CVA6 by 38.4% (geomean), achieving 86.7% and 46.3%  of the gains brought by comparable out-of-order processors featuring 32\/16-entry and 64\/32-entry Reorder Buffer and scheduler, respectively",
        "authors": [
            {
                "email": "Pierre.Ravenel@univ-grenoble-alpes.fr",
                "first": "Pierre",
                "last": "Ravenel",
                "affiliation": "TIMA \/ Kalray"
            },
            {
                "email": "Arthur.Perais@univ-grenoble-alpes.fr",
                "first": "Arthur",
                "last": "Perais",
                "affiliation": "TIMA \/ CNRS"
            },
            {
                "email": "Frederic.Petrot@univ-grenoble-alpes.fr",
                "first": "Frédéric",
                "last": "Pétrot",
                "affiliation": "TIMA \/ GINP"
            },
            {
                "email": "bddinechin@kalrayinc.com",
                "first": "Benoît Dupont",
                "last": "de Dinechin",
                "affiliation": "Kalray"
            }
        ],
        "contacts": [
            {
                "email": "Arthur.Perais@univ-grenoble-alpes.fr",
                "first": "Arthur",
                "last": "Perais",
                "affiliation": "TIMA - Grenoble"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740663809
    },
    {
        "pid": 26,
        "title": "Simuler rocHPL pour prédire la performance des supercalculateurs",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-fe8e6d97cb302b458963a7d7601a612476029b5b52db2ae94cd8450e16e1cc4b",
            "timestamp": 1740670441,
            "size": 63066
        },
        "abstract_only": true,
        "abstract": "Le HPL est le benchmark le plus couramment utilisé pour évaluer les\r\n  performances des supercalculateurs. En phase de\r\n  conception et de dimensionnement de très grands clusters, il est\r\n  donc primordial de prédire la performance du HPL pour des\r\n  architectures matérielles hétérogènes (CPU\/GPU) et un réseau\r\n  d'interconnexion optimisé.\r\n  \r\n  Dans ce cadre, la bibliothèque logicielle SimGrid\r\n  apporte un des éléments de réponse (simulation du réseau). Par\r\n  exemple, Tom Cornebize, lors de sa thèse, a ainsi\r\n  pu émuler la version CPU de référence du HPL.\r\n  \r\n  Cependant, les supercalculateurs les plus puissants et les plus\r\n  efficaces sont basés sur des noeuds de calcul CPU\/GPU, pour lesquels\r\n  les constructeurs proposent des implémentations optimisées telles\r\n  que le rocHPL pour les GPU AMD.\r\n\r\n\r\n  Notre approche repose sur une simulation à gros grains du rocHPL combinant\r\n  une reconstruction haut-niveau de l'algorithme, des modèles de\r\n  performance pour les noyaux de calcul (BLAS), et SimGrid pour la\r\n  simulation des communications.\r\n  \r\n  Nous avons validé notre méthodologie et le simulateur par une série\r\n  de comparaisons entre exécutions réelles et simulées.\r\n  \r\n  Des optimisations ont permis de réduire le coût des simulations à\r\n  grande échelle, ce qui permet de restituer les performances des\r\n  plus grands systèmes AMD du TOP500. Cette approche vise à être\r\n  étendue à des applications de calcul différentes du HPL et sur des\r\n  architectures matérielles autres que AMD.",
        "authors": [
            {
                "email": "jean.conan@eviden.com",
                "first": "Jean",
                "last": "Conan",
                "affiliation": "EVIDEN, Univ. Bordeaux, Inria, LaBRI"
            },
            {
                "email": "abdou.guermouche@u-bordeaux.fr",
                "first": "Abdou",
                "last": "Germouche",
                "affiliation": "Univ. Bordeaux, Inria, LaBRI"
            },
            {
                "email": "arnaud.legrand@imag.fr",
                "first": "Arnaud",
                "last": "Legrand",
                "affiliation": "Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG"
            },
            {
                "email": "louis.poirel@eviden.com",
                "first": "Louis",
                "last": "Poirel",
                "affiliation": "EVIDEN"
            }
        ],
        "contacts": [
            {
                "email": "jean.conan@eviden.com",
                "first": "Jean",
                "last": "Conan",
                "affiliation": "EVIDEN & Inria & LaBRI"
            }
        ],
        "topics": [
            "Parallélisme",
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740749507
    },
    {
        "pid": 27,
        "title": "ATOMEC : Quand les mobiles deviennent le moteur du Edge Computing",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-3ce67de78d473f61cb911e54cff3b68ca2d9be259f86653797acda646f396e0d",
            "timestamp": 1740668569,
            "size": 50024
        },
        "abstract_only": true,
        "abstract": "L’essor des dispositifs mobiles connectés – qu’il s’agisse de smartphones, de véhicules intelligents, de montres connectées ou d’autres objets communicants – ouvre de nouvelles perspectives pour l’infrastructure réseau. Traditionnellement sous-exploités, ces appareils disposent pourtant de ressources de calcul, de stockage et d’énergie qui pourraient renforcer le Multi-Access Edge Computing (MEC).\r\n\r\nNous explorons ainsi une approche innovante visant à étendre la capacité du MEC en tirant parti des ressources disponibles des dispositifs mobiles au-delà de la bordure réseau. L’objectif est d’optimiser l’allocation des tâches en identifiant les appareils capables de traiter des calculs de manière distribuée, tout en tenant compte des contraintes liées à leur mobilité et à la fluctuation de leurs ressources. Pour exploiter efficacement ces nœuds mobiles, le MEC doit être capable d’évaluer dynamiquement son environnement, notamment la disponibilité des ressources, les capacités de calcul, le stockage et l’autonomie énergétique.\r\n\r\nNous proposons ATOMEC (Adaptive Task Offloading for Mobile Edge Computing), un algorithme basé sur l’intelligence artificielle, conçu pour optimiser la sélection des nœuds et l’affectation des tâches en fonction de plusieurs paramètres. Notre approche repose sur deux axes principaux : un algorithme de classification permettant d’identifier avec précision les dispositifs connectés tels que les drones, les smartphones et les voitures grâce à des techniques avancées d’apprentissage automatique, afin d’évaluer leurs caractéristiques et leurs ressources de calcul, ainsi qu’un algorithme décisionnel chargé d’orchestrer efficacement le déchargement des tâches pour assurer une répartition optimale de la charge de travail.\r\n\r\nCette approche vise à renforcer la flexibilité et la puissance de calcul du MEC à grande échelle, tout en garantissant une gestion dynamique et optimale des ressources mobiles inexploitées.",
        "authors": [
            {
                "email": "sadia.khizar@capgemini.com",
                "first": "Sadia",
                "last": "Khizar",
                "affiliation": "\"None\""
            },
            {
                "email": "antoine.mallet@capgemini.com",
                "first": "Antoine",
                "last": "Mallet"
            }
        ],
        "contacts": [
            {
                "email": "sadia.khizar@capgemini.com",
                "first": "Sadia",
                "last": "Khizar",
                "affiliation": "\"None\""
            }
        ],
        "topics": [
            "Architecture",
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740668100
    },
    {
        "pid": 28,
        "title": "Distribution de l’électronique de contrôle des Qubits dans et en dehors du cryostat",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-7b24f33ab3b23789ee8e214004a342518b06d6ba887e17e1cc02e6e7c6dca35b",
            "timestamp": 1740732293,
            "size": 46610
        },
        "abstract_only": true,
        "abstract": "Les ordinateurs quantiques promettent une accélération exponentielle de certains algorithmes\r\ncombinatoires par rapport aux ordinateurs classiques, ce qui permettrait la résolution de problèmes\r\ninatteignables par un ordinateur classique. Un calculateur quantique opère sur des qubits,\r\nun dispositif qui peut être dans un état superposé et intriqué avec d’autres qubits. Il est\r\ngénéralement accepté qu’un ordinateur quantique utile devra intégrer au moins un million de\r\nqubits physiques, compte-tenu des erreurs inhérentes à leur manipulation.\r\n\r\nUn qubit nécessite environ une dizaine de signaux de contrôle. Il n’est donc pas envisageable\r\nque chaque qubit ait ses propres fils de contrôle et lecture sortant du cryostat. En effet, cela\r\nentraînerait des contraintes mécaniques, des pertes de refroidissement par conduction ainsi\r\nque de la latence dans le contrôle. Une solution pour réduire le nombre fils est de passer tout\r\nou partie de l’électronique de contrôle à froid, auprès des qubits. Cependant, cela rajoute des\r\ncontraintes de consommation et de volume de l’électronique de contrôle.\r\n\r\nLa tendance dans les propositions architecturales récentes est de déplacer les fonctions analogiques\r\n(DAC, amplification) à froid. Cette approche a l’avantage de réduire le bruit, notamment\r\nthermique, mais ne réduit pas nécessairement le nombre de fils entrant dans le cryostat. Pour\r\nce faire, des méthodes de multiplexage ont été développées, mais ces méthodes permettent de\r\nréduire le nombre de fils seulement d’un ordre de grandeur, ce qui reste incompatible avec un\r\nordinateur quantique à un million de qubits.\r\n\r\nIntel et Google ont expérimenté de déplacer la majorité de leur électronique à froid. Leurs\r\narchitectures ne passent cependant pas à l’échelle, car leur consommation est mille fois trop\r\nélevée pour pouvoir mettre un million de qubits dans un cryostat. Si on déplace une partie de\r\ncette électronique à des températures plus élevées dans le cryostat, i.e. des étages différents,\r\nelle devient plus simple à refroidir. Nous proposons donc une approche holistique permettant\r\nd’évaluer le placement approprié des fonctions numériques et analogiques de contrôle des\r\nQubits \"solides\" dans et en dehors du cryostat. Ceci nous permettra également d’explorer des\r\napproches alternatives de contrôle des Qubits.",
        "authors": [
            {
                "email": "gabriel.zerbib@cea.fr",
                "first": "Gabriel",
                "last": "Zerbib",
                "affiliation": "CEA Grenoble"
            }
        ],
        "contacts": [
            {
                "email": "gabriel.zerbib@cea.fr",
                "first": "Gabriel",
                "last": "Zerbib",
                "affiliation": "CEA Grenoble"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740732293
    },
    {
        "pid": 29,
        "title": "Kicking the Firmware Out of the TCB with the Miralis Virtual Firmware Monitor",
        "abstract_only": true,
        "abstract": "The role of firmware has evolved over the past decades. Not only is firmware responsible for discovering, initializing, and monitoring the system’s chipset, board, and devices, but it also acts as the root of trust and plays a leading role in confidential computing. Yet vulnerabilities in the non-security critical part of the firmware have repeatedly led to the compromise of the core TCB of the system\r\n\r\nWe propose an alternative architecture that excludes the non-security critical part of the firmware from the TCB by isolating it within a virtual machine with the introduction of a simple and verifiable virtual firmware monitor.\r\n\r\nWe present the design of Miralis, the first virtual firmware monitor. Miralis can successfully boot Linux with a virtualized, unmodified OpenSBI firmware on multiple RISC-V boards. We demonstrate by construction that the M-mode of the RISC-V architecture meets the Popek & Golberg criteria for classical virtualization, but ARM's EL3 does not. To enhance security, we prove the correctness of major Miralis' subsystems through lightweight formal methods. Finally, our experiments shows that Miralis can be deployed on existing RISC-V hardware, with unmodified, vendor-provided firmawre blobs and with no performance penalty. Further, we demonstrate the creation of enclaves and confidential VMs that exclude the firmware from their TCB.",
        "authors": [
            {
                "email": "charly.castes@epfl.ch",
                "first": "Charly",
                "last": "Castes",
                "affiliation": "EPFL"
            }
        ],
        "contacts": [
            {
                "email": "charly.castes@epfl.ch",
                "first": "Charly",
                "last": "Castes",
                "affiliation": "EPFL"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740732807
    },
    {
        "pid": 30,
        "title": "Pallas: : Un format de trace générique pour des analyses performantes",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-9026983b3e0c83df1bc2ffdb002d2badb7760d8e03f25bd6dc8d89010501e37d",
            "timestamp": 1741982795,
            "size": 39300
        },
        "abstract_only": true,
        "abstract": "Il est difficile d’identifier la ou les sources des problèmes de performances pour les applications\r\nparallèles. En effet, ces problèmes peuvent venir d’une variété de composants de l’application,\r\net nécessitent donc l’analyse de plusieurs systèmes. Les problèmes de performances peuvent\r\nse manifester de plusieurs manières et avoir une grande variété de cause : par exemple, un\r\ndéséquilibrage de charge de calcul peut être la source de longs temps d’attente dans MPI, ou\r\ndes problèmes d’accés concurrents à des ressources peuvent dégrader les performances des\r\nopérations d’I\/O.\r\nLa détection d’un problème de performance passe par l’analyse de l’exécution d’une applica-\r\ntion, et l’utilisation de plusieurs outils et techniques d’analyse de performance. Pour éviter de\r\nfaire de multiples exécutions longues et coûteuses, des outils de tracages sont utilisés. Ils col-\r\nlectent des informations décrivant le comportement de l’application, qu’ils regroupent ensuite\r\ndans ce que l’on appelle une trace. Cette trace peut ensuite être analysée après l’exécution.\r\nCependant, la génération et l’utilisation de trace peut poser plusieurs problèmes. La généra-\r\ntion de la trace peut changer le comportement de l’application, et ainsi changer les analyses.\r\nDe plus, les traces peuvent être composées de milliers de fichiers lourds, surtout pour des ap-\r\nplications utilisant plusieurs milliers de processus. Pour ces applications à grande échelles, les\r\ntraces existant actuellement deviennent ainsi peut pratique à utiliser, à cause de leur taille et de\r\nleur temps d’analyse.\r\nNous proposons un nouveau format de trace, nommé PALLAS. Il s’agit d’un format générique\r\nconçu pour faciliter et accélérer les analyses d’applications à grand échelles. Pendant l’exécu-\r\ntion de l’application, les évènements collectés par PALLASvont être regroupé, et leurs répéti-\r\ntions détectées. Lors de l’écriture sur le disque, PALLASva regrouper les données d’évènements\r\nsimilaires ensemble, ce qui permet de lire ces traces partiellement de manière très efficace. Cette\r\ndétection des répétions de PALLASne dégrade pas significativement la performance des appli-\r\ncations. De plus, les analyses réalisées sur le format PALLASsont plus rapides que des analyses\r\nréalisées sur d’autres formats de trace.\r\nEnfin, nous proposons une API Python permettant de rendre la recherche de problèmes de\r\nperformances plus interactive, permettant de charger des traces de plusieurs dizaines de Go en\r\nquelques secondes sur un ordinateur classique. Nous espérons que cette interactivité permet\r\nà un utilisateur de trouver des problèmes de performance plus facilement et plus rapidement\r\nqu’avec les formats de trace existant déjà actuellement.",
        "authors": [
            {
                "email": "catherine.guelque@telecom-sudparis.eu",
                "first": "Catherine",
                "last": "Guelque",
                "affiliation": "Télécom SudParis"
            },
            {
                "email": "valentin.honore@ensiie.fr",
                "first": "Valentin",
                "last": "Honoré",
                "affiliation": "ENSIEE"
            },
            {
                "email": "francois.trahay@telecom-sudparis.eu",
                "first": "François",
                "last": "Trahay",
                "affiliation": "Télécom SudParis"
            }
        ],
        "contacts": [
            {
                "email": "catherine.guelque@telecom-sudparis.eu",
                "first": "Catherine",
                "last": "Guelque",
                "affiliation": "Télécom SudParis"
            },
            {
                "email": "valentin.honore@ensiie.fr",
                "first": "Valentin",
                "last": "Honoré",
                "affiliation": "ENSIEE"
            },
            {
                "email": "francois.trahay@telecom-sudparis.eu",
                "first": "Francois",
                "last": "Trahay",
                "affiliation": "Télécom SudParis"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740739653
    },
    {
        "pid": 31,
        "title": "Scaling the SOO Global Blackbox Optimizer on a 128-core Architecture",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-c53dda0d446018279cb50ab69c98da5243a1f941bc8ff3cf1bb9bb7fa12353da",
            "timestamp": 1740757971,
            "size": 72291
        },
        "abstract_only": true,
        "abstract": "Blackbox optimization refers to the situation where no analytical knowledge about the problem is available beforehand, which is the case in a number of application fields, e.g., multi-disciplinary design, simulation optimization.\r\nIn this context, the so-called Simultaneous Optimistic Optimization (SOO) algorithm is a deterministic tree-based global optimizer exposing theoretically provable performance guarantees under mild conditions.\r\nSOO handles the search space as a tree. Each node of the tree corresponds to some region of the search space. A leaf of the tree can be split into smaller cells covering the same original space. Since the root of the tree is the whole search space, the leaves of the tree represent a partition of the space.\r\n\r\nWe consider the efficient shared-memory parallelization of SOO on a high-end HPC architecture with dozens of CPU cores. We propose different strategies based on eliciting the possible levels of parallelism underlying the SOO algorithm.\r\nWe first use a naive approach, performing multiple evaluations of the blackbox function in parallel.\r\nEven with some improvements regarding parallel memory allocations and parallel tree manipulation operations, this version does not scale with the number of cores.\r\nWe then leverage a second, less obvious, parallelism level to fully parallelize SOO based on its tree traversals, and allow the threads to swap their traversals in parallel so as to increase parallel efficiency. By contrast, this version is able to provide substantial improvements in terms of scalability and performance.\r\n\r\nWe validate our strategies with a detailed performance analysis on a compute server with two 64-core processors, using a number of diverse benchmark functions with both increasing dimensions and number of cores. The traversal-based version can offer (in average over the experimented functions) a 3.2× performance gain over the evaluation-based version. The traversal-based version can also benefit from the SMT feature of the CPU cores, providing parallel speedups up to 174.8 on 128 cores.\r\n\r\nWe will also present ongoing work on the extension of these versions to distributed-memory architectures, where we have designed an alternative fully asynchronous version of SOO.",
        "authors": [
            {
                "email": "david.redon@univ-lille.fr",
                "first": "David",
                "last": "Redon",
                "affiliation": "Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            },
            {
                "email": "bilel.derbel@univ-lille.fr",
                "first": "Bilel",
                "last": "Derbel",
                "affiliation": "Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            },
            {
                "email": "pierre.fortin@univ-lille.fr",
                "first": "Pierre",
                "last": "Fortin",
                "affiliation": "Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            }
        ],
        "contacts": [
            {
                "email": "david.redon@univ-lille.fr",
                "first": "David",
                "last": "Redon",
                "affiliation": "Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740757971
    },
    {
        "pid": 32,
        "title": "Contraintes d’OpenMP pour la parallélisation automatique à base de tâches",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-13d2d8e70651ae555d9c9e7528289d28feafaaecf9efd94471438e083150e856",
            "timestamp": 1740769591,
            "size": 172909
        },
        "abstract_only": false,
        "abstract": "Les architectures modernes sont équipées de processeurs multi-cœurs qui nécessitent l’exécu-\r\ntion d’applications parallèles pour être exploitées à pleine puissance. La parallélisation d’un\r\nprogramme peut être réalisée par des experts en utilisant, par exemple, la programmation ba-\r\nsée sur les tâches. Cela peut aussi être réalisé de façon automatique. Dans cet article, nous abor-\r\ndons la parallélisation automatique basée sur les tâches et son implémentation avec OpenMP.\r\nNotre travail se focalise sur la gestion des dépendances des tableaux et l’évaluation de la pro-\r\nfondeur lors de la création de tâches. Nous analysons différentes méthodes de gestion de ces\r\naspects et évaluons leur impact en termes de coût supplémentaire. Nos résultats expérimentaux\r\nmontrent que l’utilisation des itérateurs et de la clause if d’OpenMP entraîne un coût significa-\r\ntif. Nous identifions des alternatives plus efficaces qui permettent d’optimiser la performance\r\ndes exécutions parallèles.",
        "authors": [
            {
                "email": "julien.gaupp@inria.fr",
                "first": "Julien",
                "last": "Gaupp",
                "affiliation": "Inria CAMUS"
            },
            {
                "email": "berenger.bramas@inria.fr",
                "first": "Berenger",
                "last": "Bramas",
                "affiliation": "Inria CAMUS"
            }
        ],
        "contacts": [
            {
                "email": "julien.gaupp@inria.fr",
                "first": "Julien",
                "last": "Gaupp",
                "affiliation": "Inria CAMUS"
            },
            {
                "email": "berenger.bramas@inria.fr",
                "first": "Berenger",
                "last": "Bramas",
                "affiliation": "Inria CAMUS"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1740833446
    },
    {
        "pid": 33,
        "title": "Portage sur GPU du calcul des forces internes d'un code basé sur la méthode des éléments finis spectraux",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-5dcb80fb13eb1a7d500a48ed87796bbfcf55b02d20f653bbc3532f8b021702d5",
            "timestamp": 1741962238,
            "size": 2348371
        },
        "abstract_only": false,
        "abstract": "La méthode des éléments finis spectraux est utilisée dans de nombreux codes de simulations. Ces codes sont très exigeants en terme de calculs et nécessitent donc d'être parallélisés. Dans le cadre de la simulation de la propagation d'ondes sismiques, nous proposons de faciliter la parallélisation sur GPU du cœur de cette méthode en s’appuyant sur une approche existante vectorisée sur CPU. L’attention est portée sur l’utilisation de la mémoire partagée et les éventuels conflits d’accès. La méthode proposée pour le portage sur GPU permet d'obtenir un speedup de 7.7 par rapport à la version vectorisée sur CPU utilisant un cœur.",
        "authors": [
            {
                "email": "tom.budon@univ-orleans.fr",
                "first": "Tom",
                "last": "Budon",
                "affiliation": "Université d'Orléans"
            },
            {
                "email": "emmanuel.melin@univ-orleans.fr",
                "first": "Emmanuel",
                "last": "Melin",
                "affiliation": "Université d'Orléans"
            },
            {
                "email": "sebastien.limet@univ-orleans.fr",
                "first": "Sébastien",
                "last": "Limet",
                "affiliation": "Université d'Orléans"
            },
            {
                "email": "F.DeMartin@brgm.fr",
                "first": "Florent",
                "last": "de Martin",
                "affiliation": "BRGM"
            },
            {
                "email": "sylvain.jubertie@intel.com",
                "first": "Sylvain",
                "last": "Jubertie",
                "affiliation": "Intel"
            }
        ],
        "contacts": [
            {
                "email": "tom.budon@univ-orleans.fr",
                "first": "Tom",
                "last": "BUDON",
                "affiliation": "None"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741967301
    },
    {
        "pid": 34,
        "title": "Improving energy efficiency of HPC applications using unbalanced GPU power capping",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-b9049bd9068cd147503e172ba5a74c07f993fc8704c745fa53ac3b29e128c963",
            "timestamp": 1741008756,
            "size": 39184
        },
        "abstract_only": true,
        "abstract": "Energy efficiency represents a significant challenge in the domain of high-performance computing (HPC).  One potential key parameter to improve energy efficiency is the use of power capping, a technique for controlling the power limits of a device, such as a CPU or GPU. In this paper, we propose to examine the impact of GPU power capping in the context of HPC applications using heterogeneous computing systems. To this end, we first conduct an extensive study of the impact of GPU power capping on a compute intensive kernel, namely matrix multiplication kernel (GEMM), on different Nvidia GPU architectures. Interestingly, such compute-intensive kernels are up to 30% more energy efficient when the GPU is set to 55-70% of its Thermal Design Power (TDP). Using the best power capping configuration provided by this study, we investigate how setting different power caps for GPU devices of a heterogeneous computing node can improve the energy efficiency of the running application. We consider dense linear algebra task-based operations, namely matrix multiplication and Cholesky factorization. We show how the underlying runtime system scheduler can then automatically adapt its decisions to take advantage of the heterogeneous performance capability of each GPU.  The results show that for a given platform equipped with four GPU devices, applying a power cap on all GPUs improves the energy efficiency for matrix multiplication up to 24.3% (resp. 33.78%) for double (resp. single) precision.",
        "authors": [
            {
                "email": "albert.d-aviau-de-piolant@inria.fr",
                "first": "Albert",
                "last": "d'Aviau de Piolant",
                "affiliation": "Inria, Université de Bordeaux"
            },
            {
                "email": "hayfa.tayeb@inria.fr",
                "first": "Hayfa",
                "last": "Tayeb",
                "affiliation": "Inria, Université de Strasbourg"
            },
            {
                "email": "berenger.bramas@inria.fr",
                "first": "Berenger",
                "last": "Bramas",
                "affiliation": "Inria, Université de Strasbourg"
            },
            {
                "email": "mathieu.faverge@inria.fr",
                "first": "Mathieu",
                "last": "Faverge",
                "affiliation": "Inria, Bordeaux INP"
            },
            {
                "email": "abdou.guermouche@inria.fr",
                "first": "Abdou",
                "last": "Guermouche",
                "affiliation": "Inria, Université de Bordeaux"
            },
            {
                "email": "amina.guermouche@inria.fr",
                "first": "Amina",
                "last": "Guermouche",
                "affiliation": "Inria, Bordeaux INP"
            }
        ],
        "contacts": [
            {
                "email": "albert.d-aviau-de-piolant@inria.fr",
                "first": "Albert",
                "last": "d'Aviau de Piolant",
                "affiliation": "Inria, Université de Bordeaux"
            },
            {
                "email": "berenger.bramas@inria.fr",
                "first": "Berenger",
                "last": "Bramas",
                "affiliation": "Inria CAMUS"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741008756
    },
    {
        "pid": 35,
        "title": "Compression en virgule flottante appliquée aux H-Matrices",
        "abstract_only": true,
        "abstract": "L’algèbre des H-Matrices possède de nombreux avantages pour la résolution de système li-\r\nnéaires, que ce soit en termes de stabilité numérique ou de temps de calcul comparé à des\r\nsolveurs directs ou itératifs plus classiques, mais présente toutefois une limitation sur la taille\r\ndes problèmes. En effet, que ce soit en terme de coût spatial (occupation mémoire trop\r\nimportante) ou de coût temporel (calculs limités par la bande passante mémoire), réduire la\r\ntaille mémoire des matrices utilisées est un enjeu majeur. De plus, cela permet d’alléger le sto-\r\nckage sur disque pour les cas de calculs dits \"out-of-core\" ainsi que de diminuer le poids des\r\ncommunications, élément déterminant notamment lors des calculs sur architecture à mémoire\r\ndistribuée. Dans cette optique, et dans le cadre de cette thèse, nous nous intéressons à la com-\r\npression en virgule flottante des différents blocs d’une H-Matrice dans un contexte industriel\r\nafin de réduire l’empreinte mémoire lors des calculs pour un gain en espace et en temps, et ce\r\navec une perte de précision contrôlée. Pour cela, plusieurs schémas de compression arith-\r\nmétique sont envisagés et comparés, afin de déterminer ceux qui permettent les meilleurs taux\r\nde compression à une précision donnée. Des premiers tests seront effectués sur une version\r\nséquentielle de la librairie H-Matrice, avec comme objectif à terme d’intégrer cette compres-\r\nsion dans une version parallèle du code. Cela permettrait ainsi de traiter des problèmes de\r\nplus grande taille (et donc avec une modélisation plus fine et plus précise) ou des problèmes\r\nde même taille avec un coût spatial et temporel réduit.",
        "authors": [
            {
                "email": "clement.peaucelle@airbus.com",
                "first": "Clément",
                "last": "Peaucelle",
                "affiliation": "Equipe CONCACE - Airbus, INRIA, Cerfacs"
            }
        ],
        "contacts": [
            {
                "email": "clement.peaucelle@airbus.com",
                "first": "Clément",
                "last": "Peaucelle",
                "affiliation": "INRIA Bordeaux"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741012725
    },
    {
        "pid": 36,
        "title": "Generic Tiled Layouts for High-Performance Multidimensional Data Structures",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-70cdad2ed0cdb4785ac9c468021610b8bc9a543bb1b5411cc9f88dabbcc8fc07",
            "timestamp": 1741978387,
            "size": 331605
        },
        "abstract_only": false,
        "abstract": "Performance optimization strategies for exascale-class HPC applications primarily rely on fine-tuning implementations, requiring comprehensive knowledge of today's heterogeneous hardware architectures. While existing programming models provide abstractions of algorithmic optimizations, they overlook the potential of improving the memory layout of data structures.\r\nIn this paper, we introduce generic tiled layouts for multidimensional data structures, designed to work seamlessly with C++23's std::mdspan. Experimental results on a naive dense matrix multiplication demonstrate that, by replacing standard layouts with our proposed solution, we achieve an average speedup of over 2.2x, with peak performance improvements of up to 6.7x.",
        "authors": [
            {
                "email": "gabriel.dossantos@cea.fr",
                "first": "Gabriel Dos",
                "last": "Santos",
                "affiliation": "CEA"
            },
            {
                "email": "cedric.chevalier@cea.fr",
                "first": "Cédric",
                "last": "Chevalier",
                "affiliation": "CEA"
            },
            {
                "email": "hugo.taboada@cea.fr",
                "first": "Hugo",
                "last": "Taboada",
                "affiliation": "CEA"
            },
            {
                "email": "marc.perache@cea.fr",
                "first": "Marc",
                "last": "Pérache",
                "affiliation": "CEA"
            }
        ],
        "contacts": [
            {
                "email": "Hugo.TABOADA@cea.fr",
                "first": "Hugo",
                "last": "Taboada",
                "affiliation": "CEA"
            },
            {
                "email": "gabriel.dossantos@cea.fr",
                "first": "Gabriel",
                "last": "Dos Santos",
                "affiliation": "CEA"
            },
            {
                "email": "cedric.chevalier@cea.fr",
                "first": "Cédric",
                "last": "Chevalier",
                "affiliation": "CEA"
            },
            {
                "email": "marc.perache@cea.fr",
                "first": "Marc",
                "last": "Pérache",
                "affiliation": "CEA"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741978387
    },
    {
        "pid": 37,
        "title": "Facilitating heterogeneity management on the computing continuum",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-af99b6439632248fcee58bd5934099c394a80f7e255639ac5146ffc13c99c462",
            "timestamp": 1741967637,
            "size": 332303
        },
        "abstract_only": false,
        "abstract": "The computing continuum combines edge and cloud computing resources to create a seamless infrastructure.\r\nThis introduces resource heterogeneity.  \r\nTherefore, meeting application requirements such as fast response times, high-quality results, detailed data, low costs, and energy efficiency becomes challenging.\r\nWe suggest using self-adaptive systems and software variability management methods to manage this complexity.\r\nSpecifically, we propose modeling software variability and infrastructure heterogeneity to help configure and deploy systems effectively.\r\nWe illustrate this approach with a video analytics use case.",
        "authors": [
            {
                "email": "martin.molli@inria.fr",
                "first": "Martin",
                "last": "MOLLI",
                "affiliation": "IMT Atlantique, Inria, LS2N, UMR CNRS 6004, F-44307 Nantes, France"
            },
            {
                "email": "daniel.balouek@inria.fr",
                "first": "Daniel",
                "last": "Balouek",
                "affiliation": "IMT Atlantique, Inria, LS2N, UMR CNRS 6004, F-44307 Nantes, France"
            },
            {
                "email": "paul.temple@irisa.fr",
                "first": "Paul",
                "last": "Temple",
                "affiliation": "University of Rennes, CNRS, Inria, IRISA"
            },
            {
                "email": "Thomas.Ledoux@imt-atlantique.fr",
                "first": "Thomas",
                "last": "Ledoux",
                "affiliation": "IMT Atlantique, Inria, LS2N, UMR CNRS 6004, F-44307 Nantes, France"
            }
        ],
        "contacts": [
            {
                "email": "Thomas.Ledoux@imt-atlantique.fr",
                "first": "Thomas",
                "last": "Ledoux",
                "affiliation": "IMT Atlantique"
            },
            {
                "email": "martin.molli@inria.fr",
                "first": "Martin",
                "last": "MOLLI",
                "affiliation": "IMT Atlantique"
            },
            {
                "email": "paul.temple@irisa.fr",
                "first": "Paul",
                "last": "Temple",
                "affiliation": "University of Rennes, CNRS, Inria, IRISA"
            },
            {
                "email": "daniel.balouek@inria.fr",
                "first": "Daniel",
                "last": "Balouek",
                "affiliation": "IMT Atlantique, Inria, LS2N, UMR CNRS 6004, F-44307 Nantes, France"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741357019
    },
    {
        "pid": 38,
        "title": "Coarse-grain Congestion Regulation Using Control Theory",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-446be068dd006fc7c9939cfbcc9a4f748e2993a06362ca9fb358b79c2f43e57c",
            "timestamp": 1741354413,
            "size": 282693
        },
        "abstract_only": false,
        "abstract": "Efficient data access in High-Performance Computing (HPC) systems is essential to the perfor-mance of intensive computing tasks. Traditional optimizations of the I\/O stack aim to improvepeak performance but are often workload specific and require deep expertise, making them dif-ficult to generalize. In shared HPC environments, resource congestion can lead to unpredictableperformance, causing slowdowns and timeouts. To address these challenges, a self-adaptiveapproach based on Control Theory is proposed to dynamically regulate client-side I\/O rates.By using few run-time system load metrics, the approach aims to reduce congestion and im-prove performance stability. We implement a controller on a single node system as a prelimi-nary work, and obtain encouraging results for addressing a multi-node cluster in future works.",
        "authors": [
            {
                "email": "thomas.collignon@inria.fr",
                "first": "Thomas",
                "last": "Collignon",
                "affiliation": "Qarnot Computing, F-92120 Montrouge, France \/ Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            },
            {
                "email": "kouds.halitim@inria.fr",
                "first": "Kouds",
                "last": "Halitim",
                "affiliation": "Univ. Grenoble Alpes, Inria, CNRS, LIG, F-38000 Grenoble, France"
            },
            {
                "email": "raphael.bleuse@inria.fr",
                "first": "Raphaël",
                "last": "Bleuse",
                "affiliation": "Univ. Grenoble Alpes, Inria, CNRS, LIG, F-38000 Grenoble, France"
            },
            {
                "email": "sophie.cerf@inria.fr",
                "first": "Sophie",
                "last": "Cerf",
                "affiliation": "Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            },
            {
                "email": "bogdan.robu@inria.fr",
                "first": "Bogdan",
                "last": "Robu",
                "affiliation": "Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, F-38000 Grenoble, France"
            },
            {
                "email": "eric.rutten@inria.fr",
                "first": "Eric",
                "last": "Rutten",
                "affiliation": "Univ. Grenoble Alpes, Inria, CNRS, LIG, F-38000 Grenoble, France"
            },
            {
                "email": "lionel.seinturier@inria.fr",
                "first": "Lionel",
                "last": "Seinturier",
                "affiliation": "Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            },
            {
                "email": "alexandre.vankempen@qarnot-computing.com",
                "first": "Alexandre",
                "last": "Van Kempen",
                "affiliation": "Qarnot Computing, F-92120 Montrouge, France"
            }
        ],
        "contacts": [
            {
                "email": "sophie.cerf@inria.fr",
                "first": "Sophie",
                "last": "Cerf",
                "affiliation": "Inria"
            },
            {
                "email": "thomas.collignon@qarnot-computing.com",
                "first": "Thomas",
                "last": "Collignon",
                "affiliation": "Inria Spirals \/ Qarnot Computing"
            },
            {
                "email": "thomas.collignon@inria.fr",
                "first": "Thomas",
                "last": "Collignon",
                "affiliation": "Qarnot Computing, F-92120 Montrouge, France \/ Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            },
            {
                "email": "kouds.halitim@inria.fr",
                "first": "Kouds",
                "last": "Halitim",
                "affiliation": "Univ. Grenoble Alpes, Inria, CNRS, LIG, F-38000 Grenoble, France"
            },
            {
                "email": "raphael.bleuse@inria.fr",
                "first": "Raphaël",
                "last": "Bleuse",
                "affiliation": "Univ. Grenoble Alpes, Inria, CNRS, LIG, F-38000 Grenoble, France"
            },
            {
                "email": "bogdan.robu@inria.fr",
                "first": "Bogdan",
                "last": "Robu",
                "affiliation": "Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, F-38000 Grenoble, France"
            },
            {
                "email": "eric.rutten@inria.fr",
                "first": "Eric",
                "last": "Rutten",
                "affiliation": "Univ. Grenoble Alpes, Inria, CNRS, LIG, F-38000 Grenoble, France"
            },
            {
                "email": "lionel.seinturier@inria.fr",
                "first": "Lionel",
                "last": "Seinturier",
                "affiliation": "Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
            },
            {
                "email": "alexandre.vankempen@qarnot-computing.com",
                "first": "Alexandre",
                "last": "Van Kempen",
                "affiliation": "Qarnot Computing, F-92120 Montrouge, France"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741354516
    },
    {
        "pid": 39,
        "title": "An analysis for the design of an efficient replica management strategy",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-572adcb0e8e00216d442a509c1cc79ad8f58c748d89174e62d6d2fcb710284c0",
            "timestamp": 1741602132,
            "size": 156874
        },
        "abstract_only": false,
        "abstract": "The transition from centralized systems to geo-distributed infrastructures offers increased flexibility by enabling the use of geographically distributed resources. These infrastructures are employed in fields requiring significant processing capabilities, such as scientific research. However, data management, which is an important aspect of efficiently managing geo-distributed infrastructure and achieving optimal performance, is difficult to implement due to various constraints (hardware and software constraints). In addition, energy consumption has become a crucial factor to consider in modern infrastructures. In this work, we focus on replication, which is a common approach in data management, but, it is important to understand that naive replication can lead to costly transfers and resource loses. The success of a replication policy relies on a balance between performance, availability, and data transfer optimization. Build on this, we present a preliminary study aimed at minimizing transfers, especially by looking for an efficient number of replicas, while ensuring optimal performance. Through this study, we have shown that by focusing only on important transfers, it is possible to achieve good performance while reducing energy costs.",
        "authors": [
            {
                "email": "cherif.simohammed@ademe.fr",
                "first": "Cherif",
                "last": "SI MOHAMMED",
                "affiliation": "ADEME, LS2N, IMT Atlantique Nantes."
            },
            {
                "email": "adrien.lebre@inria.fr",
                "first": "Adrien",
                "last": "LEBRE",
                "affiliation": "INRIA, LS2N, IMT Atlantique Nantes."
            },
            {
                "email": "alexandre.vankempen@qarnot-computing.com",
                "first": "Alexandre",
                "last": "VAN KEMPEN",
                "affiliation": "Qarnot Computing"
            }
        ],
        "contacts": [
            {
                "email": "cherif.simohammed@ademe.fr",
                "first": "Cherif",
                "last": "SI MOHAMMED",
                "affiliation": "ADEME"
            },
            {
                "email": "alexandre.vankempen@qarnot-computing.com",
                "first": "Alexandre",
                "last": "Van Kempen",
                "affiliation": "Qarnot Computing, F-92120 Montrouge, France"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741602132
    },
    {
        "pid": 40,
        "title": "AndroWatts : Unpacking the Power Consumption of Mobile Device’s Components",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-1807d4eb91c666aa3ebb787dfc2e5d5812c68c2ef70a151bf4ef1a329eca9964",
            "timestamp": 1741614192,
            "size": 47152
        },
        "abstract_only": true,
        "abstract": "Ces dernières années, l’intérêt pour la mesure de la consommation des appareils mobiles n’a\r\nfait que grandir, principalement lié aux limites imposées par les batteries : celles-ci ont une\r\ndurée de vie limitée ainsi qu’un coût environnemental important tout au long de leur cycle de\r\nvie.\r\nMalheureusement, les solutions proposées pour mesurer la consommation d’une application\r\nmanquent de précision. Elles reposent en général sur la mesure de la batterie uniquement,\r\nparfois accompagnée des mesures de certains composants. Il est donc nécessaire de fournir\r\naux développeurs de nouvelles options pour quantifier et surveiller la consommation de leurs\r\napplications afin d’y apporter des optimisations pertinentes.\r\nAvec cet objectif, nous proposons une nouvelle méthode pour estimer cette consommation.\r\nPlutôt que de s’appuyer sur la décharge de la batterie directement, nous modélisons la consom-\r\nmation des composants du téléphone afin de déterminer leurs impacts sur la décharge de l’ap-\r\npareil au cours de l’exécution d’une application.\r\nDans ce papier, nous (i) modélisons la consommation d’une application en utilisant une com-\r\nbinaison des métriques fournies par Android et le téléphone, et (ii) évaluons la précision d’une\r\nstratégie basée sur la consommation par composant, ainsi que (iii) l’impact de la quantité de\r\ndonnées nécessaires pour notre modèle. Nos expérimentations nous permettent de constater\r\nune grande précision de nos modèles linéaires ainsi qu’une forte corrélation entre les valeurs\r\nde consommations prédites et constatées, notamment pour le CPU et le GPU.",
        "authors": [
            {
                "email": "noe.chachignot@inria.fr",
                "first": "Noé",
                "last": "Chachignot",
                "affiliation": "Centre Inria de l’Université de Lille"
            },
            {
                "email": "eguegain@greenspector.com",
                "first": "Édouard",
                "last": "Guégain",
                "affiliation": "Greenspector"
            },
            {
                "email": "remy.raes@inria.fr",
                "first": "Rémy",
                "last": "Raes",
                "affiliation": "Centre Inria de l’Université de Lille"
            },
            {
                "email": "clement.quinton@univ-lille.fr",
                "first": "Clément",
                "last": "Quinton",
                "affiliation": "Université de Lille"
            },
            {
                "email": "romain.rouvoy@univ-lille.fr",
                "first": "Romain",
                "last": "Rouvoy",
                "affiliation": "Université de Lille"
            }
        ],
        "contacts": [
            {
                "email": "noe.chachignot@inria.fr",
                "first": "Noé",
                "last": "Chachignot",
                "affiliation": "Inria Lille"
            },
            {
                "email": "remy.raes@inria.fr",
                "first": "Rémy",
                "last": "Raes",
                "affiliation": "Centre Inria de l’Université de Lille"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741614192
    },
    {
        "pid": 41,
        "title": "Anticipation des communications réseau grâce à la connaissance du futur dans le parallélisme à tâches",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-18e2e37e78cec7c678b68051982783dd6e2d4ff5643a9ae46f9f27a94f7f009d",
            "timestamp": 1741619698,
            "size": 54720
        },
        "abstract_only": true,
        "abstract": "Les architectures actuelles des calculateurs haute performance présentent des nœuds multi-cœurs ou GPU reliés par un réseau haute performance. Ce type d'architecture pousse à l'utilisation de modèles de programmation parallèle hybride multi-thread et distribuée. Par ailleurs, sur certains de ces réseaux haute-performance, une étape préalable à l'échange de données est l'enregistrement mémoire qui permet à la carte réseau d'accéder directement aux adresses mémoire enregistrées sans passer par l'OS. Or cette étape allonge la durée des communications car elle est réalisée juste avant l'envoi ou réception de données. Par conséquent, une pratique commune est de réaliser un cache d'enregistrement appelé rcache. Cependant, certaines applications utilisent des buffers de communications systématiquement différents et n'en bénéficient donc pas. Ainsi, notre objectif est de réaliser l'enregistrement mémoire plus tôt, ce qui est possible car cette étape n'utilise que les adresses mémoire et peut donc être réalisée avant que les contenus des buffers de réception ou d'envoi ne soient prêts. \r\n\r\nPour y parvenir, nous souhaitons mettre à profit l'utilisation des modèles de programmation parallèle hybride comme StarPU, qui est un système d'exécution s'appuyant sur un parallélisme à base de tâches. Dans sa version distribuée, StarPU s'appuie sur une bibliothèque MPI, ou bien sur NewMadeleine, et exécute les communications inter-nœuds de manière implicite en les inférant depuis le graphe de tâches. Ainsi, en utilisant la connaissance du futur fournie par le graphe de tâche maintenu par StarPU, il est possible d'être prévenu lorsque la dernière tâche de calcul utilisant un buffer d'envoi ou de réception commence. De cette manière, nous pouvons réaliser l'enregistrement mémoire pendant l'exécution de cette tâche, et éviter la latence induite par cette étape. En retirant la phase d'enregistrement mémoire du chemin critique de l'exécution des communications nous espérons réduire la durée des communications réseau, ce qui devraient conduire à de meilleures performances des applications distribuées utilisant StarPU.",
        "authors": [
            {
                "email": "tanguy.chatelain@inria.fr",
                "first": "Tanguy",
                "last": "Chatelain",
                "affiliation": "ENSEIRB-MATMECA \/ Centre Inria de l'Université de Bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "tanguy.chatelain@inria.fr",
                "first": "Tanguy",
                "last": "Chatelain",
                "affiliation": "ENSEIRB-MATMECA \/ Centre Inria de l'Université de Bordeaux"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741619945
    },
    {
        "pid": 42,
        "title": "Sufficiency in Cloud : Minimizing Idle Power Consumption Through Improved User Behavior",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-eefe729e634c92e9df3e50192a8ed4a0c6cf812c6d03eb0d0523d48a3d2d900a",
            "timestamp": 1741688973,
            "size": 40008
        },
        "abstract_only": true,
        "abstract": "Cloud computing enables scalable and flexible resource allocation. However, it often leads to unnecessary energy consumption due to inefficient user behaviors and resource provisioning strategies. This research explores sufficiency techniques in cloud environments and aims to reduce energy waste by encouraging both cloud users and providers to adopt more conscious resource usage practices.\r\n\r\n We propose that simple user-driven optimizations can lead to significant energy reductions. To find the possible optimization techniques, we analyze job records from the Grid'5000 testbed. We investigate resource usage both during idle times and under utilization, and measure their impact on energy consumption. User behavior improvements could include selecting appropriate resources for workloads and actively managing idle periods. Our contribution focus on understanding resource usage patterns and ensuring that only the required resources are allocated. \r\n    \r\nTo demonstrate how small adjustments in user practices can contribute to sustainability in cloud computing, we simulate these behavior changes and assess their impact on the overall idle consumption of the system. Using the  simulation platform Batsim, we evaluate the potential energy savings when users adjust their behaviors. To achieve this, we perform simulations with original and modified workloads, and compare the results.\r\n\r\nThis study aims to bridge the gap between energy-efficient cloud management and user awareness. We believe that intentional sufficiency techniques rather than just technological advancements can drive substantial reductions in cloud energy consumption.",
        "authors": [
            {
                "email": "eyvaz.ahmadzada@irit.fr",
                "first": "Eyvaz",
                "last": "Ahmadzada",
                "affiliation": "Institut de Recherche en Informatique de Toulouse - IRIT"
            },
            {
                "email": "patricia.stolf@irit.fr",
                "first": "Patricia",
                "last": "Stolf",
                "affiliation": "Institut de Recherche en Informatique de Toulouse - IRIT"
            },
            {
                "email": "jean-marc.pierson@irit.fr",
                "first": "Jean-Marc",
                "last": "Pierson",
                "affiliation": "Institut de Recherche en Informatique de Toulouse - IRIT"
            },
            {
                "email": "laurent.lefevre@ens-lyon.fr",
                "first": "Laurent",
                "last": "Lefèvre",
                "affiliation": "ENS de Lyon - École normale supérieure de Lyon"
            }
        ],
        "contacts": [
            {
                "email": "jean-marc.pierson@irit.fr",
                "first": "Jean-Marc",
                "last": "Pierson",
                "affiliation": "Institut de recherche en informatique de Toulouse"
            },
            {
                "email": "eyvaz.ahmadzada@irit.fr",
                "first": "Eyvaz",
                "last": "Ahmadzada",
                "affiliation": "Institut de Recherche en Informatique de Toulouse - IRIT"
            },
            {
                "email": "patricia.stolf@irit.fr",
                "first": "Patricia",
                "last": "Stolf",
                "affiliation": "Institut de Recherche en Informatique de Toulouse - IRIT"
            },
            {
                "email": "laurent.lefevre@ens-lyon.fr",
                "first": "Laurent",
                "last": "Lefèvre",
                "affiliation": "ENS de Lyon - École normale supérieure de Lyon"
            }
        ],
        "topics": [
            "Architecture",
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741871840
    },
    {
        "pid": 43,
        "title": "Flexible Performance Anomaly Detection in Microservice Applications with ctl-SRNN",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-ef7b717fa469bed5985f90107fd01a0ff5f8e2c23a9962d4ba6b71ad3a8d2fe1",
            "timestamp": 1741939675,
            "size": 246252
        },
        "abstract_only": false,
        "abstract": "Detecting performance anomalies in microservice applications is challenging because the services composing such applications are highly diverse and frequently updated. Machine learning approaches capable of automatically detecting performance anomalies are highly desirable but often difficult to use in practice, particularly if they require labeled data, significant training time, or predefined thresholds to distinguish normal from abnormal behavior. This paper presents ctl-SRNN, a generative probabilistic model designed to detect performance anomalies in microservice applications. The ctl-SRNN model benefits of a control variable to give contextual information on service workload while incorporating uncertainty quantification to dynamically define normal behavior of its resource usage. Experiments conducted using two microservice applications, realistic workloads, and various types of anomalies demonstrate that ctl-SRNN achieves an improvement of more than 50% in anomaly detection accuracy compared to state-of-the-art approaches. Furthermore, results show that it is the combination of an probabilistic model with a control variable and uncertainty quantification that improves anomaly detection.",
        "authors": [
            {
                "email": "gabriel.job-antunes-grabher@univ-grenoble-alpes.fr",
                "first": "Gabriel",
                "last": "Grabher",
                "affiliation": "Université Grenoble-Alpes"
            }
        ],
        "contacts": [
            {
                "email": "gabriel.job-antunes-grabher@univ-grenoble-alpes.fr",
                "first": "Gabriel",
                "last": "Grabher",
                "affiliation": "Université Grenoble-Alpes"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741846779
    },
    {
        "pid": 44,
        "title": "La Quête du Scheduling Parfait : Jouer sur Tous les Tableaux",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-eea62ad82504409f74a90889d3b3ec3e0ea7ea27b59e37a1ed0a6b15a8b15e60",
            "timestamp": 1741991032,
            "size": 282170
        },
        "abstract_only": false,
        "abstract": "Ces dernières années, les innovations dans l'utilisation de centres de données et la popularité de nouveaux services tels que l'apprentissage profond ont permis d'obtenir des avancées importantes dans de nombreux domaines comme la santé, l'éducation ou encore le commerce. Ces progrès vont avec une consommation d'énergie toujours plus importante, malgré de nombreux gains d'efficacité, tandis qu'il faudrait maîtriser notre consommation d'énergie dans un contexte de crise climatique. Nous avons identifié un potentiel de réduction d'énergie grâce à des techniques de déplacement de charges informatiques sur des serveurs qui modifient le moment d'exécution, le lieu ou encore qui adaptent le processus d'exécution par une alternative pour un résultat similaire.\r\n\r\nDans ce papier, nous explorons la possibilité de combiner ces différentes techniques, et évaluons les impacts de ces techniques combinées. A l'aide du simulateur Batsim, nous avons développé une méthodologie pour évaluer l'efficacité de techniques combinées sur des ensembles de tâches générées artificiellement.",
        "authors": [
            {
                "email": "nicolas.tirel@univ-pau.fr",
                "first": "Nicolas",
                "last": "Tirel",
                "affiliation": "Université de Pau et des Pays de l'Adour"
            },
            {
                "email": "Philippe.Roose@iutbayonne.univ-pau.fr",
                "first": "Philippe",
                "last": "Roose",
                "affiliation": "Université de Pau et des Pays de l'Adour"
            },
            {
                "email": "silarri@unizar.es",
                "first": "Sergio",
                "last": "Ilarri",
                "affiliation": "Universidad de Zaragoza (UNIZAR)"
            },
            {
                "email": "olivier.logoaer@univ-pau.fr",
                "first": "Olivier",
                "last": "Le Goaër",
                "affiliation": "Université de Pau et des Pays de l'Adour"
            },
            {
                "email": "adel.noureddine@univ-pau.fr",
                "first": "Adel",
                "last": "Noureddine",
                "affiliation": "Université de Pau et des Pays de l'Adour"
            }
        ],
        "contacts": [
            {
                "email": "Philippe.Roose@iutbayonne.univ-pau.fr",
                "first": "Philippe",
                "last": "Roose",
                "affiliation": "Université de Pau - Bayonne"
            },
            {
                "email": "nicolas.tirel@univ-pau.fr",
                "first": "Nicolas",
                "last": "Tirel",
                "affiliation": "Universite de Pau et des Pays de l'Adour"
            },
            {
                "email": "silarri@unizar.es",
                "first": "Sergio",
                "last": "Ilarri",
                "affiliation": "Universidad de Zaragoza (UNIZAR)"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741991032
    },
    {
        "pid": 45,
        "title": "Traitement de données en temps réel pour les systèmes embarqués communicants : l’adéquation algorithme-architecture à la rescousse",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-c613b47a2d485db978422852a643e2393760c7a3bf947830e33854f13b2376a6",
            "timestamp": 1741864157,
            "size": 62762
        },
        "abstract_only": true,
        "abstract": "Les systèmes embarqués modernes nécessitent fréquemment le transfert rapide de grandes quantités de données vers des processeurs à capacité mémoire limitée, notamment dans des domaines comme le traitement radar, l'imagerie ou l'intelligence artificielle embarquée. Dans ce contexte, le traitement et la vérification de l'intégrité des données, via un calcul en temps réel du CRC (Cyclic Redundancy Check) par exemple, représente un véritable défi, notamment en raison des contraintes temporelles. En effet, cela demande au système d'être capable de récupérer une donnée, la sauvegarder au bon emplacement et de calculer un CRC cumulatif avant de recevoir une nouvelle donnée, pour éviter une perte d'information. Les solutions traditionnelles requièrent typiquement de multiples domaines d'horloge et une fréquence CPU supérieure à celle de la communication, complexifiant ainsi le système et augmentant la consommation énergétique.\r\n\r\nCe travail propose de faire fonctionner le processeur à la fréquence de communication du SPI, supprimant ainsi le besoin de multiples domaines d'horloges. Cette approche simplifie la conception matérielle, mais apporte de fortes contraintes en termes de temps d'exécution.\r\nUne méthodologie basée sur l'adéquation algorithme-architecture (AAA) permet cependant de garantir un traitement en temps réel, malgré un nombre de cycles d'horloges très limités pour l'exécution du traitement.\r\nL'utilisation de coprocesseurs et de périphériques dédiés intégrés dans une architecture ouverte RISC-V permet d'accélérer les calculs tout en minimisant le nombre d'instructions exécutées par le CPU. Pour atteindre les performances souhaitées, une série de stratégies d'optimisations logicielles ont été mises en place. Ceci inclus des opérations SIMD (Single Instruction on Multiple Data), le déroulement des boucles et la suppression d'instructions redondantes ou inutiles grâce à une analyse au niveau assembleur. \r\n\r\nLes résultats expérimentaux sur FPGA et ASIC montrent qu'une approche combinée matériel-logiciel permet de respecter strictement les contraintes de temps réel, atteignant un traitement effectif d'un octet reçu (8 bits) en moins de 8 cycles d'horloge. Ces résultats ont été valorisés lors de la conférence ICECS en novembre 2024 et le code source est ouvert et accessible en ligne. À terme, la méthodologie proposée rend possible le remplacement d'ASIC existants par des architectures programmables, tout en conservant une compatibilité broche-à-broche avec les systèmes existants.",
        "authors": [
            {
                "email": "jonathan.saussereau@ims-bordeaux.fr",
                "first": "Jonathan",
                "last": "Saussereau",
                "affiliation": "IMS Bordeaux"
            },
            {
                "email": "christophe.jego@ims-bordeaux.fr",
                "first": "Christophe",
                "last": "Jego",
                "affiliation": "IMS Bordeaux"
            },
            {
                "email": "camille.leroux@ims-bordeaux.fr",
                "first": "Camille",
                "last": "Leroux",
                "affiliation": "IMS Bordeaux"
            },
            {
                "email": "jb.begueret@ims-bordeaux.fr",
                "first": "Jean-Baptiste",
                "last": "Begueret",
                "affiliation": "IMS Bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "jonathan.saussereau@ims-bordeaux.fr",
                "first": "Jonathan",
                "last": "Saussereau",
                "affiliation": "IMS Bordeaux"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741864157
    },
    {
        "pid": 46,
        "title": "Partitionnement hiérarchique de graphe pour la compression H-matrice",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-964073ed642b37dc05c680c6e377367ed8fbfb9ebfd220c90c16f4f62d2af22a",
            "timestamp": 1741875158,
            "size": 70320
        },
        "abstract_only": true,
        "abstract": "La méthode des éléments finis de frontière (Boundary Element Method ou BEM) présente de nombreux avantages pour la simulation de phénomènes complexes. En plaçant les inconnues du problème (ou degrés de liberté) sur des frontières entre différents milieux, il devient possible de modéliser des problèmes avec des conditions limites lointaines (écoulements d’un fluide autour d’un objet, diffraction d’une onde acoustique ou électromagnétique, transferts thermiques par rayonnement, ...). En contrepartie, les degrés de liberté se trouvent couplés et la matrice représentative du système devient dense. Lorsque cette matrice dense peut être décomposée en sous-blocs de rang faible, il est possible de construire une matrice hiérarchique (H-matrice) approchant le système à une précision choisie. Dans les cas les plus favorables, cette approximation permet de réduire la complexité spatiale de O(n^2) à O(n log(n)) en compressant les sous-blocs de la matrice initiale. Ce travail explore le lien entre le partitionnement des degrés de liberté et le taux de compression de la H-matrice. Une nouvelle technique de partitionnement hiérarchique, conçue pour optimiser la compression H-matrice, est également présentée.  Contrairement à des algorithmes exploitant la géométrie du problème (Median cut, Cobblestone ou Space-filling curves), cette nouvelle méthode repose sur l’utilisation d’un graphe de connectivité des degrés de liberté. Ce graphe est construit en temps quasi-linéaire (O(n log(n))) à partir du maillage de l’objet étudié et partitionné en temps log-quadratique (O(n^2 log(n))) grâce à un partitionneur multi-niveau. Une contrainte supplémentaire est imposée afin d’équilibrer la charge des partitions, pour des optimisations sur des supports d’exécution à base de tâches.  Des expériences numériques sont réalisées sur un ensemble de cas d’application, issus de simulations en électromagnétisme. Ces cas couvrent différents types de géométries et vont de quelques milliers de degrés de liberté à plus de huit-cent-mille. Le nouvel algorithme est comparé aux méthodes géométriques selon trois critères : le temps de partitionnement, le taux de compression des H-matrices et le temps de factorisation.",
        "authors": [
            {
                "email": "dimitri.walther@inria.fr",
                "first": "Dimitri",
                "last": "Walther",
                "affiliation": "CEA, Univ. Bordeaux, CNRS, Bordeaux INP, INRIA, LaBRI, UMR 5800, F-33400 Talence, France"
            }
        ],
        "contacts": [
            {
                "email": "dimitri.walther@inria.fr",
                "first": "Dimitri",
                "last": "Walther",
                "affiliation": "CEA, Univ. Bordeaux, CNRS, Bordeaux INP, INRIA, LaBRI, UMR 5800, F-33400 Talence, France"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741875158
    },
    {
        "pid": 47,
        "title": "Classifieur acoustique multi-sources embarqué pour milieu sous-marin",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-9e3a92d571c978472106a9641a44e469121de8551ee6fcdf0ae875795f87087e",
            "timestamp": 1741876457,
            "size": 51454
        },
        "abstract_only": true,
        "abstract": "La reconnaissance des sources acoustiques sous-marines représente un défi majeur en raison de la complexité inhérente à l'environnement marin. Cet environnement est marqué par une diversité des sources sonores d'origine animale ou d'activités humaines, un bruit de fond important et des schémas de propagation sonore non linéaires. Ces facteurs compliquent considérablement la classification précise des sources dans des conditions réelles.\r\n\r\nDans ce poster, nous présentons une approche pour la classification de signaux en acoustique sous-marine (ASM), intégrant l'ensemble de la chaîne de traitement et de classification sur une plateforme matérielle embarquée. Cette approche permet d'effectuer un maximum de traitements in situ tout en minimisant le transfert des données volumineuses. Notre méthode est conçue pour détecter et classifier une large gamme de sources acoustiques sous-marines. Cela inclut les signaux stationnaires, non stationnaires (transitoires et impulsionnels), anthropiques et biologiques.\r\n\r\nNous combinons la fusion de caractéristiques acoustiques multi-niveaux avec un réseau de neurones convolutif résiduel (ResNet), optimisé pour une implémentation embarquée. Cette approche exploite les avantages intrinsèques des FPGA, notamment leur capacité de traitement en temps réel et basse consommation. Elle respecte également les contraintes strictes de faible consommation énergétique et d'efficacité computationnelle.\r\n\r\nLes caractéristiques acoustiques sont extraites à l'aide de pré-traitements incluant les MFCC (Mel-frequency cepstrum coefficient), les GFCC (Gammatone frequency cepstral coefficient), le spectre LOFAR (Low-Frequency Analyzer and Recorder) et la CQT (constant Q transform). Ces caractéristiques fusionnées alimentent un modèle CNN résiduel embarqué, utilisé comme classificateur pour l'identification des cibles acoustiques sous-marines. Ce type de système pourrait permettre de transmettre des alertes enrichies d’indicateurs de confiance et d’évaluation des risques.\r\n\r\nLes résultats expérimentaux démontrent l'efficacité de cette méthode pour améliorer la précision et la robustesse de la classification dans des environnements sous-marins complexes. Cette approche ouvre de nouvelles perspectives pour des applications avancées en surveillance marine et en exploration autonome.",
        "authors": [
            {
                "email": "mohand.hamadouche@univ-brest.fr",
                "first": "Mohand",
                "last": "Hamadouche",
                "affiliation": "Lab-STICC, Université de Bretagne Occidentale (UBO)"
            },
            {
                "email": "Catherine.Dezan@univ-brest.fr",
                "first": "Catherine",
                "last": "Dezan",
                "affiliation": "Lab-STICC, Université de Bretagne Occidentale (UBO)"
            },
            {
                "email": "Erwan.Fabiani@univ-brest.fr",
                "first": "Erwan",
                "last": "Fabiani",
                "affiliation": "Lab-STICC, Université de Bretagne Occidentale (UBO)"
            },
            {
                "email": "helene.pihan-le.bars@shom.fr",
                "first": "Hélène Pihan-Le",
                "last": "Bars",
                "affiliation": "Service Hydrographique et Océanographique de la Marine (SHOM)"
            },
            {
                "email": "myriam.lajaunie@shom.fr",
                "first": "Myriam",
                "last": "Lajaunie",
                "affiliation": "Service Hydrographique et Océanographique de la Marine (SHOM)"
            },
            {
                "email": "david.dellong@shom.fr",
                "first": "David",
                "last": "Dellong",
                "affiliation": "Service Hydrographique et Océanographique de la Marine (SHOM)"
            }
        ],
        "contacts": [
            {
                "email": "mohand.hamadouche@univ-brest.fr",
                "first": "Mohand",
                "last": "Hamadouche",
                "affiliation": "Université de Bretagne Occidentale (UBO)"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741876457
    },
    {
        "pid": 48,
        "title": "Implémentation et évaluation de l'algorithme AKAZE en SYCL",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-5c9a1b8ceeada47b9be959c867473cdf654061ef883e6eacc38e292ed348c271",
            "timestamp": 1741965265,
            "size": 42366
        },
        "abstract_only": true,
        "abstract": "Dans cet exposé, on présentera une étude de cas portant sur la programmation parallèle de l'algorithme AKAZE en SYCL, réalisée dans le cadre d'une thèse débutant au sein du laboratoire commun Lab-STICC\/Thales-LAS. AKAZE est un algorithme de traitement d'image pour l'extraction de caractéristiques (pixels clefs), qui permet un suivi 2D d'un objet dans un flot vidéo avec une bonne tolérance aux transformations géométriques. En se basant sur une version C++ de référence avec parallélisation openMP pour multicœur, on expliquera la nature des modifications pour le portage en SYCL et on établira une comparaison des performances obtenues sur multicœur. \r\n\r\nLe standard SYCL (du consortium Khronos) est basé sur du C++, est mono-source et met en œuvre la portabilité et l'hétérogénéité, via un modèle normalisé de tâches de calcul (kernels) communiquant par des tampons mémoires, dont le support d'exécution est paramétrable. SYCL vise à devenir un standard industriel, déployable et portable sur toute plateforme de calcul haute performance hétérogène, depuis les supercalculateurs jusqu'aux puces embarquées. Cette capacité est nécessaire pour faciliter la maitrise des différents modèles de calcul sous-jacents et leurs interactions dans une plateforme hétérogène qui combine des architectures de nature différente (multicœur, GPGPU, FPGA). \r\n\r\nLa thèse vise à évaluer les apports de la programmation SYCL pour les traitements embarqués spécifiques au domaine métier considéré (traitement du signal, traitement d'image, classification). Elle devra établir des patrons de conception pour les modèles d'architectures considérés et, in fine, développer un environnement d'aide à la conception améliorant la productivité, la portabilité et les performances pour des traitements simulés et leur version embarquée. La problématique de recherche et le contexte des travaux de thèse seront également présentés dans cet exposé. \r\n\r\nUne première partie de ces travaux consiste à évaluer le portage en SYCL de benchmarks significatifs (dont AKAZE) pour le domaine métier. Ces expérimentations combinent de nombreux points de variation à évaluer : environnement de développement SYCL (DPC++ ou adaptiveCPP), constructions parallèles utilisées, support multicœur ou GPGPU, paramètres de la complexité du calcul.",
        "authors": [
            {
                "email": "sonia.haddouche@ensta.fr",
                "first": "Sonia",
                "last": "Haddouche",
                "affiliation": "ENSTA\/Lab-STICC\/Thales LAS"
            },
            {
                "email": "fabiani@univ-brest.fr",
                "first": "Erwan",
                "last": "Fabiani",
                "affiliation": "Univ Brest\/Lab-STICC"
            },
            {
                "email": "loic.lagadec@ensta.fr",
                "first": "Loïc",
                "last": "Lagadec",
                "affiliation": "ENSTA\/Lab-STICC"
            },
            {
                "email": "franck.danober@thalesgroup.com",
                "first": "Franck",
                "last": "Danober",
                "affiliation": "Thales LAS"
            },
            {
                "email": "robin.lembach@thalesgroup.com",
                "first": "Robin",
                "last": "Lembach",
                "affiliation": "Thales LAS"
            },
            {
                "email": "christophe.guillet@thalesgroup.com",
                "first": "Christophe",
                "last": "Guillet",
                "affiliation": "Thales LAS"
            }
        ],
        "contacts": [
            {
                "email": "fabiani@univ-brest.fr",
                "first": "Erwan",
                "last": "Fabiani",
                "affiliation": "Université de Bretagne Occidentale"
            },
            {
                "email": "sonia.haddouche@ensta.fr",
                "first": "Sonia",
                "last": "Haddouche",
                "affiliation": "ENSTA\/Lab-STICC"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741965265
    },
    {
        "pid": 49,
        "title": "Parallélisation automatique à base de tâches avec modélisation de performances",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-7f1534dbf991875477fc6e4232cb40c87616e2b73408f98d176d6bcc9239896d",
            "timestamp": 1741884961,
            "size": 56145
        },
        "abstract_only": true,
        "abstract": "Dans le paradigme de programmation parallèle à base de tâches, un programme est décomposé en tâches liées par des contraintes de dépendance que l'on confie à un moteur d'exécution. Bien que le programmeur se retrouve délesté de certains aspects complexes de la parallélisation, le passage d'un programme séquentiel à un programme parallèle à base de tâches correct et efficace n'est pas trivial. Traditionnellement, la parallélisation automatique cible des nids de boucles. Or, pour de nombreuses applications, la charge de travail est essentiellement concentrée dans certaines fonctions. Une possibilité pour exposer le parallélisme potentiel de ce type d'applications dans un environnement d'exécution à base de tâches, est d'exécuter les fonction coûteuses sous la forme de tâches concurrentes. Cependant cette approche de parallélisation automatique a été moins explorée. Dans [1], nous avons proposé APAC, un traducteur automatique pour réécrire un programme C\/C++ séquentiel en un programme parallèle à base de tâches avec des directives OpenMP. Depuis, nous avons formalisé APAC comme une architecture de compilation source-à-source qui s'articule autour d'une représentation intermédiaire que l'on appelle le graphe de candidats de tâches. Dans celui-ci, chaque sommet comprend une ou plusieurs instructions du code source séquentiel. Les dépendances de données entre les instructions se traduisent par des arêtes reliant les sommets correspondants. Des analyses et transformations sur le graphe nous permettent d'identifier les candidats adaptés pour devenir des tâches parallélisables dans le code source résultant et d'y introduire des mécanismes pour contrôler le nombre de tâches et la profondeur du parallélisme à l'exécution. Nous avons également doté APAC d'un module pour modéliser le temps d'exécution des candidats de tâches en fonction de la valeur des arguments des instructions associées. Le but est d'affiner la sélection des candidats adaptés et conditionner la création de tâches à l'exécution. Sur la base d'OptiTrust [2], un cadriciel de transformation de code source, nous avons implémenté un prototype d'APAC capable de traduire un programme séquentiel écrit dans un sous-ensemble de C en un programme parallèle à base de tâches OpenMP. Une étude expérimentale préliminaire nous a permis de valider le prototype et de démontrer l'efficacité des programmes produits sur une sélection de codes sources.\r\n\r\n[1] G. Kusoglu, B. Bramas, et S. Genaud, « Automatic task-based parallelization of C++ applications by source-to-source transformations », in Compas 2020 - Conférence francophone en informatique, Lyon, France, juin 2020. [En ligne]. Disponible sur: https:\/\/inria.hal.science\/hal-02867413\r\n\r\n[2] A. Charguéraud, B. Bytyqi, D. Rouhling, et Y. A. Barsamian, « OptiTrust: an Interactive Framework for Source-to-Source Transformations », septembre 2022. [En ligne]. Disponible sur: https:\/\/inria.hal.science\/hal-03773485",
        "authors": [
            {
                "email": "berenger.bramas@inria.fr",
                "first": "Bérenger",
                "last": "Bramas",
                "affiliation": "Inria - Strasbourg"
            },
            {
                "email": "marek.felsoci@lip6.fr",
                "first": "Marek",
                "last": "Felšöci",
                "affiliation": "LiP6 - Paris"
            },
            {
                "email": "genaud@unistra.fr",
                "first": "Stéphane",
                "last": "Genaud",
                "affiliation": "ICube - Strasbourg"
            }
        ],
        "contacts": [
            {
                "email": "genaud@unistra.fr",
                "first": "Stéphane",
                "last": "Genaud",
                "affiliation": "ICube - Strasbourg"
            },
            {
                "email": "berenger.bramas@inria.fr",
                "first": "Berenger",
                "last": "Bramas",
                "affiliation": "Inria CAMUS"
            },
            {
                "email": "marek.felsoci@lip6.fr",
                "first": "Marek",
                "last": "Felšöci",
                "affiliation": "LiP6"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741884961
    },
    {
        "pid": 50,
        "title": "Fine grain energy measurement in a task based runtime system",
        "abstract_only": true,
        "abstract": "Power consumption remains a critical concern for supercomputers. For instance, El Capitan, the world's fastest supercomputer, consumes approximately 29.5 MW of power. Consequently, reducing the energy footprint of high-performance computing (HPC) applications is imperative. A first step toward this goal is monitoring power consumption. Servers may be equipped with wattmeters capable of measuring the power usage of CPUs, memory, and GPUs with a typical measurement period of tens of milliseconds. However, within such intervals, an application may execute hundreds of functions, many of which operate on the microsecond scale. Identifying power-intensive functions within these short execution times remains a significant challenge.\r\n\r\nIn this work, we leverage the repetitive nature of task-based runtime systems to estimate the power consumption of individual tasks at the microsecond scale. Specifically, we investigate the StarPU task-based runtime system for heterogeneous computing architectures.\r\n\r\nTo achieve this, we trace application execution while periodically measuring energy consumption via vendor-provided software interfaces. The collected trace includes both energy measurements and task execution data. Post-execution, we map tasks to their respective energy measurement intervals, formulating an overdetermined linear system that correlates tasks with energy consumption. Solving this system allows us to estimate the power consumption of each task idependently from their execution times.",
        "authors": [
            {
                "email": "jules.risse@inria.fr",
                "first": "Jules",
                "last": "Risse",
                "affiliation": "INRIA, Telecom Sudparis"
            }
        ],
        "contacts": [
            {
                "email": "jules.risse@inria.fr",
                "first": "Jules",
                "last": "Risse",
                "affiliation": "INRIA, Telecom Sudparis"
            }
        ],
        "topics": [
            "Architecture",
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741885323
    },
    {
        "pid": 51,
        "title": "(Don’t you) Forget about me : de l’importance du temps dans les données",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-22853c1468c4d6ffd057446dd113b8bd4d4ab7540fdb21bda8df90427af420e4",
            "timestamp": 1741967287,
            "size": 52387
        },
        "abstract_only": true,
        "abstract": "Grâce à leurs multiples capteurs, les smartphones produisent des flux de données de taille conséquente — plusieurs centaines de mégaoctets par jour pour les données de géolocalisation —qui sont difficilement stockables et traitables in situ à cause des capacités limitées des smartphones. De plus, ces séries temporelles recèlent des informations personnelles, telles que les adresses professionnelle et personnelle pour les données de géolocalisation, qui imposent la compression locale des données du smartphone. Les stratégies actuelles de compression de séries temporelles, consistant à supprimer les données les plus anciennes ou à moyenner les données par fenêtre de temps, ne permettent pas de conserver les valeurs aberrantes (outliers) parmi les données, provoquant ainsi une perte d’information.\r\nDans ce travail, nous proposons une nouvelle technique adaptive de compression de séries temporelles qui (i) conserve les valeurs aberrantes dans les données, (ii) favorise la compression des données les plus anciennes, conservant les données récentes non-compressées, et (iii) permet la compression de l’intégralité des données en cas de saturation de l’espace disque. En appliquant cette technique à un dataset de données de géolocalisation, nous montrons qu’elle permet d’exploiter ces données de façon quasi-identique à une version non-compressée, mais que la précision des résultats décroît avec l’âge des données, instaurant ainsi une dégradation progressive des données renforçant la protection de la vie privée.",
        "authors": [
            {
                "email": "remy.raes@inria.fr",
                "first": "Rémy",
                "last": "Raes",
                "affiliation": "Centre Inria de l’Université de Lille"
            },
            {
                "email": "adrien.luxey@inria.fr",
                "first": "Adrien",
                "last": "Luxey-Bitri",
                "affiliation": "Centre Inria de l’Université de Lille"
            },
            {
                "email": "romain.rouvoy@inria.fr",
                "first": "Romain",
                "last": "Rouvoy",
                "affiliation": "Centre Inria de l’Université de Lille"
            },
            {
                "email": "francois.taiani@inria.fr",
                "first": "François",
                "last": "Taïani",
                "affiliation": "Centre Inria de l’Université de Rennes"
            },
            {
                "email": "davide.frey@inria.fr",
                "first": "Davide",
                "last": "Frey",
                "affiliation": "Centre Inria de l’Université de Rennes"
            }
        ],
        "contacts": [
            {
                "email": "remy.raes@inria.fr",
                "first": "Rémy",
                "last": "Raes",
                "affiliation": "Centre Inria de l’Université de Lille"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741967287
    },
    {
        "pid": 52,
        "title": "Conception et validation de multicoeur hétérogènes avec NOC et accélérateurs matériels sur Système sur Puce FPGA",
        "abstract_only": true,
        "abstract": "Sous l'impulsion de la loi de Moore, l'évolution rapide des systèmes embarqués et du HPC pose de manière régulière la nécessité de besoins en formation en cursus ingénieur prenant en compte architecture, accélérateurs matériels, implémentation physique, et programmation parallèle. L'ENSTA Paris a mis en place depuis 5 ans un cours MPSOC (Multiprocessors System on Chip) pour les élèves ingénieurs de 3A dans lequel les élèves concoivent un multicore hétérogène de 5 coeurs , 1 ARM9 dual-core superscalaire, 4 processeurs Microblaze configurables connectés par un réseau sur puce (NOC - Network on Chip) AXI aux mémoires locales et DDR. Cette architecture est enrichie par des accélérateurs matériels dédiés générés par HLS (High Level Synthesis) et intégrés dans le multicoeur. Cette architecture est implémentée sur un Soc (System on Chip) circuit Zynq FPGA Xilinx et validée par exécution sur carte zedboard.  Le projet impose une exploration automatisée (Design Space Exploration) multiobjective du MPSOC <Max P, Min R, Min E> d'optimisation de la performance, de la minimisation des ressources et de minimisation de l'énergie. Les élèves ont le choix des applications parallèles implémentées qui souvent sont issues du traitement d'images, de la robotique et de l'IA. Ce cours exigeant permet aux élèves d'etre exposés aux problématiques d'architecture, de parallélisme, de système et d'implémentation physiques sous des contraintes de temps. La présentation exposera les travaux résultant des projets des élèves, les difficultés rencontrées les solutions apportées et les évolutions futures.",
        "authors": [
            {
                "email": "omar.hammami@ensta-paris.fr",
                "first": "Omar",
                "last": "HAMMAMI",
                "affiliation": "IP PARIS"
            }
        ],
        "contacts": [
            {
                "email": "omar.hammami@ensta-paris.fr",
                "first": "Omar",
                "last": "HAMMAMI",
                "affiliation": "IP PARIS"
            }
        ],
        "topics": [
            "Architecture",
            "Parallélisme",
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741933649
    },
    {
        "pid": 53,
        "title": "Batching the tasks of the LU factorization with partial pivoting on top of runtime systems",
        "abstract_only": true,
        "abstract": "Task-based runtime systems have demonstrated efficiency in leveraging the capabilities of large heterogeneous architectures. Many linear algebra algorithms and applications have been implemented on top of runtime systems to increase their performance. However, the LU factorization with partial pivoting has not yet been successfully implemented using task-based runtime systems. This operation is used to solve large dense linear systems in numerical simulations, such as the Maxwell equations in electromagnetism. This factorization is a major part of the High Performance Linpack (HPL) benchmark used in the TOP500 to evaluate and rank supercomputers.  \r\n\r\nWe explore solutions to implement efficient LU factorization with partial pivoting using the sequential task-flow programming model. These solutions have been implemented in the dense linear algebra library Chameleon on top of the StarPU runtime system.  \r\n\r\nWe showed that, due to the pivoting strategy, this algorithm generates a large number of very small tasks, which usually overloads the runtime system and makes it inefficient. With a naive task batching strategy, we improved the efficiency and reduced the number of tasks. We propose solutions to adapt the batch size to the granularity of the tasks.  \r\nIn order to do that, we first distinguish two types of tasks and set an adapted batch size for each. Then, we introduce a heuristic based on the number of operations per tasks to adapt the batch size to the computational complexity of the tasks during the factorization.\r\n\r\nExperiments conducted on our cluster with these optimizations show that our LU factorization with partial pivoting asymptotically reaches about 96% of the performance of the non-pivoting algorithm. Thanks to the adaptive batch size mechanism, the performance peak is reached even faster.",
        "authors": [
            {
                "email": "alycia.lisito@inria.fr",
                "first": "Alycia",
                "last": "Lisito",
                "affiliation": "Eviden, Inria, LaBRI, Université de Bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "alycia.lisito@inria.fr",
                "first": "Alycia",
                "last": "Lisito",
                "affiliation": "Eviden, Inria, LaBRI, Université de Bordeaux"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741940708
    },
    {
        "pid": 54,
        "title": "Placement de tâches MPI pour minimiser la charge par carte réseau et améliorer la localité",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-fe1ddf3a451753ff1976c82d555bc3146f62f78c47ed1f3d8c920e3e4993fcea",
            "timestamp": 1741945621,
            "size": 38218
        },
        "abstract_only": true,
        "abstract": "Avec l’augmentation en taille et en complexité des supercalculateurs, il est devenu crucial de faire correspondre le schéma de communication des applications à la topologie matérielle sous-jacente. Cette optimisation permet de minimiser le temps passé dans les communications et de limiter la contention générée sur le réseau. Certaines implémentations MPI tiennent compte de la topologie, principalement intra-nœud, pour adapter les communications effectuées au sein des opérations collectives de communication. Cependant, ces implémentations ne tiennent pas compte de la topologie réseau de manière générique, ni de la charge de communication générée sur les cartes réseau. Cette métrique pourrait aider à améliorer les performances car, bien que les caractéristiques du réseau indiquent une certaine bande passante, la bande passante en sortie d’un nœud reste inférieur. Lors d’une succession de communications collectives, le même algorithme sera appliqué, ce qui aura comme conséquence de générer un déséquilibre sur la charge de communication à effectuer par carte réseau. Ainsi, lors d’un broadcast binomial enchaîné en conservant la même racine, le volume de communications effectuées à partir du nœud sur lequel la racine est située sera bien supérieur à celui des autres nœuds, ce qui induit une surcharge sur la carte réseau du nœud et ralentit les communications dépendantes des données envoyées par la racine.\r\nAfin d’adapter la charge de communication par nœud lors de communications enchainées, nous mettons au point un outil de renumérotation des processus MPI pour les opérations de communication collectives. La métrique principale que nous cherchons à minimiser est la charge de communication par nœud. Cependant, la localité des communication est une autre métrique que nous essayons de prendre en compte. Les résultats préliminaires montrent que notre outil de renumérotation des rangs des processus MPI permet de réduire la charge par carte réseau ainsi que le nombre de messages remontant dans la topologies réseau dans le cas d’un enchaînement de broadcasts ou de scatters binomial.",
        "authors": [
            {
                "email": "brice.goglin@inria.fr",
                "first": "Brice",
                "last": "GOGLIN",
                "affiliation": "inria"
            },
            {
                "email": "julien.jaeger@cea.fr",
                "first": "Julien",
                "last": "JAEGER",
                "affiliation": "CEA"
            },
            {
                "email": "guillaume.mercier@inria.fr",
                "first": "Guillaume",
                "last": "MERCIER",
                "affiliation": "inria"
            },
            {
                "email": "thibaut.pepin@cea.fr",
                "first": "Thibaut",
                "last": "PEPIN",
                "affiliation": "CEA"
            }
        ],
        "contacts": [
            {
                "email": "julien.jaeger@cea.fr",
                "first": "Julien",
                "last": "Jaeger",
                "affiliation": "CEA"
            },
            {
                "email": "thibaut.pepin33@gmail.com",
                "first": "Thibaut",
                "last": "Pepin",
                "affiliation": "CEA"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741945621
    },
    {
        "pid": 55,
        "title": "Software transactional memory for parallel meshing applications",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-afa066eab2fa5fdcacb46fc2a672c95777f145d68c683f1810a7ef8d44960f78",
            "timestamp": 1741945998,
            "size": 304980
        },
        "abstract_only": false,
        "abstract": "Parallelizing remeshing algorithms presents significant challenges due to their dynamic nature, where operations alter data that dictates subsequent actions. We aim to address the problems\r\ncreated by these dynamic workloads without relying on conflict avoidance, and propose an\r\nalternative approach to ensure data integrity in this context. To that end, we leverage specific\r\ndata structure, synchronization mechanism and programming language features. We bench-\r\nmark our implementation with two algorithms: a topology-heavy kernel with non-trivial parallelization, and a simpler vertex relaxation routine. Preliminary results suggest STM effectively manages conflicts, maintaining structure validity under concurrency. Our work offers a\r\nrobust framework for parallel remeshing, balancing correctness and scalability.",
        "authors": [
            {
                "email": "isaie.muron@cea.fr",
                "first": "Isaïe",
                "last": "Muron",
                "affiliation": "CEA"
            },
            {
                "email": "franck.ledoux@cea.fr",
                "first": "Franck",
                "last": "Ledoux",
                "affiliation": "CEA"
            },
            {
                "email": "cedric.chevalier@cea.fr",
                "first": "Cédric",
                "last": "Chevalier",
                "affiliation": "CEA"
            }
        ],
        "contacts": [
            {
                "email": "isaie.muron@cea.fr",
                "first": "Isaïe",
                "last": "Muron",
                "affiliation": "CEA"
            },
            {
                "email": "cedric.chevalier@cea.fr",
                "first": "Cédric",
                "last": "Chevalier",
                "affiliation": "CEA"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741945998
    },
    {
        "pid": 56,
        "title": "Automatic Multi-Versioning of Computation Kernels",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-c126f537b02fb8fde7e26694e2a102726a2dd29edce9d2c516ac917b83a1d74d",
            "timestamp": 1741967377,
            "size": 128651
        },
        "abstract_only": false,
        "abstract": "Program optimization is crucial for scientific programs that can run for several days or even weeks. Generating code tailored for a specific target architecture is key to exploiting the hardware with maximum efficiency, both in terms of performance and energy savings, allowing more precise and complete experiments to be run in a shorter time while consuming less energy.\r\nGeneralist optimization tools have become better over the years, enabling automatic generation of efficient machine code. These tools aim to achieve good performance on average across a range of different programs, and are thus missing optimization decisions that may be beneficial for specific programs, or runs of a program with a given input data.\r\nApproaches such as multi-versioning or iterative compilation aim to specialize optimizations to specific programs and input data, but are hard to put into practice, and often only rely on static information for generating code. Our approach aims to leverage knowledge from the execution contexts of a program to derive optimization strategies tailored to these specific execution contexts, and to automatically generate code using known techniques such as polyhedral loop transformations.",
        "authors": [
            {
                "email": "raphael.colin@inria.fr",
                "first": "Raphaël",
                "last": "Colin",
                "affiliation": "Inria"
            }
        ],
        "contacts": [
            {
                "email": "raphael.colin@inria.fr",
                "first": "Raphaël",
                "last": "Colin",
                "affiliation": "Inria"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741947644
    },
    {
        "pid": 57,
        "title": "Méduse : reproduire des circuits obsolètes",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-e645062ff571556ae5ed86347d88f31e460522257daaa5972ef6ea38719c9745",
            "timestamp": 1741950804,
            "size": 270034
        },
        "abstract_only": false,
        "abstract": "Afin de restaurer et recréer des systèmes informatiques des années 1970 et 1980, nous proposons un outillage matériel et logiciel pour assister à la rétro-ingénierie et la réplication de circuits imprimés en électronique numérique, la Méduse. À partir de séries de mesures de continuité électrique entre points du circuit, la Méduse produit un graphe des connexions ou netlist, qui peut être exporté sous forme de code Verilog pour analyse, simulation, ou synthèse sur FPGA. Son utilisation est illustrée par la rétro-conception de cartes d'un mini-ordinateur Mitra 125 de 1978.",
        "authors": [
            {
                "email": "caroline.collange@inria.fr",
                "first": "Caroline",
                "last": "Collange",
                "affiliation": "IRISA, Inria, CNRS, Univ Rennes"
            }
        ],
        "contacts": [
            {
                "email": "caroline.collange@inria.fr",
                "first": "Caroline",
                "last": "Collange",
                "affiliation": "Inria"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741950804
    },
    {
        "pid": 58,
        "title": "Connecting Kokkos with the Polyhedral Model",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-66f33f3da69ed8202398af08a1b26d7ba4c5672c40d664a5c4f1b77d14d5246d",
            "timestamp": 1741954440,
            "size": 132364
        },
        "abstract_only": false,
        "abstract": "The modernization of programming paradigms force the scientists to change their default programming language for writing simulations (for examples, using C++ instead of Fortran). Writing simulation codes is a complex task and in order to improve performance, developers need to program in a way that utilizes all of the resources of a supercomputer. Modern C++ libraries such as Kokkos are becoming increasingly popular as they help ease development of parallel applications.\r\nAlthough Kokkos provides simplified access to parallel programming for heterogeneous hardware targets, it does not include advanced optimizing code transformations. In this paper, we propose to make advanced loop optimizing and parallelizing transformations based on the polyhedral model available for Kokkos.",
        "authors": [
            {
                "email": "ugo.battiston@inria.fr",
                "first": "Ugo",
                "last": "BATTISTON",
                "affiliation": "INRIA"
            }
        ],
        "contacts": [
            {
                "email": "ugo.battiston@inria.fr",
                "first": "Ugo",
                "last": "BATTISTON",
                "affiliation": "INRIA"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741954440
    },
    {
        "pid": 60,
        "title": "A Probabilistic Approach to Explore Spatial and Temporal Locality for Lossless Compression in CFD Simulations",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-9f727b6393cdfc5f9a815c1cb4ec4edc44dd5fe0e562ce5d5bd97f9519f217c1",
            "timestamp": 1741955788,
            "size": 309342
        },
        "abstract_only": false,
        "abstract": "Modern applications are able to tackle larger workloads than ever before, mainly due to the democratization of GPU-based architectures. Despite this, the overall ratio of storage space per computing unit is diminishing, turning memory management into a limiting factor.\r\nCompression has been explored as a solution for handling large datasets in visualization, data storage, etc. However, few works address this topic for numerical simulation purposes. \r\nIn this paper, we explore the challenges of leveraging lossless compression to reduce memory usage in GPU CFD simulations. We propose a compact data representation based on a common predictor + delta algorithm to split the data into a base with coarser resolution than the initial mesh and a compressible residual.\r\n\r\nWe assume that spatial coherence far from discontinuities is strong enough for a value at a given position to be a good candidate to predict neighboring values. Temporal coherence can then be leveraged by transferring the residual between the host and the GPU separately from the base, which may be reused during several iterations.\r\n\r\nTo evaluate the relevance of our method, we present the basis of a probabilistic description of simulation data to assess the prospective compression gains achievable by a given approximation.",
        "authors": [
            {
                "email": "teodora.hovi@cea.fr",
                "first": "Téodora",
                "last": "Hovi",
                "affiliation": "Université de Reims Champagne-Ardennes | CEA"
            },
            {
                "email": "francois.letierce@cea.fr",
                "first": "François",
                "last": "Letierce",
                "affiliation": "CEA"
            },
            {
                "email": "laurent.lucas@univ-reims.fr",
                "first": "Laurent",
                "last": "Lucas",
                "affiliation": "Université de Reims Champagne-Ardennes"
            }
        ],
        "contacts": [
            {
                "email": "teodora.hovi@cea.fr",
                "first": "Téodora",
                "last": "Hovi",
                "affiliation": "Université de Reims Champagne-Ardennes | CEA"
            },
            {
                "email": "francois.letierce@cea.fr",
                "first": "François",
                "last": "Letierce",
                "affiliation": "CEA"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741955787
    },
    {
        "pid": 61,
        "title": "FedE-ator : A framework for energy consumption analysis of federated learning in distributed systems",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-c598fedaf9f062962982ffa683ad95ad96ee9bd4c2333cb1bfbc71a21166e104",
            "timestamp": 1741992753,
            "size": 181554
        },
        "abstract_only": false,
        "abstract": "Federated learning (FL) is a machine learning (ML) technique with multiple entities collaboratively training a model while keeping their data decentralized. The growth of ML is driven by increasingly large and computationally demanding ML models and datasets, leads to substantial energy consumption. FL also faces this challenge, with the distributed and complex design of FL, simply estimating energy by simulation may not be enough, accurately monitoring its energy usage becomes necessary. This work focuses on developing FedE-ator, a framework designed to measure the actual energy consumption of FL processes in real-world distributed systems. As a first step in validating the framework's usability, we conducted an experiment on the CIFAR10 dataset, systematically varying FL settings (server strategies, client models) and hardware configurations (CPU frequencies), with multiple repetitions. The experiment was carried out on the Grid'5000 (g5k) platform.",
        "authors": [
            {
                "email": "huong.do-mai@irit.fr",
                "first": "Mai Huong",
                "last": "Do",
                "affiliation": "IRIT, UNIVERSITY PAUL SABATIER"
            },
            {
                "email": "millian.poquet@irit.fr",
                "first": "Millian",
                "last": "Poquet",
                "affiliation": "IRIT, UNIVERSITY PAUL SABATIER"
            },
            {
                "email": "georges.da-costa@irit.fr",
                "first": "Georges",
                "last": "Da Costa",
                "affiliation": "IRIT, UNIVERSITY PAUL SABATIER"
            }
        ],
        "contacts": [
            {
                "email": "huong.do-mai@irit.fr",
                "first": "Mai Huong",
                "last": "Do",
                "affiliation": "IRIT, UNIVERSITY PAUL SABATIER"
            }
        ],
        "topics": [
            "Parallélisme",
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741956104
    },
    {
        "pid": 62,
        "title": "Optimisation de la contraction de tenseurs grâce à l'utilisation de supports d'exécution appliquée à la méthode Coupled-Cluster",
        "abstract_only": true,
        "abstract": "Les systèmes de tâches et de tâches distribuées ont prouvé leur capacité à utiliser les ressources de systèmes matériels distribués et hétérogènes. Ils permettent d'exposer le maximum de parallélisme, d'utiliser pleinement et efficacement les ressources et de passer à l'échelle, au moins dans le cas du calcul matriciel.\r\nUne nouvelle question se pose alors quant à la généralisation de ce résultat au calcul tensoriel. En effet l'algèbre multidimensionnelle prend une place très importante dans le calcul haute performance, en particulier récemment avec le développement très rapide de l'intelligence artificielle. D'autre part d'autres domaines comme la chimie quantique calculatoire, qui nous intéresse ici, utilisent des tenseurs depuis plus longtemps.\r\nDans notre étude, nous étendons les capacités du logiciel d'algèbre linéaire dense Chameleon à la contraction de tenseurs. Pour notre application, on se concentre sur des tenseurs à faible nombre de dimensions (inférieur ou égal à 6) mais dont les dimensions sont grandes, que l'on trouve dans la méthode coupled-cluster[1].\r\nLa contraction de tenseurs apporte plusieurs nouvelles difficultés par rapport au produit de matrices. En effet, les dimensions qui sont contractées sont ordonnées arbitrairement. Il est quand même possible de se ramener à des opérations matricielles classiques, mais il est potentiellement nécessaire d'effectuer des permutations\/transpositions de nos tenseurs. Cela permet de profiter des opérations matricielles optimisées de Chameleon. Par ailleurs, à mesure que le nombre de dimensions augmente, les tenseurs deviennent très rapidement trop grands. Il faut donc également répondre au problème de stockages des tenseurs.\r\nPour appliquer notre système de tâches à ce problème, nous mettons en évidence les transformations élémentaires et composables qui permettent de réaliser toutes les permutations possibles des dimensions des tenseurs, ainsi que leur implémentation au dessus du support d'exécution StarPU.\r\nNous appliquons notre implémentation au calcul des résidus de la méthode coupled-cluster avec density fitting afin d'en illustrer l'efficacité.\r\n\r\n[1] Deprince, Eugene & Sherrill, C. (2013). Accuracy and Efficiency of Coupled-Cluster Theory Using Density Fitting\/Cholesky Decomposition, Frozen Natural Orbitals, and a t1-Transformed Hamiltonian. Journal of Chemical Theory and Computation. 9. 2687–2696. 10.1021\/ct400250u.",
        "authors": [
            {
                "email": "brieuc.nicolas@inria.fr",
                "last": "Brieuc",
                "affiliation": "Centre Inria de l'Université de Bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "brieuc.nicolas@inria.fr",
                "first": "Brieuc",
                "last": "Nicolas",
                "affiliation": "Centre Inria de l'Université de Bordeaux"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741957723
    },
    {
        "pid": 63,
        "title": "Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized Modest Computer Cluster",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-45498c4fc28098af79f89cc96d9f9eecfad4976eac8d3b31d5c4a69e63b87688",
            "timestamp": 1741963057,
            "size": 68680
        },
        "abstract_only": true,
        "abstract": "Analyzing gigapixel images is widely recognized as computationally demanding. That is why we introduce PyramidAI, a technique for analyzing gigapixel images with reduced computational cost. It efficiently reduces computational requirements by adopting a gradual approach to image analysis, beginning with lower resolutions and progressively concentrating on regions of interest for detailed examination at higher resolutions. In addition to our adaptive resolution scheme, we implemented two strategies for tuning the accuracy-performance trade-off, validated against the Camelyon 16 dataset composed of biomedical images. Our results demonstrate that PyramidAI substantially decreases the amount of processed data required for analysis by up to 2.65x, while preserving the accuracy in identifying relevant sections on a single computer. To ensure democratization of gigapixel image analysis, we evaluated the potential to use mainstream computers to perform the computation by exploiting the high parallelism potential of the approach. Our simulator emphasized that synchronization to balance the load before initiating the next resolution level tiles analysis is not required. The most efficient and stable data distribution strategy for this use-case is the Round-Robin, while the load balancing technique is the graph leaves work-stealing one when the number of available workers increases. Our implementation using the DecentralizePy python framework, highlighted the same conclusions in a real-world setting. It offers the possibility to reduce from a few minutes when using 12 workers. This method offers a practical solution for efficient large-scale image analysis.",
        "authors": [
            {
                "email": "marie.reinbigler@telecom-sudparis.eu",
                "first": "Marie",
                "last": "Reinbigler",
                "affiliation": "Télécom Sudparis, Institut Polytechnique de Paris, Inria"
            }
        ],
        "contacts": [
            {
                "email": "marie.reinbigler@telecom-sudparis.eu",
                "first": "Marie",
                "last": "Reinbigler",
                "affiliation": "Télécom Sudparis, Institut Polytechnique de Paris, Inria"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741960574
    },
    {
        "pid": 64,
        "title": "Uplifting ocean models PDE solvers with Poseidon a Domain Specific source-to-source compiler.",
        "abstract_only": true,
        "abstract": "A wide range of heterogeneous user communities develop and use ocean model simulations, such as those developing biological ice models and components accounting for other natural phenomena and those using models to predict climate in a hundred years. Consequently, it is not easy to develop code that is efficient for all these use cases in terms of extensibility, performance portability, and ease of use.\r\n\r\nResearch has been conducted mainly on disruptive approaches, particularly using domain-specific languages (DSLs) and their attached domain-specific (DS) compiler optimization passes. Knowing more about the code behavior allows DSL compilers to be much more greedy in their optimization, which will be more efficient on a given architecture and potentially improve performance portability.\r\n\r\nHowever, existing code is extremely valuable, complex, and large, particularly in the ocean modeling community.\r\nThus, using a domain-specific language to rewrite an entire, or just some parts, of an existing code is often evaluated as too costly because it requires significant human resources and time investments.\r\n\r\nIn this work, we investigate our source-to-source Poseidon compiler. It uses a new methodology by considering the existing code of the dynamical cores of ocean models as an embedded DSL. In this way, we can enrich the information extracted from the code with domain-specific information. Helping HPC and numerics experts discuss codesign, Poseidon effectively lifts various restrictions compilers suffer from performing in-depth optimizations and code analysis.\r\n\r\nWe have applied Poseidon on three cases, each time only focusing on the Dynamical Core part of the code:\r\n* A tutorial Fortran code (500LOC) solving a 2D shallow water model with Runge-Kutta (RK2) method.\r\n* Schweinshaxe, a (13kLOC) Fortran Object Oriented code, also solving 2D shallow water model: Arakawa C grid and the Multi-step time integrator (AB3-AM4) from CROCO.\r\n* CROCO, a >100kLOC legacy-Fortran code. We focus on the same kind of solver as with Schweinshaxe.\r\n\r\nApplying some state-of-the-art compiler optimization passes, with the domain-specific knowledge we added, we can transform existing Fortran code without any parallelization information to GPU with parallelization and performance optimizations beyond regular compilers.",
        "authors": [
            {
                "email": "hugo.brunie@univ-grenoble-alpes.fr",
                "first": "Hugo",
                "last": "Brunie",
                "affiliation": "University Of Grenoble Alpes"
            },
            {
                "email": "martin.schreiber@univ-grenoble-alpes.fr",
                "first": "Martin",
                "last": "Schreiber",
                "affiliation": "University Of Grenoble Alpes"
            },
            {
                "email": "julien.remy@univ-grenoble-alpes.fr",
                "first": "Julien",
                "last": "Remy",
                "affiliation": "University Of Grenoble Alpes"
            }
        ],
        "contacts": [
            {
                "email": "hugo.brunie@univ-grenoble-alpes.fr",
                "first": "Hugo",
                "last": "Brunie",
                "affiliation": "University Of Grenoble Alpes"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741961046
    },
    {
        "pid": 65,
        "title": "A fast implementation of 3D wavelet compression on GPU",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-b7909643f0fd32ee5e3dbec7f5f353ba6ee1d99108424c6995ae45a0b6707fa5",
            "timestamp": 1741969520,
            "size": 5250729
        },
        "abstract_only": false,
        "abstract": "GPUs are widely used in high-performance computing for performing computationally inten-sive tasks. However, GPUs have fixed memory capacities that are much smaller than those ofCPUs, often limiting the full exploitation of their computational power. Moreover, the memorybus between CPUs and GPUs is an order of magnitude slower, hindering the efficient feedingof data to GPUs during computation. To overcome these limitations, we developed an efficientcompression kernel for GPUs. Our implementation relies on the wavelet method used in theJPEG2000 format and is highly optimized in CUDA using the GPU’s shared memory. We de-monstrate that our kernel can compress and decompress data faster than transferring data overthe PCIe bus, allowing to store more data and transferring them faster.",
        "authors": [
            {
                "email": "atoli.huppe@inria.fr",
                "first": "Atoli",
                "last": "Huppé",
                "affiliation": "INRIA"
            },
            {
                "email": "berenger.bramas@inria.fr",
                "first": "Bérenger",
                "last": "Bramas",
                "affiliation": "INRIA"
            },
            {
                "email": "clement.flint@inria.fr",
                "first": "Clément",
                "last": "Flint",
                "affiliation": "INRIA"
            },
            {
                "email": "genaud@unistra.fr",
                "first": "Stéphane",
                "last": "Genaud",
                "affiliation": "ICube - Strasbourg"
            }
        ],
        "contacts": [
            {
                "email": "genaud@unistra.fr",
                "first": "Stéphane",
                "last": "Genaud",
                "affiliation": "ICube - Strasbourg"
            },
            {
                "email": "berenger.bramas@inria.fr",
                "first": "Berenger",
                "last": "Bramas",
                "affiliation": "Inria CAMUS"
            },
            {
                "email": "atoli.huppe@inria.fr",
                "first": "Atoli",
                "last": "Huppé",
                "affiliation": "INRIA"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741969520
    },
    {
        "pid": 66,
        "title": "Modélisation et projection de performances d'applications parallèles sur environnement GPU",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-77331c6194e21d7d4eae0934a16f1d6bf152e11de0357c24380aefedb94cfe7a",
            "timestamp": 1741962865,
            "size": 54695
        },
        "abstract_only": true,
        "abstract": "With the advent of heterogeneous systems that combine CPUs and GPUs, designing a supercomputer becomes more and more complex. The hardware characteristics of GPUs significantly impact the performance. Choosing the GPU that will maximize performance for a limited budget is tedious because it requires predicting the performance on a non-existing hardware platform.\r\n\r\nIn this paper, we propose a new methodology for predicting the performance of kernels running on GPUs. This method analyzes the behavior of an application running on an existing platform, and\r\nprojects its performance on another GPU based on the target hardware characteristics. The performance projection relies on a hierarchical roofline model as well as on a comparison of the kernel's assembly instructions of both GPUs to estimate the operational intensity of the target GPU.\r\n\r\nWe demonstrate the validity of our methodology on modern NVIDIA GPUs on several mini-applications. The experiments show that the performance is predicted with a mean absolute percentage error of 20.3~\\% for LULESH, 10.2~\\% for MiniMDock, and 5.9~\\% for Quicksilver.",
        "authors": [
            {
                "email": "lucas.vanlanker@cea.fr",
                "first": "Lucas",
                "last": "Van Lanker",
                "affiliation": "CEA, DAM, DIF, F-91297 Arpajon, France"
            },
            {
                "email": "hugo.taboada@cea.fr",
                "first": "Hugo",
                "last": "Taboada",
                "affiliation": "CEA, DAM, DIF, F-91297 Arpajon, France"
            },
            {
                "email": "elisabeth.brunet@telecom-sudparis.eu",
                "first": "Elisabeth",
                "last": "Brunet",
                "affiliation": "Télécom Sud Paris, Inria - Evry"
            },
            {
                "email": "francois.trahay@telecom-sudparis.eu",
                "first": "François",
                "last": "Trahay",
                "affiliation": "Télécom Sud Paris, Inria - Evry"
            }
        ],
        "contacts": [
            {
                "email": "elisabeth.brunet@telecom-sudparis.eu",
                "first": "Elisabeth",
                "last": "Brunet",
                "affiliation": "Télécom Sud Paris, Inria - Evry"
            },
            {
                "email": "Hugo.TABOADA@cea.fr",
                "first": "Hugo",
                "last": "Taboada",
                "affiliation": "CEA"
            },
            {
                "email": "francois.trahay@telecom-sudparis.eu",
                "first": "Francois",
                "last": "Trahay",
                "affiliation": "Télécom SudParis"
            },
            {
                "email": "lucas.vanlanker@cea.fr",
                "first": "Lucas",
                "last": "Van Lanker",
                "affiliation": "CEA, DAM, DIF, F-91297 Arpajon, France"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741962865
    },
    {
        "pid": 67,
        "title": "Rejeu de leviers environnementaux dans des infrastructures de Clouds et continuums",
        "abstract_only": true,
        "abstract": "Les datacentres ont une empreinte mondiale environnementale significative. Celle-ci se décline en plusieurs aspects, comme l'émission carbone, l'utilisation de terres et métaux rares, ou encore la consommation d'eau et d'électricité. Pour gérer ces impacts, les fournisseurs de datacentres ont à leur portée un éventail de techniques, appelées \"leviers\" qui permettent de changer le comportement des composants des datacentres. L'étude de leviers clé, tels que l'extinction ou le ralentissement de ressources inutilisées, ou la consolidation de VMs, jouent un rôle important dans l'atténuation de ces empreintes. Cependant, la combinaison de ces leviers est difficile à cause de l'hétérogénéité de composants et services présents dans les datacentres, et l'incompatibilité d'application entre certains leviers. Aucune méthode efficace n'existe jusqu'ici pour résoudre ce problème d'orchestration de leviers à grande échelle.\r\n\r\nMa thèse, qui a débuté en octobre 2024, s'inscrit alors dans ce contexte, en s'appuyant notamment sur les travaux d'anciens doctorant de mon équipe sur la modélisation, l'orchestration et la combinaison de leviers hétérogènes. Elle consiste en :\r\n- l'extension du modèle existant pour prendre en compte une grande diversité de leviers, leurs impacts et leurs contraintes de placement,\r\n- la conception et le développement d'algorithmes d'orchestration automatique de leviers à grande échelle,\r\n- l'évaluation des atténuations d'impacts environnementaux\r\n- l'extension de la méthode au continuum Cloud-Fog-Edge\r\nUne importance sera aussi accordée à la modélisation d'une diversité de scénarios pouvant se présenter aux fournisseurs de datacentres. Les prises de décisions lors de ces scénarios pourront alors être simulées et rejouées de manière à minimiser les empreintes associées.\r\n\r\nJ'ai donc pu formaliser dans ces premiers mois de thèse une formalisation du \"problème de placement de leviers\", en présentant aussi une analyse théorique de sa complexité, une stratégie de gestion des leviers, ainsi qu'une première validation de cette méthode de résolution à grande échelle. Dans ce workshop, ma présentation consisterait alors en une présentation de mon sujet et de mon plan de thèse, ainsi que des premiers résultats obtenus.",
        "authors": [
            {
                "email": "thomas.stavis@ens-lyon.fr",
                "first": "Thomas",
                "last": "Stavis",
                "affiliation": "Univ Lyon1, Inria, ENS de Lyon, CNRS Lyon, France"
            },
            {
                "email": "laurent.lefevre@inria.fr",
                "first": "Laurent",
                "last": "Lefèvre",
                "affiliation": "Univ Lyon1, Inria, ENS de Lyon, CNRS Lyon, France"
            },
            {
                "email": "anne-cecile.orgerie@inria.fr",
                "first": "Anne-Cécile",
                "last": "Orgerie",
                "affiliation": "Univ. Rennes, Inria, CNRS, IRISA, Rennes, France"
            }
        ],
        "contacts": [
            {
                "email": "thomas.stavis@ens-lyon.fr",
                "first": "Thomas",
                "last": "Stavis",
                "affiliation": "Univ Lyon1, Inria, ENS de Lyon, CNRS Lyon, France"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741971938
    },
    {
        "pid": 68,
        "title": "Efficient and portable neutron transport sweep kernels on multicore CPUs and GPUs using Kokkos",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-042913dcaca63f29ad2c72ae788d48bfe7694552acc3fc4805220ccd577b4bc8",
            "timestamp": 1741966306,
            "size": 38240
        },
        "abstract_only": true,
        "abstract": "La résolution de l’équation de transport neutronique avec la méthode des ordonnées discrètes\r\n(SN) en angle et une discrétisation de type Galerkine Discontinue (DG) en espace repose sur\r\nune imbrications d’algorithmes itératifs. À chaque itération interne, un balayage ordonné du\r\nmaillage spatial pour un ensemble de directions permet de calculer l’itéré suivant, par l’as-\r\nsemblage et la résolution d’un petit système linéaire local dans chaque cellule. Le noyau de\r\nbalayage étant la partie la plus chronophage de la résolution, il est essentiel de le paralléliser\r\nefficacement. Avec la part croissante des GPUs dans les architectures de calculs modernes, il\r\ndevient crucial d’exploiter aux mieux ces accélérateurs sous peine de se priver de la majorité\r\nde la puissance de calcul offerte par le hardware. Dans ce travail, nous présentons différentes\r\nimplémentations parallèles du balayage pour les CPUs multicœurs et les GPUs dans le cadre\r\nde schémas spatiaux d’ordres élevés. Ces implémentations sont basées sur le framework C++\r\nKokkos pour la portabilité sur différentes architectures. Sur CPU, une stratégie de vectorisation\r\nportable basée sur les types SIMD de Kokkos, un réarrangement de la mémoire et des boucles\r\ndans le balayages, ainsi qu’une implémentation légère de tâches au sein de Kokkos permettent\r\nd’obtenir de très bonnes performances sur CPU. Sur GPU, une adaptation de l’algorithme est\r\nnécessaire, et passe par une maximisation du degré de parallélisme disponible à chaque étape,\r\net un assemblage\/résolution batchés des systèmes linéaires locaux. Après optimisation du sol-\r\nveur linéaire, les performances sur GPU sont très largement limitées par la bande passante de\r\nla mémoire globale, notamment pendant la phase d’assemblage.",
        "authors": [
            {
                "email": "gabriel.suau@cea.fr",
                "first": "Gabriel",
                "last": "Suau",
                "affiliation": "CEA"
            }
        ],
        "contacts": [
            {
                "email": "gabriel.suau@cea.fr",
                "first": "Gabriel",
                "last": "Suau",
                "affiliation": "CEA"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741973352
    },
    {
        "pid": 69,
        "title": "Fork-nox: a new virtualization technique to enforce system security",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-a5c67ff6046be2669e109462e580496daba5b331f8c163d0f42b742d53abeaff",
            "timestamp": 1741977185,
            "size": 39579
        },
        "abstract_only": true,
        "abstract": "Operating system security is a major concern, because vulnerabilities discovered at this layer would endanger the whole software stack.\r\nDespite many efforts, kernel security mechanisms are not isolated enough from the kernel to offer strong integrity guarantees.\r\nIndeed, the trusted computing base of these mechanisms incorporates by design the whole kernel and its drivers.\r\nIn the case of Linux, this represents nearly 25 millions lines of code, difficult to trust and hard to debug.\r\nWith this project, we argue that integrity guarantees on kernel security can be achieved using a more privileged layer without sacrificing the semantics.\r\nBuilding on this idea, we employ hardware virtualization to run Fork-nox, a small security hypervisor.\r\n\r\nWe implement Fork-nox as a Linux module, which separates itself from the kernel to gain more privileges, deploying necessary hypervisor features.\r\nMoreover, we have designed it to be a minimal, independent and auditable codebase.\r\n\r\nWith this project, we demonstrate the efficiency of our tool by enforcing memory protection, and evaluating the hardening of kernel security interposition.\r\nHence, our goal is to ensure kernel security system's integrity.\r\nFork-nox creates a trusted computing base in the kernel to safely handle security threats.\r\nThus, we envision a hardened hook mechanism to perform advanced security checks and optionally livepatching through a safe interface allowing to define on-demand policies.",
        "authors": [
            {
                "email": "jean-francois.dumollard@telecom-sudparis.eu",
                "first": "Jean-François",
                "last": "Dumollard",
                "affiliation": "Benagil Team, Inria Saclay, Télécom SudParis, Institut-Polytechnique de Paris, Institut Mines-Télécom"
            },
            {
                "email": "nicolas.derumigny@inria.fr",
                "first": "Nicolas",
                "last": "Derumigny",
                "affiliation": "Benagil Team, Inria Saclay"
            },
            {
                "email": "mathieu.bacou@telecom-sudparis.eu",
                "first": "Mathieu",
                "last": "Bacou",
                "affiliation": "Télécom SudParis, IP Paris, Inria Saclay"
            },
            {
                "email": "gael.thomas@inria.fr",
                "first": "Gaël",
                "last": "Thomas",
                "affiliation": "Benagil Team, Inria Saclay"
            }
        ],
        "contacts": [
            {
                "email": "mathieu.bacou@telecom-sudparis.eu",
                "first": "Mathieu",
                "last": "Bacou",
                "affiliation": "Télécom SudParis, IP Paris, Inria Saclay"
            },
            {
                "email": "jean-francois.dumollard@telecom-sudparis.eu",
                "first": "Jean-François",
                "last": "Dumollard",
                "affiliation": "Benagil Team, Inria Saclay, Télécom SudParis, Institut-Polytechnique de Paris, Institut Mines-Télécom"
            },
            {
                "email": "nicolas.derumigny@inria.fr",
                "first": "Nicolas",
                "last": "Derumigny",
                "affiliation": "Benagil Team, Inria Saclay"
            },
            {
                "email": "gael.thomas@inria.fr",
                "first": "Gaël",
                "last": "Thomas",
                "affiliation": "Benagil Team, Inria Saclay"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741967784
    },
    {
        "pid": 70,
        "title": "Exploring C++ Standard Parallelism Features for GPU Programming in a Particle-In-Cell Application",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-79f23eb513a798574eeef0833899f079d0421f6eb5f2500c554f1f515956dbd3",
            "timestamp": 1741970766,
            "size": 2490875
        },
        "abstract_only": false,
        "abstract": "The C++17 standard introduced stdpar as a high-level parallelism model, which NVIDIA la-\r\nter implemented to support GPUs in heterogeneous architectures. This study evaluates NVI-\r\nDIA’s stdpar implementation on GPUs within a GPU-based Particle-In-Cell code, comparing it\r\nto Thrust and Kokkos in terms of performance and ease of use. Our results indicate that while\r\nstdpar significantly enhances developer productivity, it remains less efficient in terms of raw\r\nperformance. It shows an advantage at low particle counts but loses efficiency as the count\r\nincreases, becoming up to 1.88× slower than Kokkos and 1.34× slower than Thrust on GH200.",
        "authors": [
            {
                "email": "ester.elkhoury@cea.fr",
                "first": "Ester",
                "last": "El Khoury",
                "affiliation": "Commissariat à l'énergie atomique et aux énergies alternatives, Saclay, France"
            },
            {
                "email": "mathieu.lobet@cea.fr",
                "first": "Mathieu",
                "last": "Lobet",
                "affiliation": "Commissariat à l'énergie atomique et aux énergies alternatives, Saclay, France"
            },
            {
                "email": "Laurent.Colombet@cea.fr",
                "first": "Laurent",
                "last": "Colombet",
                "affiliation": "Commissariat à l'énergie atomique et aux énergies alternatives, DAM Ile de France, France"
            },
            {
                "email": "Julien.Bigot@cea.fr",
                "first": "Julien",
                "last": "Bigot",
                "affiliation": "Commissariat à l'énergie atomique et aux énergies alternatives, Saclay, France"
            }
        ],
        "contacts": [
            {
                "email": "ester.elkhoury@cea.fr",
                "first": "Ester",
                "last": "El Khoury",
                "affiliation": "Commissariat à l'énergie atomique et aux énergies alternatives"
            },
            {
                "email": "mathieu.lobet@cea.fr",
                "first": "Mathieu",
                "last": "Lobet",
                "affiliation": "Commissariat à l'énergie atomique et aux énergies alternatives"
            },
            {
                "email": "Laurent.Colombet@cea.fr",
                "first": "Laurent",
                "last": "Colombet",
                "affiliation": "Commissariat à l'énergie atomique et aux énergies alternatives"
            },
            {
                "email": "Julien.Bigot@cea.fr",
                "first": "Julien",
                "last": "Bigot",
                "affiliation": "Commissariat à l'énergie atomique et aux énergies alternatives"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741980927
    },
    {
        "pid": 71,
        "title": "Schemes for conflict-free data-parallel physics-informed neural networks",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-d731812c25aa6dd330d4ce1fd02f2067853c4f61ddb32d3d3d151a28108b7880",
            "timestamp": 1741972350,
            "size": 24674
        },
        "abstract_only": true,
        "abstract": "Physics-informed neural networks (PINNs) [5] have emerged as a powerful tool for solving complex physical problems by integrating domain knowledge into deep learning models. \r\n\r\nHowever, their training is often inefficient due to the interplay of multiple loss terms, which act as implicit regularizers.This challenge becomes even more pronounced when dealing with high-dimensional, multi-physics, or nonlinear problems that demand extensive computational resources. Efficient training schemes are therefore crucial for scaling PINNs to real-world applications. Various strategies have been proposed to mitigate this issue, including hyper-parameter optimization [3],multi-objective frameworks [1], and conflict-free schemes [4].\r\n\r\nIn this work, we explore data-parallel and conflict-free approaches to achieve scalable, high throughpout training [2]. We extend [2] and compare different methods for assembling the various loss components in a decentralized manner, drawing inspiration from traditional conflict-free gradient averaging methods, all-reduce operations, and aggregation mechanisms such as Adasum and PCGrad. Using the Horovod distributed training framework, we achieve scalable parallelization without compromising model accuracy. Our results demonstrate that increasing the number of GPUs maintains efficiency and ensures robust convergence. We also compare the classical allreduce application to other routines, and compare them. Our findings show that PINN training can be both highly scalable and computationally efficient with the right acceleration strategies. By integrating SciML-specific insights with modern DL frameworks, we increase performance of PINNs. This work paves the way for large-scale, high-throughput physics-informed machine learning (PIML), enabling its application to complex scientific and engineering problems",
        "authors": [
            {
                "email": "paul.escapil@inria.cl",
                "first": "Paul",
                "last": "Escapil-Inchauspé",
                "affiliation": "Inria Chile"
            },
            {
                "email": "lmarti@inria.cl",
                "first": "Luis",
                "last": "Marti",
                "affiliation": "Inria Chile"
            },
            {
                "email": "nayat.sanchez-pi@inria.cl",
                "first": "Nayat",
                "last": "Sanchez-Pi",
                "affiliation": "Inria Chile"
            }
        ],
        "contacts": [
            {
                "email": "paul.escapil@inria.cl",
                "first": "Paul",
                "last": "Escapil-Inchauspé",
                "affiliation": "Inria Chile"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741986952
    },
    {
        "pid": 72,
        "title": "Bridding OT and PaaS in Edge-to-Cloud Continuum",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-07127f38fe4912ee97d759215adfc2f00314006aa144d0d1db1a68c5032481d4",
            "timestamp": 1741972483,
            "size": 3143576
        },
        "abstract_only": false,
        "abstract": "The Operational Technology Platform as a Service (OTPaaS) initiative provides a structured framework for the efficient management and storage of data. It ensures excellent response times while improving security, reliability, data and technology sovereignty, robustness, and energy efficiency, which are crucial for industrial transformation and data sovereignty. This paper illustrates successful deployment, adaptable application management, and various integration components catering to Edge and Cloud environments. It leverages the advantages of the Platform as a Service model and highlights key challenges that have been addressed for specific use cases.",
        "authors": [
            {
                "email": "carlos-jaime.barrios-hernandez@inria.fr",
                "first": "Carlos Jaime",
                "last": "BARRIOS HERNANDEZ",
                "affiliation": "LIG\/INRIA"
            },
            {
                "email": "yves.denneulin@grenoble-inp.fr",
                "first": "Yves",
                "last": "DENNEULIN",
                "affiliation": "GRENOBLE-INP\/LIG\/INRIA"
            }
        ],
        "contacts": [
            {
                "email": "carlos-jaime.barrios-hernandez@inria.fr",
                "first": "Carlos Jaime",
                "last": "BARRIOS HERNANDEZ",
                "affiliation": "LIG\/INRIA"
            },
            {
                "email": "yves.denneulin@grenoble-inp.fr",
                "first": "Yves",
                "last": "DENNEULIN",
                "affiliation": "GRENOBLE-INP\/LIG\/INRIA"
            }
        ],
        "topics": [
            "Architecture",
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741972483
    },
    {
        "pid": 73,
        "title": "Utilisation d'une mini-application de simulation en volumes finis pour expérimenter diverses stratégies de parallélisation multi-GPU",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-bd870fd8e09141a03cce48181f42ea8fde8bab4d9bfe9ac2c3fb1ded406e1505",
            "timestamp": 1741975172,
            "size": 59634
        },
        "abstract_only": true,
        "abstract": "Les simulations numériques s'appuyant sur des schémas en volumes finis sur des maillages non-structurés sont très utilisées en mécanique des fluides, mais demandent un effort considérable pour exploiter des architectures multi-GPU de manière efficace.\r\n\r\nCes travaux explorent, au travers d'une mini-app C++ proposée par Airbus, d'explorer plusieurs stratégies de parallélisation du code de manière la plus indépendante possible des aspects physiques de la simulation. En particulier, nous avons examiné l'intérêt d'utiliser un partitionneur, tel que Scotch, pour réorganiser les mailles afin d'augmenter la localité mémoire et de réduire les échanges de données ainsi que les synchronisations nécessaires entre les GPUs. Ensuite, nous avons évalué l'efficacité de l'utilisation de bordures d'épaisseur arbitraire (halos de ghost cells), pour minimiser les communications. Nous avons également analysé l'efficacité de l'utilisation de la mémoire unifiée entre les GPU au sein d'un même nœud. De plus, nous avons exploré la possibilité de recouvrement des communications par des calculs de sous-domaines. Par ailleurs, nous avons étudié l'utilisation de CUDA graphs pour améliorer l'enchaînement des noyaux. Enfin, nous avons évalué l'utilisation de CUDASTF pour automatiser la gestion de l'asynchronie et recouvrir les calculs.\r\n\r\nPour conduire ces expériences d'une manière non invasive dans le code ainsi que dynamique, nous avons implémenté des classes C++ génériques qui autorisant une parallèlisation hiérarchique arbitraire, c'est-à-dire qui permettent de tester facilement diverses combinaisons et configurations de méthodes de décomposition telles que le partitionnement de cellules ou la coloration de faces. Cette idée généralise les résultats à l'état de l'art qui, jusqu'ici, étaient limités à deux niveaux de coloration successifs.\r\n\r\nNous pensons que le travail effectué sur cette proxy-app peut d'une part être facilement poursuivi par l'ajout d'autres stratégies de passage à l'échelle, fondées sur des modèles de programmation émergents tels que Kokkos ou Raja, et d'autre part qu'il constituera une sorte de guide de bonnes pratiques pour la communauté s'intéressant au portage multi-GPU de codes opérants sur des maillages non-structurés.",
        "authors": [
            {
                "email": "line.fremery@ens-lyon.fr",
                "first": "Line",
                "last": "Fremery",
                "affiliation": "Inria"
            }
        ],
        "contacts": [
            {
                "email": "raymond.namyst@u-bordeaux.fr",
                "first": "Raymond",
                "last": "Namyst",
                "affiliation": "Université de Bordeaux"
            },
            {
                "email": "line.fremery@ens-lyon.fr",
                "first": "Line",
                "last": "Fremery",
                "affiliation": "Inria"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741975172
    },
    {
        "pid": 74,
        "title": "Towards optimal reconfigurable constant multipliers",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-eea403677852f1f5cca27d7d66976e6783f12fd9e0982eb7ad7ca14799a8a1e5",
            "timestamp": 1741982685,
            "size": 112057
        },
        "abstract_only": false,
        "abstract": "This paper introduces a novel algorithm for generating run-time reconﬁgurable single constant multipliers which are optimal in the number of adders and multiplexers they use. Optimality is ensured by an exhaustive exploration of the design space mixing constraint programming, depth-ﬁrst search, and branch-and-bound techniques. Experimental evaluations demonstrate signiﬁcant reductions in resource usage compared to prior methods. The proposed approach also works for larger sets of target constants. Some applications to neural network inference are discussed.",
        "authors": [
            {
                "email": "bastien.barbe@insa-lyon.fr",
                "first": "Bastien",
                "last": "BARBE",
                "affiliation": "INSA Lyon"
            },
            {
                "email": "xiao.peng@insa-lyon.fr",
                "first": "Xiao",
                "last": "PENG",
                "affiliation": "INSA Lyon"
            },
            {
                "email": "anastasia.volkova@inria.fr",
                "first": "Anastasia",
                "last": "VOLKOVA",
                "affiliation": "INRIA"
            },
            {
                "email": "florent.de-dinechin@insa-lyon.fr",
                "first": "Florent",
                "last": "De DINECHIN",
                "affiliation": "INSA Lyon"
            }
        ],
        "contacts": [
            {
                "email": "bastien.barbe@insa-lyon.fr",
                "first": "Bastien",
                "last": "BARBE",
                "affiliation": "INSA Lyon"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741982685
    },
    {
        "pid": 75,
        "title": "Vers un banc d'essai flexible pour les systèmes d'exploitation réseau dans le Computing Continuum",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-157e7790d2bff2ec4ab6717a53378e0b917b64c0d1dad4cc2ce7819beb6444af",
            "timestamp": 1741984127,
            "size": 287628
        },
        "abstract_only": false,
        "abstract": "La réalisation complète du Computing Continuum reste un défi en raison du manque d'infrastructures de recherche adaptées pour l'expérimentation à grande échelle. Pour y remédier, nous proposons un banc d'essai conceptuel visant à améliorer la reproductibilité, l'évolutivité et la robustesse des expériences de réseau Computing Continuum. Ce banc d'essai offre une grande flexibilité, permettant aux expérimentateurs de modifier les systèmes d'exploitation des équipements réseau et de reconfigurer dynamiquement les topologies réseau. Nous définissons trois scénarios d'utilisation distincts, allant des environnements multi-opérateurs aux architectures de réseau internes des fournisseurs de télécommunications, tous déployables sur la topologie de réseau proposée. L'article explore également des solutions pour la gestion de la topologie virtuelle, le déploiement du système d'exploitation et l'orchestration des services.",
        "authors": [
            {
                "email": "julien.caposiena@insa-lyon.fr",
                "first": "Julien",
                "last": "CAPOSIENA",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "frederic.le-mouel@insa-lyon.fr",
                "first": "Frédéric",
                "last": "LE MOUËL",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "oscar.carrillo@insa-lyon.fr",
                "first": "Oscar",
                "last": "CARRILLO",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "baptiste.jonglez@inria.fr",
                "first": "Baptiste",
                "last": "JONGLEZ",
                "affiliation": "Nantes Université, École Centrale Nantes, CNRS, Inria, LS2N, UMR 6004, F-44000 Nantes, France"
            },
            {
                "email": "pierre.neyron@imag.fr",
                "first": "Pierre",
                "last": "NEYRON",
                "affiliation": "CNRS, Univ. Grenoble Alpes, Inria, Grenoble INP, LIG, UMR 5217, 38058 Grenoble, France"
            },
            {
                "email": "thierry.arrabal@insa-lyon.fr",
                "first": "Thierry",
                "last": "ARRABAL",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            }
        ],
        "contacts": [
            {
                "email": "thierry.arrabal@insa-lyon.fr",
                "first": "Thierry",
                "last": "ARRABAL",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "julien.caposiena@insa-lyon.fr",
                "first": "Julien",
                "last": "CAPOSIENA",
                "affiliation": "Laboratoire CITI - Bâtiment Hedy Lamar - 6 Av. des arts 69100 Villeurbanne - France"
            },
            {
                "email": "frederic.le-mouel@insa-lyon.fr",
                "first": "Fredéric",
                "last": "Le Mouël",
                "affiliation": "INSA Lyon, Laboratoire CITI"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741984127
    },
    {
        "pid": 76,
        "title": "VoliMem : Leveraging a user-land page table towards transparent usage of persistent memory",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-4f375b5487bba3fbb8efcedc5e8d02ed19fa9caff1403edc95c70bdc9135c9ad",
            "timestamp": 1741987064,
            "size": 49165
        },
        "abstract_only": true,
        "abstract": "Over the last decades, memory technology has evolved significantly, resulting in the creation of failure-resilient persistent memory (PMEM). Due to its page-cache bypassing and byte-addressability, PMEM offers the durability of SSD with speeds approaching those of modern RAM. However, hardware support alone is insufficient as processor caches remain volatile, which results in data inconsistency in case of failure. For that reason, using PMEM also requires code instrumentation to log memory accesses. Currently, it is necessary to manually instrument the code, which is error-prone and adds additional burden on developers.\r\n\r\nWe propose VoliMem, a user-space runtime that relies on virtualization to provide \\emph{transparent} persistent memory environment for application developers. Namely, VoliMem creates a virtualized process-like abstraction capable of accessing a page table directly in userland. The userland page table is therefore our tool to implement transparent logging using two possible techniques. The first one consists of intercepting writes by removing corresponding permission to the pages. The second one leverages the dirty bit set by the hardware each time when a page is modified. \r\n\r\nWith the help of the VoliMem page table, developing failure-resilient applications with PMEM can become as easy as using only volatile memory. Modifying allocation sites and making periodic snapshots is sufficient to ensure that a legacy application, such as Memcached, is resilient to system crashes. Furthermore, techniques that we apply are language agnostic, as we rely on hardware mechanisms, that are invisible at the application level. So far, we successfully implemented persistency for standard C++ data structures, and we are in the process of providing such integration for Rust and Python applications.",
        "authors": [
            {
                "email": "jana.toljaga@telecom-sudparis.eu",
                "first": "Jana",
                "last": "Toljaga",
                "affiliation": "Inria Saclay, Telecom SudParis, Institut Polytechnique de Paris"
            },
            {
                "email": "derumigny.nicolas@telecom-sudparis.eu",
                "first": "Nicolas",
                "last": "Derumigny",
                "affiliation": "Inria Saclay, Telecom SudParis, Institut Polytechnique de Paris"
            },
            {
                "email": "yohan.pipereau@telecom-sudparis.eu",
                "first": "Yohan",
                "last": "Pipereau",
                "affiliation": "Telecom SudParis, Institut Polytechnique de Paris"
            },
            {
                "email": "mathieu.bacou@telecom-sudparis.eu",
                "first": "Mathieu",
                "last": "Bacou",
                "affiliation": "Télécom SudParis, IP Paris, Inria Saclay"
            },
            {
                "email": "gael.thomas@inria.fr",
                "first": "Gaël",
                "last": "Thomas",
                "affiliation": "Inria Saclay"
            }
        ],
        "contacts": [
            {
                "email": "mathieu.bacou@telecom-sudparis.eu",
                "first": "Mathieu",
                "last": "Bacou",
                "affiliation": "Télécom SudParis, IP Paris, Inria Saclay"
            },
            {
                "email": "gael.thomas@inria.fr",
                "first": "Gaël",
                "last": "Thomas",
                "affiliation": "Benagil Team, Inria Saclay"
            },
            {
                "email": "jana.toljaga@telecom-sudparis.eu",
                "first": "Jana",
                "last": "Toljaga",
                "affiliation": "Inria Saclay, Telecom SudParis, Institut Polytechnique de Paris"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741987064
    },
    {
        "pid": 77,
        "title": "Modèle de micro-data center pour l’évaluation de l’empreinte carbone et l’optimisation énergétique",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-aebe80ef7a4073cd59a804ee085228b03287d7f862b24cb996158542cfac915a",
            "timestamp": 1741990608,
            "size": 121569
        },
        "abstract_only": false,
        "abstract": "Les entreprises se sont tournées vers l'hébergement Cloud pour faciliter l'administration et maîtriser les coûts. Cependant, les data centers partagés posent des problèmes de contrôle des données, notamment en termes de géolocalisation et de protection de la vie privée, car les données de plusieurs clients peuvent être réparties mondialement, soulevant ainsi des questions juridiques et de conformité aux lois sur la protection des données. De plus, la forte consommation d’énergie des data centers génère des émissions significatives de gaz à effet de serre, aggravées par la dépendance aux services Cloud. Les micro-data centers offrent une alternative en réduisant l’empreinte carbone grâce à une efficacité énergétique locale, en respectant les réglementations de souveraineté des données et en diminuant la latence grâce à une proximité accrue avec les utilisateurs finaux. Cependant, ils doivent être faciles à gérer et permettre un suivi précis de l’empreinte énergétique et carbone à différents niveaux (services, VM, serveurs, etc.). Nos travaux proposent de modéliser un micro-data center comme un jumeau numérique avec divers niveaux de granularité, allant du data center aux serveurs, racks, services et workloads. Cette approche permet d’évaluer l’empreinte carbone et la consommation d’énergie en tenant compte des profils d’applications. En preuve de concept, nous intégrons des données statiques pour identifier les principaux facteurs d’émission de GES à chaque niveau et utilisons des wattmètres logiciels pour obtenir des informations en temps réel sur les workloads et leurs possibles placements et migrations, offrant ainsi une analyse détaillée de l’impact environnemental.",
        "authors": [
            {
                "email": "nour.osta@insa-lyon.fr",
                "first": "Noue",
                "last": "Osta",
                "affiliation": "INSA Lyon, Laboratoire CITI"
            },
            {
                "email": "philippe.roose@iutbayonne.univ-pau.fr",
                "first": "Philippe",
                "last": "Roose",
                "affiliation": "UPPA, LIUPPA"
            },
            {
                "email": "jerome.gaysse@spie.com",
                "first": "Jérôme",
                "last": "Gaysse",
                "affiliation": "SPIE ICS"
            },
            {
                "email": "elene.anton@univ-pau.fr",
                "first": "Elene",
                "last": "Anton",
                "affiliation": "UPPA, LIUPPA"
            },
            {
                "email": "frederic.le-mouel@insa-lyon.fr",
                "first": "Fredéric",
                "last": "Le Mouël",
                "affiliation": "INSA Lyon, Laboratoire CITI"
            }
        ],
        "contacts": [
            {
                "email": "Philippe.Roose@iutbayonne.univ-pau.fr",
                "first": "Philippe",
                "last": "Roose",
                "affiliation": "Université de Pau - Bayonne"
            },
            {
                "email": "frederic.le-mouel@insa-lyon.fr",
                "first": "Fredéric",
                "last": "Le Mouël",
                "affiliation": "INSA Lyon, Laboratoire CITI"
            },
            {
                "email": "nour.osta@insa-lyon.fr",
                "first": "nour",
                "last": "osta",
                "affiliation": "INSA Lyon, Laboratoire CITI"
            },
            {
                "email": "jerome.gaysse@spie.com",
                "first": "Jérôme",
                "last": "Gaysse",
                "affiliation": "SPIE ICS"
            },
            {
                "email": "elene.anton@univ-pau.fr",
                "first": "Elene",
                "last": "Anton",
                "affiliation": "UPPA, LIUPPA"
            }
        ],
        "topics": [
            "Système"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741990608
    },
    {
        "pid": 78,
        "title": "Technologies habilitantes pour l’acquisition et le traitement en temps réel de grands volumes de données et leurs applications aux télescopes astronomiques géants et aux systèmes radar",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-72dd4f632b91c67a493bd7dccb3c1f9afdbcbd8a8449b77964ab1210fb2ead19",
            "timestamp": 1741990748,
            "size": 46340
        },
        "abstract_only": true,
        "abstract": "L'émergence de très grandes infrastructures pour l'étude et la caractérisation du monde physique entraîne une augmentation significative du volume de données produites, qui doivent être traitées et réduites par des systèmes informatiques toujours plus puissants et complexe. L'un des principaux défis à venir est le passage à l'Exascale, qui nécessite le développement de nouveaux outils et méthodologies pour gérer et traiter efficacement ces quantités massives de données. Un exemple de ce changement de paradigme dans le domaine de l'astronomie est le Square Kilometer Array (SKA), un projet international de radiotélescope produisant plusieurs dizaines de petaoctets par jour de données brutes. Il devient impératif de développer des outils adaptés à ces nouveaux besoins, capables de passer à l’échelle, en exploitant la croissance de la taille des problèmes pour améliorer la parallélisation et l’ordonnancement des traitements.\r\n\r\nLe travail réalisé au cours de cette thèse vise à l'amélioration et à l'optimisation de la reconstruction d’images en radioastronomie avec DDFacet\\cite{tasse_faceting_2018}. Nos premiers résultats montrent une sous-utilisation des capacités de calcul durant la phase de déconvolution, qui représente 75$\\%$ du temps de traitement total. Le nouvel algorithme multi-échelle d'Offringa et Smirnov\\cite{offringa_optimized_2017} a été implémenté dans DDFacet, offrant de nouvelles possibilités de parallélisme. Les travaux effectués durant cette thèse se concentrent également sur l’étude de différentes stratégies d’ordonnancement et leur impact sur diverses métriques (temps de calcul, occupation des ressources, consommation d'énergie) selon différents cas d'utilisation.\r\n\r\nLes techniques développées pour la radioastronomie peuvent être appliquées à des problèmes connus de l'industrie, notamment dans les applications de traitement du signal en temps réel, comme le radar secondaire. L’objectif principal est d’améliorer la fiabilité du système en contrôlant la latence, en mettant un accent particulier sur la réduction des variations de latence (jitter). La première étape consiste en l'extraction d'un graphe de tâches pour étudier la parallélisation et les stratégies d'ordonnancement possibles, suivie de l'implémentation de prototypes en utilisant des outils tels que StreamPU\\cite{cassagne_streampu_2023}, optimisé pour la basse latence, en prenant en compte la charge CPU et la gestion des priorités des tâches.",
        "authors": [
            {
                "email": "hugo.gaquere@obspm.fr",
                "first": "Hugo",
                "last": "Gaquere",
                "affiliation": "LIRA, Observatoire de Paris"
            },
            {
                "email": "lionel.matias@thalesgroup.com",
                "first": "Lionel",
                "last": "Matias",
                "affiliation": "Thales LAS"
            },
            {
                "email": "Frederic.Vivien@inria.fr",
                "first": "Frederic",
                "last": "Vivien",
                "affiliation": "Inria, ROMA"
            },
            {
                "email": "olivier.aumage@inria.fr",
                "first": "Olivier",
                "last": "Aumage",
                "affiliation": "Inria, Storm"
            },
            {
                "email": "adrien.cassagne@lip6.fr",
                "first": "Adrien",
                "last": "Cassagne",
                "affiliation": "Sorbonne Université, LIP6, Paris"
            },
            {
                "email": "damien.gratadour@obspm.fr",
                "first": "Damien",
                "last": "Gratadour",
                "affiliation": "LIRA, Observatoire de Paris"
            }
        ],
        "contacts": [
            {
                "email": "hugo.gaquere@obspm.fr",
                "first": "Hugo",
                "last": "Gaquere",
                "affiliation": "LIRA, Observatoire de Paris"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741990748
    },
    {
        "pid": 79,
        "title": "Efficient parallel deep learning",
        "abstract_only": true,
        "abstract": "As deep learning models grow larger, they demand more computation power and memory to train in reasonable timeframes. Researchers and companies typically solve this by using multiple accelerators working together, which makes efficient parallelization essential. Among various parallelization strategies proposed in literature, we focus on Pipeline Parallelism, which faces two main challenges: wasted idle time from sequential computation, and excessive memory usage from storing intermediate activations. While the Zero Bubble \\cite{zb} approach effectively addresses idle time issues, memory consumption remains problematic. Our research aims to enable distributed training for models that are too large to fit in accelerators' GPUs using standard methods.\\\\\r\n    One promising solution we explore is rematerialization (activation checkpointing), which can significantly reduce memory usage while performing additional recomputation during the accelerators' idle time, mitigating the overhead. We formulate this as an optimization problem to find the best rematerialization strategy for specific pipelined training scenarios. We also plan to investigate other strategies such as offloading and combination with data\/tensor parallelism.\\\\\r\n    To support this research, we also present ELF, a new PyTorch-based library for distributed deep learning. ELF's flexible design makes it easy to implement and experiment on new ideas.",
        "authors": [
            {
                "email": "adrien.aguila--multner@inria.fr",
                "first": "Adrien",
                "last": "Aguila--Multner",
                "affiliation": "Inria"
            },
            {
                "email": "yulia.gusak@inria.fr",
                "first": "Julia",
                "last": "Gusak",
                "affiliation": "Inria"
            },
            {
                "email": "olivier.beaumont@inria.fr",
                "first": "Olivier",
                "last": "Beaumont",
                "affiliation": "Inria"
            }
        ],
        "contacts": [
            {
                "email": "adrien.aguila--multner@inria.fr",
                "first": "Adrien",
                "last": "Aguila--Multner",
                "affiliation": "Inria"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741991144
    },
    {
        "pid": 80,
        "title": "Vers une abstraction composable des méthodes hiérarchiques pour l’accélération de produits matrice-vecteur",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-0bfe89bd9a415194d0a59ff64605a3a357fc01ed5dc3afd5adc4c3ac5a1bfaae",
            "timestamp": 1741992434,
            "size": 87030
        },
        "abstract_only": true,
        "abstract": "Le produit matrice-vecteur est une opération élémentaire apparaissant dans de nombreuses\r\napplications en calcul scientifique. Il s’agit notamment de la pierre angulaire de nombreux sol-\r\nveurs itératifs. Accélérer cette opération permet ainsi d’améliorer la résolution de nombreux\r\nproblèmes. Nous nous intéressons à l’accélération de y = A x où A est une matrice dense liée à\r\ndes interactions entre des positions géométriques et qui par conséquent présente une structure\r\n(hiérarchique) en tuiles de rang faible (ou une structure similaire). Dans cette configuration, les\r\nméthodes hiérarchiques permettent d’accélérer cette opération en la faisant passer d’un coût\r\nquadratique à une coût quasi-linéaire voir linéaire. Nous pouvons citer par exemple les ma-\r\ntrices H, les matrices H2, les matrices HODLR, les matrices HSS et la méthode des multipoles rapide (FMM). Bien que ces méthodes aient des bases communes, il y a eu\r\njusqu’à présent très peu de rapprochements et de tentatives d’unification entre ces dernières.\r\nNous proposons ainsi une abstraction de ces méthodes basée sur 4 composants : (1) un schéma\r\nde partitionnement pour organiser la matrice en tuiles, (2) un critère d’admissibilité pour dé-\r\nterminer les tuiles qui seront compressés (admissibles), (3) une stratégie de compression pour\r\napprocher les tuiles admissibles (analytique, algébrique, ou encore arithmétique...), et (4) une\r\nstratégie pour gérer les bases afin factoriser certaines étapes de compression. Ces différents\r\ncomposants peuvent ensuite se spécialiser pour retrouver les différentes méthodes de la lit-\r\ntérature mentionnées précédemment. De cette abstraction, nous en avons déduis un unique\r\nalgorithme générique pouvant s’accommoder à toutes les méthodes hiérarchiques. Cette algo-\r\nrithme composable offre une opportunité d’unifier la parallèlisation de toutes ces méthodes.\r\nPar ailleurs, puisque notre abstraction se base sur un modèle de matrice tuilée, nous pouvons\r\nbénéficier des derniers travaux de parallèlisation sur les produits de matrices. D’autre part,\r\nun tel algorithme permettrait à termes d’explorer de nouvelles méthodes de compression jus-\r\nqu’ici peu abordées dans la littérature. Une implémentation séquentielle de cet algorithme a\r\nété réalisée dans la bibliothèque composyx tandis qu’une implémentation parallèle est prévue\r\ndans de futurs travaux.",
        "authors": [
            {
                "email": "antoine.a.gicquel@inria.fr",
                "first": "Antoine",
                "last": "Gicquel",
                "affiliation": "Inria Bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "antoine.a.gicquel@inria.fr",
                "first": "Antoine",
                "last": "Gicquel",
                "affiliation": "Inria Bordeaux"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741992384
    },
    {
        "pid": 81,
        "title": "Peut-on exprimer un produit de matrices distribué haute performance avec un modèle Map-Reduce ?",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-3f2a032dd7b4ba02ebba23fa00635d6c5bf17287a33c17b1489e18b6ada7e22f",
            "timestamp": 1742206920,
            "size": 332754
        },
        "abstract_only": false,
        "abstract": "Le modèle de programmation fonctionnelle MapReduce a permis d’écrire de manière compacte, élégante et performante de nombreux algorithmes parallèles pour le traitement de données massives  sur  des  architectures  distribuées.  En  revanche,  il  n’est  sans  doute  pas  exagéré de penser que son utilisation effective pour la mise au point d’algorithmes de calcul haute performance (HPC) sur des supercalculateurs puisse être considérée comme ésotérique en 2025. Dans cette note, nous considérons un algorithme HPC étendard, à savoir le produit de matrices C←α·A·B+β·C (GEMM) et étudions si les schémas complexes de sa parallélisation posés dans la littérature peuvent être exprimés dans ce paradigme. Nous montrons qu’en enrichissant le modèle pour exprimer sur quels processus placer les données et effectuer les calculs, il est effectivement possible d’exprimer les schémas de parallélisation de la littérature du produit C←α·A·B. Nous montrons que cette expression ne permet cependant pas d’exprimer toute la localité d’accès aux données au sein d’un processus requise pour une performance optimale. Nous proposons finalement en enrichissant encore le modèle avec une troisième fonction inline_mapreduce (en sus de map et reduce), d’exprimer également cette localité et de supporter le cas plus général C←α·A·B+β·C. Un prototype de ce modèle MapReduce HPC a été implémenté au-dessus du moteur d’exécution StarPU.",
        "authors": [
            {
                "email": "hugo.dodelin@inria.fr",
                "first": "Hugo",
                "last": "Dodelin",
                "affiliation": "INRIA Bordeaux"
            }
        ],
        "contacts": [
            {
                "email": "hugo.dodelin@inria.fr",
                "first": "Hugo",
                "last": "Dodelin",
                "affiliation": "INRIA Bordeaux"
            },
            {
                "email": "dodelin.hugo@gmail.com",
                "first": "Hugo",
                "last": "Dodelin",
                "affiliation": "INRIA Bordeaux"
            }
        ],
        "topics": [
            "Parallélisme"
        ],
        "format": "Présentation avec poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1741993166
    },
    {
        "pid": 83,
        "title": "A software-controlled hardware cache for memory disaggregation",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-b3a6a0777bac63ec0ebb040a6a3623fba9323bc5cf02ae0a98001d75ebb0ade7",
            "timestamp": 1742215555,
            "size": 40041
        },
        "abstract_only": true,
        "abstract": "Garbage collection (GC) is an automatic memory management technique essential for reclaiming memory occupied by dead objects. However, efficient garbage collection in distributed and memory-disaggregated systems faces significant challenges due to high synchronization overheads, cache pollution, and increased memory access latency. Specifically, synchronizing application and GC states often leads to cache pollution, degrading system performance, and requiring costly synchronization just to maintain a consistent memory snapshot for garbage collection.\r\n  \r\n\r\nIn this work, we present a novel cache architecture integrating a software-controlled write-back barrier to efficiently support memory snapshotting for garbage collection. Our design introduces dynamic cache line pinning, ensuring dirty lines remain pinned until their contents are preserved in an undo log. This mechanism enables precise memory snapshotting, allowing the garbage collector to operate on a consistent memory state without incurring typical stop-the-world delays. Furthermore, modifications introduced within the QEMU emulation environment include enhancing the virtual address translation process in the software MMU, facilitating detailed simulation of cache behavior and its contents.\r\n\r\n\r\nBy decoupling garbage collection activities from the application's execution context, our proposed solution aims to significantly reduce synchronization overhead. Future work will focus on refining the cache control API, exploring hardware implementations via FPGA, and extending the methodology to other applications that could benefit from efficient, low-overhead memory snapshotting.",
        "authors": [
            {
                "email": "nevena.vasilevska@telecom-sudparis.eu",
                "first": "Nevena",
                "last": "Vasilevska",
                "affiliation": "Telecom SudParis"
            },
            {
                "email": "julie.dumas@univ-grenoble-alpes.fr",
                "first": "Julie",
                "last": "Dumas",
                "affiliation": "TIMA - Grenoble"
            },
            {
                "email": "derumigny.nicolas@telecom-sudparis.eu",
                "first": "Nicolas",
                "last": "Derumigny",
                "affiliation": "Telecom SudParis"
            },
            {
                "email": "gael.thomas@inria.fr",
                "first": "Gael",
                "last": "Thomas",
                "affiliation": "Inria"
            }
        ],
        "contacts": [
            {
                "email": "julie.dumas@univ-grenoble-alpes.fr",
                "first": "Julie",
                "last": "Dumas",
                "affiliation": "TIMA - Grenoble"
            },
            {
                "email": "gael.thomas@inria.fr",
                "first": "Gaël",
                "last": "Thomas",
                "affiliation": "Benagil Team, Inria Saclay"
            },
            {
                "email": "nevena.vasilevska@telecom-sudparis.eu",
                "first": "Nevena",
                "last": "Vasilevska",
                "affiliation": "Telecom SudParis"
            }
        ],
        "topics": [
            "Architecture"
        ],
        "format": "Présentation sans poster",
        "decision": 1,
        "status": "accepted",
        "submitted": true,
        "submitted_at": 1742215554
    }
]
